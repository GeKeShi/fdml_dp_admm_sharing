iteration 0 batch 0 trainingloss 0.6812757589023914
iteration 0 batch 10 trainingloss 0.6886953974383626
iteration 0 batch 20 trainingloss 0.6842436143167798
iteration 0 batch 30 trainingloss 0.6842436143167799
iteration 0 batch 40 trainingloss 0.6872114697311684
iteration 0 batch 50 trainingloss 0.6827596866095857
iteration 0 batch 60 trainingloss 0.6901793251455568
iteration 0 batch 70 trainingloss 0.6901793251455568
iteration 0 batch 80 trainingloss 0.6886953974383626
iteration 0 batch 90 trainingloss 0.6872114697311684
iteration 0 batch 100 trainingloss 0.6872114697311684
iteration 0 batch 110 trainingloss 0.6916632528527511
iteration 0 batch 120 trainingloss 0.6872114697311684
iteration 0 batch 130 trainingloss 0.6916632528527511
iteration 0 batch 140 trainingloss 0.6857275420239741
iteration 0 batch 150 trainingloss 0.6872114697311684
iteration 0 batch 160 trainingloss 0.6901793251455568
iteration 0 batch 170 trainingloss 0.6931471805599453
iteration 0 batch 180 trainingloss 0.6901793251455568
iteration 0 batch 190 trainingloss 0.6916632528527511
iteration 0 batch 200 trainingloss 0.6872114697311684
iteration 0 batch 210 trainingloss 0.6916632528527511
iteration 0 batch 220 trainingloss 0.6901793251455568
iteration 0 batch 230 trainingloss 0.6872114697311684
iteration 0 batch 240 trainingloss 0.6901793251455568
iteration 0 batch 250 trainingloss 0.6916632528527511
iteration 0 batch 260 trainingloss 0.6886953974383626
iteration 0 batch 270 trainingloss 0.6916632528527511
iteration 0 batch 280 trainingloss 0.6886953974383626
iteration 0 batch 290 trainingloss 0.6886953974383626
iteration 0 batch 300 trainingloss 0.6916632528527511
iteration 0 batch 310 trainingloss 0.6886953974383626
iteration 0 batch 320 trainingloss 0.6916632528527511
iteration 0 batch 330 trainingloss 0.6901793251455568
iteration 0 batch 340 trainingloss 0.6916632528527511
iteration 0 batch 350 trainingloss 0.6931471805599453
iteration 0 batch 360 trainingloss 0.6901793251455568
iteration 0 batch 370 trainingloss 0.6931471805599453
iteration 0 batch 380 trainingloss 0.6931471805599453
iteration 0 batch 390 trainingloss 0.6916632528527511
iteration 0 batch 400 trainingloss 0.6886953974383626
iteration 0 batch 410 trainingloss 0.6886953974383626
iteration 0 batch 420 trainingloss 0.6901793251455568
iteration 0 batch 430 trainingloss 0.6886953974383626
iteration 0 batch 440 trainingloss 0.6872114697311684
iteration 0 batch 450 trainingloss 0.6886953974383626
iteration 0 batch 460 trainingloss 0.6872114697311684
iteration 0 batch 470 trainingloss 0.6886953974383626
iteration 0 batch 480 trainingloss 0.6916632528527511
iteration 0 batch 490 trainingloss 0.6886953974383626
iteration 0 batch 500 trainingloss 0.6931471805599453
iteration 0 batch 510 trainingloss 0.6872114697311684
iteration 0 batch 520 trainingloss 0.6916632528527511
iteration 0 batch 530 trainingloss 0.6931471805599453
iteration 0 batch 540 trainingloss 0.6901793251455568
iteration 0 batch 550 trainingloss 0.6901793251455568
iteration 0 batch 560 trainingloss 0.6872114697311684
iteration 0 batch 570 trainingloss 0.6931471805599453
iteration 0 batch 580 trainingloss 0.6857275420239741
iteration 0 batch 590 trainingloss 0.6886953974383626
iteration 0 batch 600 trainingloss 0.6901793251455568
iteration 0 batch 610 trainingloss 0.6857275420239741
iteration 0 batch 620 trainingloss 0.6916632528527511
iteration 0 batch 630 trainingloss 0.6916632528527511
iteration 0 batch 640 trainingloss 0.6916632528527511
iteration 0 batch 650 trainingloss 0.6872114697311684
iteration 0 batch 660 trainingloss 0.6886953974383626
iteration 0 batch 670 trainingloss 0.6901793251455568
iteration 0 batch 680 trainingloss 0.6857275420239741
iteration 0 batch 690 trainingloss 0.6827596866095857
iteration 0 batch 700 trainingloss 0.6872114697311684
iteration 0 batch 710 trainingloss 0.6857275420239741
iteration 0 batch 720 trainingloss 0.6872114697311684
iteration 0 batch 730 trainingloss 0.6886953974383626
iteration 0 batch 740 trainingloss 0.6931471805599453
iteration 0 batch 750 trainingloss 0.6931471805599453
iteration 0 batch 760 trainingloss 0.6916632528527511
iteration 0 batch 770 trainingloss 0.6901793251455568
iteration 0 batch 780 trainingloss 0.6886953974383626
iteration 0 batch 790 trainingloss 0.6886953974383626
iteration 0 batch 800 trainingloss 0.6901793251455568
iteration 0 batch 810 trainingloss 0.6931471805599453
iteration 0 batch 820 trainingloss 0.6857275420239741
iteration 0 batch 830 trainingloss 0.6916632528527511
iteration 0 batch 840 trainingloss 0.6901793251455568
iteration 0 batch 850 trainingloss 0.6916632528527511
iteration 0 batch 860 trainingloss 0.6872114697311684
iteration 0 batch 870 trainingloss 0.6886953974383626
iteration 0 batch 880 trainingloss 0.6872114697311684
iteration 0 batch 890 trainingloss 0.6916632528527511
iteration 0 batch 900 trainingloss 0.6886953974383626
iteration 0 batch 910 trainingloss 0.6916632528527511
iteration 0 batch 920 trainingloss 0.6901793251455568
iteration 0 batch 930 trainingloss 0.6857275420239741
iteration 0 batch 940 trainingloss 0.6931471805599453
iteration 0 batch 950 trainingloss 0.6886953974383626
iteration 0 batch 960 trainingloss 0.6886953974383626
iteration 0 batch 970 trainingloss 0.6916632528527511
iteration 0 batch 980 trainingloss 0.6827596866095857
iteration 0 batch 990 trainingloss 0.6916632528527511
iteration 0 batch 1000 trainingloss 0.6872114697311684
iteration 0 batch 1010 trainingloss 0.6916632528527511
iteration 0 batch 1020 trainingloss 0.6901793251455568
iteration 0 batch 1030 trainingloss 0.6901793251455568
iteration 0 batch 1040 trainingloss 0.6886953974383626
iteration 0 batch 1050 trainingloss 0.6931471805599453
iteration 0 batch 1060 trainingloss 0.6886953974383626
iteration 0 batch 1070 trainingloss 0.6857275420239741
iteration 0 batch 1080 trainingloss 0.6901793251455568
iteration 0 batch 1090 trainingloss 0.6901793251455568
iteration 0 batch 1100 trainingloss 0.6872114697311684
iteration 0 batch 1110 trainingloss 0.6886953974383626
iteration 0 batch 1120 trainingloss 0.6901793251455568
iteration 0 batch 1130 trainingloss 0.6916632528527511
iteration 0 batch 1140 trainingloss 0.6916632528527511
iteration 0 batch 1150 trainingloss 0.6872114697311684
iteration 0 batch 1160 trainingloss 0.6857275420239741
iteration 0 batch 1170 trainingloss 0.6886953974383626
iteration 0 batch 1180 trainingloss 0.6886953974383626
iteration 0 batch 1190 trainingloss 0.6886953974383626
iteration 0 batch 1200 trainingloss 0.6931471805599453
iteration 0 batch 1210 trainingloss 0.6916632528527511
iteration 0 batch 1220 trainingloss 0.6931471805599453
iteration 0 batch 1230 trainingloss 0.6857275420239741
iteration 0 batch 1240 trainingloss 0.6901793251455568
iteration 0 batch 1250 trainingloss 0.6886953974383626
iteration 0 batch 1260 trainingloss 0.6916632528527511
iteration 0 batch 1270 trainingloss 0.6916632528527511
iteration 0 batch 1280 trainingloss 0.6886953974383626
iteration 0 batch 1290 trainingloss 0.6916632528527511
iteration 0 batch 1300 trainingloss 0.6931471805599453
iteration 0 batch 1310 trainingloss 0.6901793251455568
iteration 0 batch 1320 trainingloss 0.6916632528527511
iteration 0 batch 1330 trainingloss 0.6872114697311684
iteration 0 batch 1340 trainingloss 0.6886953974383626
iteration 0 batch 1350 trainingloss 0.6886953974383626
iteration 0 batch 1360 trainingloss 0.6931471805599453
iteration 0 batch 1370 trainingloss 0.6931471805599453
iteration 0 batch 1380 trainingloss 0.6872114697311684
iteration 0 batch 1390 trainingloss 0.6916632528527511
iteration 0 batch 1400 trainingloss 0.6916632528527511
iteration 0 batch 1410 trainingloss 0.6872114697311684
iteration 0 batch 1420 trainingloss 0.6886953974383626
iteration 0 batch 1430 trainingloss 0.6931471805599453
iteration 0 batch 1440 trainingloss 0.6931471805599453
iteration 0 batch 1450 trainingloss 0.6872114697311683
iteration 0 batch 1460 trainingloss 0.6916632528527511
iteration 0 batch 1470 trainingloss 0.6931471805599453
iteration 0 batch 1480 trainingloss 0.6901793251455568
iteration 0 batch 1490 trainingloss 0.6916632528527511
iteration 0 batch 1500 trainingloss 0.6916632528527511
iteration 0 batch 1510 trainingloss 0.6901793251455568
iteration 0 batch 1520 trainingloss 0.6916632528527511
iteration 0 batch 1530 trainingloss 0.6916632528527511
iteration 0 batch 1540 trainingloss 0.6916632528527511
iteration 0 batch 1550 trainingloss 0.6916632528527511
iteration 0 batch 1560 trainingloss 0.6901793251455568
iteration 0 batch 1570 trainingloss 0.6931471805599453
iteration 0 batch 1580 trainingloss 0.6901793251455568
iteration 0 batch 1590 trainingloss 0.6886953974383626
iteration 0 batch 1600 trainingloss 0.6886953974383626
iteration 0 batch 1610 trainingloss 0.6916632528527511
iteration 0 batch 1620 trainingloss 0.6931471805599453
iteration 0 batch 1630 trainingloss 0.6916632528527511
iteration 0 batch 1640 trainingloss 0.6931471805599453
iteration 0 batch 1650 trainingloss 0.6916632528527511
iteration 0 batch 1660 trainingloss 0.6916632528527511
iteration 0 batch 1670 trainingloss 0.6886953974383626
iteration 0 batch 1680 trainingloss 0.6886953974383626
iteration 0 batch 1690 trainingloss 0.6916632528527511
iteration 0 batch 1700 trainingloss 0.6872114697311684
iteration 0 batch 1710 trainingloss 0.6931471805599453
iteration 0 batch 1720 trainingloss 0.6901793251455568
iteration 0 batch 1730 trainingloss 0.6901793251455568
iteration 0 batch 1740 trainingloss 0.6857275420239741
iteration 0 batch 1750 trainingloss 0.6901793251455568
iteration 0 batch 1760 trainingloss 0.6901793251455568
iteration 0 batch 1770 trainingloss 0.6842436143167799
iteration 0 batch 1780 trainingloss 0.6901793251455568
iteration 0 batch 1790 trainingloss 0.6916632528527511
iteration 0 batch 1800 trainingloss 0.6886953974383626
iteration 0 batch 1810 trainingloss 0.6931471805599453
iteration 0 batch 1820 trainingloss 0.6857275420239741
iteration 0 batch 1830 trainingloss 0.6916632528527511
iteration 0 batch 1840 trainingloss 0.6931471805599453
iteration 0 batch 1850 trainingloss 0.6886953974383626
iteration 0 batch 1860 trainingloss 0.6916632528527511
iteration 0 batch 1870 trainingloss 0.6916632528527511
iteration 0 batch 1880 trainingloss 0.6916632528527511
iteration 0 batch 1890 trainingloss 0.6916632528527511
iteration 0 batch 1900 trainingloss 0.6916632528527511
iteration 0 batch 1910 trainingloss 0.6901793251455568
iteration 0 batch 1920 trainingloss 0.6931471805599453
iteration 0 batch 1930 trainingloss 0.6931471805599453
iteration 0 batch 1940 trainingloss 0.6916632528527511
iteration 0 batch 1950 trainingloss 0.6901793251455568
iteration 0 batch 1960 trainingloss 0.6901793251455568
iteration 0 batch 1970 trainingloss 0.6886953974383626
iteration 0 batch 1980 trainingloss 0.6931471805599453
iteration 0 batch 1990 trainingloss 0.6901793251455568
iteration 0 batch 2000 trainingloss 0.6901793251455568
iteration 0 batch 2010 trainingloss 0.6916632528527511
iteration 0 batch 2020 trainingloss 0.6916632528527511
iteration 0 batch 2030 trainingloss 0.6931471805599453
iteration 0 batch 2040 trainingloss 0.6931471805599453
iteration 0 batch 2050 trainingloss 0.6886953974383626
iteration 0 batch 2060 trainingloss 0.6901793251455568
iteration 0 batch 2070 trainingloss 0.6916632528527511
iteration 0 batch 2080 trainingloss 0.6916632528527511
iteration 0 batch 2090 trainingloss 0.6901793251455568
iteration 0 batch 2100 trainingloss 0.6916632528527511
iteration 0 batch 2110 trainingloss 0.6916632528527511
iteration 0 batch 2120 trainingloss 0.6931471805599453
iteration 0 batch 2130 trainingloss 0.6931471805599453
iteration 0 batch 2140 trainingloss 0.6886953974383626
iteration 0 batch 2150 trainingloss 0.6916632528527511
iteration 0 batch 2160 trainingloss 0.6931471805599453
iteration 0 batch 2170 trainingloss 0.6872114697311684
iteration 0 batch 2180 trainingloss 0.6931471805599453
iteration 0 batch 2190 trainingloss 0.6916632528527511
iteration 0 batch 2200 trainingloss 0.6901793251455568
iteration 0 batch 2210 trainingloss 0.6916632528527511
iteration 0 batch 2220 trainingloss 0.6901793251455568
iteration 0 batch 2230 trainingloss 0.6916632528527511
iteration 0 batch 2240 trainingloss 0.6886953974383626
iteration 0 batch 2250 trainingloss 0.6916632528527511
iteration 0 batch 2260 trainingloss 0.6916632528527511
iteration 0 batch 2270 trainingloss 0.6916632528527511
iteration 0 batch 2280 trainingloss 0.6916632528527511
iteration 0 batch 2290 trainingloss 0.6886953974383626
iteration 0 batch 2300 trainingloss 0.6931471805599453
iteration 0 batch 2310 trainingloss 0.6916632528527511
iteration 0 batch 2320 trainingloss 0.6872114697311684
iteration 0 batch 2330 trainingloss 0.6901793251455568
iteration 0 batch 2340 trainingloss 0.6886953974383626
iteration 0 batch 2350 trainingloss 0.6916632528527511
iteration 0 batch 2360 trainingloss 0.6916632528527511
iteration 0 batch 2370 trainingloss 0.6931471805599453
iteration 0 batch 2380 trainingloss 0.6916632528527511
iteration 0 batch 2390 trainingloss 0.6916632528527511
iteration 0 batch 2400 trainingloss 0.6931471805599453
iteration 0 batch 2410 trainingloss 0.6886953974383626
iteration 0 batch 2420 trainingloss 0.6901793251455568
iteration 0 batch 2430 trainingloss 0.6931471805599453
iteration 0 batch 2440 trainingloss 0.6901793251455568
iteration 0 batch 2450 trainingloss 0.6886953974383626
iteration 0 batch 2460 trainingloss 0.6931471805599453
iteration 0 batch 2470 trainingloss 0.6931471805599453
iteration 0 batch 2480 trainingloss 0.6872114697311684
iteration 0 batch 2490 trainingloss 0.6916632528527511
iteration 0 batch 2500 trainingloss 0.6916632528527511
iteration 0 batch 2510 trainingloss 0.6916632528527511
iteration 0 batch 2520 trainingloss 0.6931471805599453
iteration 0 batch 2530 trainingloss 0.6931471805599453
iteration 0 batch 2540 trainingloss 0.6931471805599453
iteration 0 batch 2550 trainingloss 0.6931471805599453
iteration 0 batch 2560 trainingloss 0.6886953974383626
iteration 0 batch 2570 trainingloss 0.6931471805599453
iteration 0 batch 2580 trainingloss 0.6901793251455568
iteration 0 batch 2590 trainingloss 0.6931471805599453
iteration 0 batch 2600 trainingloss 0.6916632528527511
iteration 0 batch 2610 trainingloss 0.6872114697311684
iteration 0 batch 2620 trainingloss 0.6916632528527511
iteration 0 batch 2630 trainingloss 0.6901793251455568
iteration 0 batch 2640 trainingloss 0.6916632528527511
iteration 0 batch 2650 trainingloss 0.6931471805599453
iteration 0 batch 2660 trainingloss 0.6901793251455568
iteration 0 batch 2670 trainingloss 0.6931471805599453
iteration 0 batch 2680 trainingloss 0.6901793251455568
iteration 0 batch 2690 trainingloss 0.6916632528527511
iteration 0 batch 2700 trainingloss 0.6916632528527511
iteration 0 batch 2710 trainingloss 0.6916632528527511
iteration 0 batch 2720 trainingloss 0.6886953974383626
iteration 0 batch 2730 trainingloss 0.6886953974383626
iteration 0 batch 2740 trainingloss 0.6916632528527511
iteration 0 batch 2750 trainingloss 0.6916632528527511
iteration 0 batch 2760 trainingloss 0.6916632528527511
iteration 0 batch 2770 trainingloss 0.6916632528527511
iteration 0 batch 2780 trainingloss 0.6931471805599453
iteration 0 batch 2790 trainingloss 0.6916632528527511
iteration 0 batch 2800 trainingloss 0.6931471805599453
iteration 0 batch 2810 trainingloss 0.6901793251455568
iteration 0 batch 2820 trainingloss 0.6931471805599453
iteration 0 batch 2830 trainingloss 0.6886953974383626
iteration 0 batch 2840 trainingloss 0.6916632528527511
iteration 0 batch 2850 trainingloss 0.6916632528527511
iteration 0 batch 2860 trainingloss 0.6872114697311684
iteration 0 batch 2870 trainingloss 0.6931471805599453
iteration 0 batch 2880 trainingloss 0.6916632528527511
iteration 0 batch 2890 trainingloss 0.6901793251455568
iteration 0 batch 2900 trainingloss 0.6886953974383626
iteration 0 batch 2910 trainingloss 0.6916632528527511
iteration 0 batch 2920 trainingloss 0.6872114697311684
iteration 0 batch 2930 trainingloss 0.6886953974383626
iteration 0 batch 2940 trainingloss 0.6931471805599453
iteration 0 batch 2950 trainingloss 0.6931471805599453
iteration 0 batch 2960 trainingloss 0.6901793251455568
iteration 0 batch 2970 trainingloss 0.6931471805599453
iteration 0 batch 2980 trainingloss 0.6931471805599453
iteration 0 batch 2990 trainingloss 0.6931471805599453
iteration 0 batch 3000 trainingloss 0.6931471805599453
iteration 0 batch 3010 trainingloss 0.6872114697311684
iteration 0 batch 3020 trainingloss 0.6916632528527511
iteration 0 batch 3030 trainingloss 0.6931471805599453
iteration 0 batch 3040 trainingloss 0.6931471805599453
iteration 0 batch 3050 trainingloss 0.6931471805599453
iteration 0 batch 3060 trainingloss 0.6916632528527511
iteration 0 batch 3070 trainingloss 0.6931471805599453
iteration 0 batch 3080 trainingloss 0.6916632528527511
iteration 0 batch 3090 trainingloss 0.6916632528527511
iteration 0 batch 3100 trainingloss 0.6916632528527511
iteration 0 batch 3110 trainingloss 0.6916632528527511
iteration 0 batch 3120 trainingloss 0.6916632528527511
iteration 0 batch 3130 trainingloss 0.6931471805599453
iteration 0 batch 3140 trainingloss 0.6916632528527511
iteration 0 batch 3150 trainingloss 0.6901793251455568
iteration 0 batch 3160 trainingloss 0.6931471805599453
iteration 0 batch 3170 trainingloss 0.6916632528527511
iteration 0 batch 3180 trainingloss 0.6931471805599453
iteration 0 batch 3190 trainingloss 0.6931471805599453
iteration 0 batch 3200 trainingloss 0.6931471805599453
iteration 0 batch 3210 trainingloss 0.6872114697311684
iteration 0 batch 3220 trainingloss 0.6916632528527511
iteration 0 batch 3230 trainingloss 0.6916632528527511
iteration 0 batch 3240 trainingloss 0.6872114697311684
iteration 0 batch 3250 trainingloss 0.6901793251455568
iteration 0 batch 3260 trainingloss 0.6916632528527511
iteration 0 batch 3270 trainingloss 0.6901793251455568
iteration 0 batch 3280 trainingloss 0.6872114697311684
iteration 0 batch 3290 trainingloss 0.6916632528527511
iteration 0 batch 3300 trainingloss 0.6901793251455568
iteration 0 batch 3310 trainingloss 0.6931471805599453
iteration 0 batch 3320 trainingloss 0.6901793251455568
iteration 0 batch 3330 trainingloss 0.6842436143167799
iteration 0 batch 3340 trainingloss 0.6886953974383626
iteration 0 batch 3350 trainingloss 0.6916632528527511
iteration 0 batch 3360 trainingloss 0.6901793251455568
iteration 0 batch 3370 trainingloss 0.6931471805599453
iteration 0 batch 3380 trainingloss 0.6901793251455568
iteration 0 batch 3390 trainingloss 0.6931471805599453
iteration 0 batch 3400 trainingloss 0.6901793251455568
iteration 0 batch 3410 trainingloss 0.6931471805599453
iteration 0 batch 3420 trainingloss 0.6901793251455568
iteration 0 batch 3430 trainingloss 0.6931471805599453
iteration 0 batch 3440 trainingloss 0.6916632528527511
iteration 0 batch 3450 trainingloss 0.6916632528527511
iteration 0 batch 3460 trainingloss 0.6931471805599453
iteration 0 batch 3470 trainingloss 0.6916632528527511
iteration 0 batch 3480 trainingloss 0.6931471805599453
iteration 0 batch 3490 trainingloss 0.6916632528527511
iteration 0 batch 3500 trainingloss 0.6931471805599453
iteration 0 batch 3510 trainingloss 0.6931471805599453
iteration 0 batch 3520 trainingloss 0.6916632528527511
iteration 0 batch 3530 trainingloss 0.6931471805599453
iteration 0 batch 3540 trainingloss 0.6901793251455568
iteration 0 batch 3550 trainingloss 0.6931471805599453
iteration 0 batch 3560 trainingloss 0.6931471805599453
iteration 0 batch 3570 trainingloss 0.6916632528527511
iteration 0 batch 3580 trainingloss 0.6931471805599453
iteration 0 batch 3590 trainingloss 0.6901793251455568
iteration 0 batch 3600 trainingloss 0.6931471805599453
iteration 0 batch 3610 trainingloss 0.6931471805599453
iteration 0 batch 3620 trainingloss 0.6901793251455568
iteration 0 batch 3630 trainingloss 0.6886953974383626
iteration 0 batch 3640 trainingloss 0.6872114697311684
iteration 0 batch 3650 trainingloss 0.6916632528527511
iteration 0 batch 3660 trainingloss 0.6916632528527511
iteration 0 batch 3670 trainingloss 0.6916632528527511
iteration 0 batch 3680 trainingloss 0.6901793251455568
iteration 0 batch 3690 trainingloss 0.6886953974383626
iteration 0 batch 3700 trainingloss 0.6931471805599453
iteration 0 batch 3710 trainingloss 0.6916632528527511
iteration 0 batch 3720 trainingloss 0.6916632528527511
iteration 0 batch 3730 trainingloss 0.6901793251455568
iteration 0 batch 3740 trainingloss 0.6916632528527511
iteration 0 batch 3750 trainingloss 0.6916632528527511
iteration 0 batch 3760 trainingloss 0.6916632528527511
iteration 0 batch 3770 trainingloss 0.6901793251455568
iteration 0 batch 3780 trainingloss 0.6931471805599453
iteration 0 batch 3790 trainingloss 0.6916632528527511
iteration 0 batch 3800 trainingloss 0.6901793251455568
iteration 0 batch 3810 trainingloss 0.6916632528527511
iteration 0 batch 3820 trainingloss 0.6872114697311684
iteration 0 batch 3830 trainingloss 0.6872114697311684
iteration 0 batch 3840 trainingloss 0.6872114697311684
iteration 0 batch 3850 trainingloss 0.6916632528527511
iteration 0 batch 3860 trainingloss 0.6886953974383626
iteration 0 batch 3870 trainingloss 0.6916632528527511
iteration 0 batch 3880 trainingloss 0.6916632528527511
iteration 0 batch 3890 trainingloss 0.6916632528527511
iteration 0 batch 3900 trainingloss 0.6901793251455568
iteration 0 batch 3910 trainingloss 0.6931471805599453
iteration 0 batch 3920 trainingloss 0.6916632528527511
iteration 0 batch 3930 trainingloss 0.6931471805599453
iteration 0 batch 3940 trainingloss 0.6916632528527511
iteration 0 batch 3950 trainingloss 0.6931471805599453
iteration 0 batch 3960 trainingloss 0.6916632528527511
iteration 0 batch 3970 trainingloss 0.6901793251455568
iteration 0 batch 3980 trainingloss 0.6916632528527511
iteration 0 batch 3990 trainingloss 0.6872114697311684
iteration 0 batch 4000 trainingloss 0.6931471805599453
iteration 0 batch 4010 trainingloss 0.6886953974383626
iteration 0 batch 4020 trainingloss 0.6916632528527511
iteration 0 batch 4030 trainingloss 0.6886953974383626
iteration 0 batch 4040 trainingloss 0.6931471805599453
iteration 0 batch 4050 trainingloss 0.6931471805599453
iteration 0 batch 4060 trainingloss 0.6931471805599453
iteration 0 batch 4070 trainingloss 0.6931471805599453
iteration 0 batch 4080 trainingloss 0.6931471805599453
iteration 0 batch 4090 trainingloss 0.6886953974383626
iteration 0 batch 4100 trainingloss 0.6886953974383626
iteration 0 batch 4110 trainingloss 0.6931471805599453
iteration 0 batch 4120 trainingloss 0.6916632528527511
iteration 0 batch 4130 trainingloss 0.6901793251455568
iteration 0 batch 4140 trainingloss 0.6916632528527511
iteration 0 batch 4150 trainingloss 0.6901793251455568
iteration 0 batch 4160 trainingloss 0.6901793251455568
iteration 0 batch 4170 trainingloss 0.6931471805599453
iteration 0 batch 4180 trainingloss 0.6916632528527511
iteration 0 batch 4190 trainingloss 0.6916632528527511
iteration 0 batch 4200 trainingloss 0.6931471805599453
iteration 0 batch 4210 trainingloss 0.6916632528527511
iteration 0 batch 4220 trainingloss 0.6916632528527511
iteration 0 batch 4230 trainingloss 0.6916632528527511
iteration 0 batch 4240 trainingloss 0.6931471805599453
iteration 0 batch 4250 trainingloss 0.6931471805599453
iteration 0 batch 4260 trainingloss 0.6931471805599453
iteration 0 batch 4270 trainingloss 0.6916632528527511
iteration 0 batch 4280 trainingloss 0.6916632528527511
iteration 0 batch 4290 trainingloss 0.6901793251455568
iteration 0 batch 4300 trainingloss 0.6931471805599453
iteration 0 batch 4310 trainingloss 0.6931471805599453
iteration 0 batch 4320 trainingloss 0.6931471805599453
iteration 0 batch 4330 trainingloss 0.6931471805599453
iteration 0 batch 4340 trainingloss 0.6916632528527511
iteration 0 batch 4350 trainingloss 0.6931471805599453
iteration 0 batch 4360 trainingloss 0.6916632528527511
iteration 0 batch 4370 trainingloss 0.6931471805599453
iteration 0 batch 4380 trainingloss 0.6916632528527511
iteration 0 batch 4390 trainingloss 0.6901793251455568
iteration 0 batch 4400 trainingloss 0.6931471805599453
iteration 0 batch 4410 trainingloss 0.6916632528527511
iteration 0 batch 4420 trainingloss 0.6931471805599453
iteration 0 batch 4430 trainingloss 0.6886953974383626
iteration 0 batch 4440 trainingloss 0.6931471805599453
iteration 0 batch 4450 trainingloss 0.6916632528527511
iteration 0 batch 4460 trainingloss 0.6931471805599453
iteration 0 batch 4470 trainingloss 0.6916632528527511
iteration 0 batch 4480 trainingloss 0.6916632528527511
iteration 0 batch 4490 trainingloss 0.6931471805599453
iteration 0 batch 4500 trainingloss 0.6901793251455568
iteration 0 batch 4510 trainingloss 0.6931471805599453
iteration 0 batch 4520 trainingloss 0.6886953974383626
iteration 0 batch 4530 trainingloss 0.6901793251455568
iteration 0 batch 4540 trainingloss 0.6886953974383626
iteration 0 batch 4550 trainingloss 0.6916632528527511
iteration 0 batch 4560 trainingloss 0.6916632528527511
iteration 0 batch 4570 trainingloss 0.6901793251455568
iteration 0 batch 4580 trainingloss 0.6931471805599453
iteration 0 batch 4590 trainingloss 0.6886953974383626
iteration 0 batch 4600 trainingloss 0.6916632528527511
iteration 0 batch 4610 trainingloss 0.6916632528527511
iteration 0 batch 4620 trainingloss 0.6916632528527511
iteration 0 batch 4630 trainingloss 0.6931471805599453
iteration 0 batch 4640 trainingloss 0.6901793251455568
iteration 0 batch 4650 trainingloss 0.6916632528527511
iteration 0 batch 4660 trainingloss 0.6872114697311684
iteration 0 batch 4670 trainingloss 0.6931471805599453
iteration 0 batch 4680 trainingloss 0.6872114697311684
iteration 0 batch 4690 trainingloss 0.6931471805599453
iteration 0 batch 4700 trainingloss 0.6901793251455568
iteration 0 batch 4710 trainingloss 0.6931471805599453
iteration 0 batch 4720 trainingloss 0.6886953974383626
iteration 0 batch 4730 trainingloss 0.6901793251455568
iteration 0 batch 4740 trainingloss 0.6901793251455568
iteration 0 batch 4750 trainingloss 0.6916632528527511
iteration 0 batch 4760 trainingloss 0.6916632528527511
iteration 0 batch 4770 trainingloss 0.6931471805599453
iteration 0 batch 4780 trainingloss 0.6916632528527511
iteration 0 batch 4790 trainingloss 0.6916632528527511
iteration 0 batch 4800 trainingloss 0.6901793251455568
iteration 0 batch 4810 trainingloss 0.6901793251455568
iteration 0 batch 4820 trainingloss 0.6886953974383626
iteration 0 batch 4830 trainingloss 0.6916632528527511
iteration 0 batch 4840 trainingloss 0.6916632528527511
iteration 0 batch 4850 trainingloss 0.6931471805599453
iteration 0 batch 4860 trainingloss 0.6931471805599453
iteration 0 batch 4870 trainingloss 0.6931471805599453
iteration 0 batch 4880 trainingloss 0.6872114697311683
iteration 0 batch 4890 trainingloss 0.6901793251455568
iteration 0 batch 4900 trainingloss 0.6901793251455568
iteration 0 batch 4910 trainingloss 0.6931471805599453
iteration 0 batch 4920 trainingloss 0.6931471805599453
iteration 0 batch 4930 trainingloss 0.6857275420239741
iteration 0 batch 4940 trainingloss 0.6872114697311683
iteration 0 batch 4950 trainingloss 0.6901793251455568
iteration 0 batch 4960 trainingloss 0.6916632528527511
iteration 0 batch 4970 trainingloss 0.6901793251455568
iteration 0 batch 4980 trainingloss 0.6857275420239741
iteration 0 batch 4990 trainingloss 0.6886953974383626
iteration 0 batch 5000 trainingloss 0.6901793251455568
iteration 0 batch 5010 trainingloss 0.6931471805599453
iteration 0 batch 5020 trainingloss 0.6916632528527511
iteration 0 batch 5030 trainingloss 0.6916632528527511
iteration 0 batch 5040 trainingloss 0.6916632528527511
iteration 0 batch 5050 trainingloss 0.6886953974383626
iteration 0 batch 5060 trainingloss 0.6886953974383626
iteration 0 batch 5070 trainingloss 0.6901793251455568
iteration 0 batch 5080 trainingloss 0.6901793251455568
iteration 0 batch 5090 trainingloss 0.6901793251455568
iteration 0 batch 5100 trainingloss 0.6916632528527511
iteration 0 batch 5110 trainingloss 0.6931471805599453
iteration 0 batch 5120 trainingloss 0.6901793251455568
iteration 0 batch 5130 trainingloss 0.6916632528527511
iteration 0 batch 5140 trainingloss 0.6916632528527511
iteration 0 batch 5150 trainingloss 0.6901793251455568
iteration 0 batch 5160 trainingloss 0.6931471805599453
iteration 0 batch 5170 trainingloss 0.6916632528527511
iteration 0 batch 5180 trainingloss 0.6916632528527511
iteration 0 batch 5190 trainingloss 0.6886953974383626
iteration 0 batch 5200 trainingloss 0.6931471805599453
iteration 0 batch 5210 trainingloss 0.6916632528527511
iteration 0 batch 5220 trainingloss 0.6901793251455568
iteration 0 batch 5230 trainingloss 0.6886953974383626
iteration 0 batch 5240 trainingloss 0.6931471805599453
iteration 0 batch 5250 trainingloss 0.6901793251455568
iteration 0 batch 5260 trainingloss 0.6916632528527511
iteration 0 batch 5270 trainingloss 0.6931471805599453
iteration 0 batch 5280 trainingloss 0.6916632528527511
iteration 0 batch 5290 trainingloss 0.6916632528527511
iteration 0 batch 5300 trainingloss 0.6931471805599453
iteration 0 batch 5310 trainingloss 0.6931471805599453
iteration 0 batch 5320 trainingloss 0.6916632528527511
iteration 0 batch 5330 trainingloss 0.6931471805599453
iteration 0 batch 5340 trainingloss 0.6901793251455568
iteration 0 batch 5350 trainingloss 0.6916632528527511
iteration 0 batch 5360 trainingloss 0.6901793251455568
iteration 0 batch 5370 trainingloss 0.6901793251455568
iteration 0 batch 5380 trainingloss 0.6931471805599453
iteration 0 batch 5390 trainingloss 0.6931471805599453
iteration 0 batch 5400 trainingloss 0.6916632528527511
iteration 0 batch 5410 trainingloss 0.6916632528527511
iteration 0 batch 5420 trainingloss 0.6931471805599453
iteration 0 batch 5430 trainingloss 0.6931471805599453
iteration 0 batch 5440 trainingloss 0.6916632528527511
iteration 0 batch 5450 trainingloss 0.6931471805599453
iteration 0 batch 5460 trainingloss 0.6931471805599453
iteration 0 batch 5470 trainingloss 0.6886953974383626
iteration 0 batch 5480 trainingloss 0.6901793251455568
iteration 0 batch 5490 trainingloss 0.6901793251455568
iteration 0 batch 5500 trainingloss 0.6916632528527511
iteration 0 batch 5510 trainingloss 0.6931471805599453
iteration 0 batch 5520 trainingloss 0.6916632528527511
iteration 0 batch 5530 trainingloss 0.6886953974383626
iteration 0 batch 5540 trainingloss 0.6931471805599453
iteration 0 batch 5550 trainingloss 0.6916632528527511
iteration 0 batch 5560 trainingloss 0.6886953974383626
iteration 0 batch 5570 trainingloss 0.6931471805599453
iteration 0 batch 5580 trainingloss 0.6916632528527511
iteration 0 batch 5590 trainingloss 0.6931471805599453
iteration 0 batch 5600 trainingloss 0.6916632528527511
iteration 0 batch 5610 trainingloss 0.6931471805599453
iteration 0 batch 5620 trainingloss 0.6931471805599453
iteration 0 batch 5630 trainingloss 0.6931471805599453
iteration 0 batch 5640 trainingloss 0.6931471805599453
iteration 0 batch 5650 trainingloss 0.6901793251455568
iteration 0 batch 5660 trainingloss 0.6916632528527511
iteration 0 batch 5670 trainingloss 0.6931471805599453
iteration 0 batch 5680 trainingloss 0.6931471805599453
iteration 0 batch 5690 trainingloss 0.6901793251455568
iteration 0 batch 5700 trainingloss 0.6916632528527511
iteration 0 batch 5710 trainingloss 0.6931471805599453
iteration 0 batch 5720 trainingloss 0.6916632528527511
iteration 0 batch 5730 trainingloss 0.6931471805599453
iteration 0 batch 5740 trainingloss 0.6901793251455568
iteration 0 batch 5750 trainingloss 0.6916632528527511
iteration 0 batch 5760 trainingloss 0.6931471805599453
iteration 0 batch 5770 trainingloss 0.6916632528527511
iteration 0 batch 5780 trainingloss 0.6916632528527511
iteration 0 batch 5790 trainingloss 0.6916632528527511
iteration 0 batch 5800 trainingloss 0.6886953974383626
iteration 0 batch 5810 trainingloss 0.6916632528527511
iteration 0 batch 5820 trainingloss 0.6931471805599453
iteration 0 batch 5830 trainingloss 0.6931471805599453
iteration 0 batch 5840 trainingloss 0.6901793251455568
iteration 0 batch 5850 trainingloss 0.6931471805599453
iteration 0 batch 5860 trainingloss 0.6916632528527511
iteration 0 batch 5870 trainingloss 0.6931471805599453
iteration 0 batch 5880 trainingloss 0.6931471805599453
iteration 0 batch 5890 trainingloss 0.6931471805599453
iteration 0 batch 5900 trainingloss 0.6931471805599453
iteration 0 batch 5910 trainingloss 0.6931471805599453
iteration 0 batch 5920 trainingloss 0.6931471805599453
iteration 0 batch 5930 trainingloss 0.6916632528527511
iteration 0 batch 5940 trainingloss 0.6931471805599453
iteration 0 batch 5950 trainingloss 0.6886953974383626
iteration 0 batch 5960 trainingloss 0.6886953974383626
iteration 0 batch 5970 trainingloss 0.6931471805599453
iteration 0 batch 5980 trainingloss 0.6931471805599453
iteration 0 batch 5990 trainingloss 0.6931471805599453
iteration 0 batch 6000 trainingloss 0.6901793251455568
iteration 0 batch 6010 trainingloss 0.6916632528527511
iteration 0 batch 6020 trainingloss 0.6931471805599453
iteration 0 batch 6030 trainingloss 0.6931471805599453
iteration 0 batch 6040 trainingloss 0.6931471805599453
iteration 0 batch 6050 trainingloss 0.6931471805599453
iteration 0 batch 6060 trainingloss 0.6931471805599453
iteration 0 batch 6070 trainingloss 0.6931471805599453
iteration 0 batch 6080 trainingloss 0.6916632528527511
iteration 0 batch 6090 trainingloss 0.6916632528527511
iteration 0 batch 6100 trainingloss 0.6901793251455568
iteration 0 batch 6110 trainingloss 0.6931471805599453
iteration 0 batch 6120 trainingloss 0.6931471805599453
iteration 0 batch 6130 trainingloss 0.6931471805599453
iteration 0 batch 6140 trainingloss 0.6931471805599453
iteration 0 batch 6150 trainingloss 0.6931471805599453
iteration 0 batch 6160 trainingloss 0.6916632528527511
iteration 0 batch 6170 trainingloss 0.6901793251455568
iteration 0 batch 6180 trainingloss 0.6916632528527511
iteration 0 batch 6190 trainingloss 0.6931471805599453
iteration 0 batch 6200 trainingloss 0.6931471805599453
iteration 0 batch 6210 trainingloss 0.6931471805599453
iteration 0 batch 6220 trainingloss 0.6916632528527511
iteration 0 batch 6230 trainingloss 0.6916632528527511
iteration 0 batch 6240 trainingloss 0.6886953974383626
iteration 0 batch 6250 trainingloss 0.6931471805599453
iteration 0 batch 6260 trainingloss 0.6931471805599453
iteration 0 batch 6270 trainingloss 0.6931471805599453
iteration 0 batch 6280 trainingloss 0.6916632528527511
iteration 0 batch 6290 trainingloss 0.6901793251455568
iteration 0 batch 6300 trainingloss 0.6931471805599453
iteration 0 batch 6310 trainingloss 0.6886953974383626
iteration 0 batch 6320 trainingloss 0.6916632528527511
iteration 0 batch 6330 trainingloss 0.6931471805599453
iteration 0 batch 6340 trainingloss 0.6916632528527511
iteration 0 batch 6350 trainingloss 0.6931471805599453
iteration 0 batch 6360 trainingloss 0.6916632528527511
iteration 0 batch 6370 trainingloss 0.6916632528527511
iteration 0 batch 6380 trainingloss 0.6916632528527511
iteration 0 batch 6390 trainingloss 0.6931471805599453
iteration 0 batch 6400 trainingloss 0.6916632528527511
iteration 0 batch 6410 trainingloss 0.6901793251455568
iteration 0 batch 6420 trainingloss 0.6931471805599453
iteration 0 batch 6430 trainingloss 0.6931471805599453
iteration 0 batch 6440 trainingloss 0.6886953974383626
iteration 0 batch 6450 trainingloss 0.6931471805599453
iteration 0 batch 6460 trainingloss 0.6931471805599453
iteration 0 batch 6470 trainingloss 0.6916632528527511
iteration 0 batch 6480 trainingloss 0.6901793251455568
iteration 0 batch 6490 trainingloss 0.6931471805599453
iteration 0 batch 6500 trainingloss 0.6916632528527511
iteration 0 batch 6510 trainingloss 0.6931471805599453
iteration 0 batch 6520 trainingloss 0.6931471805599453
iteration 0 batch 6530 trainingloss 0.6901793251455568
iteration 0 batch 6540 trainingloss 0.6901793251455568
iteration 0 batch 6550 trainingloss 0.6916632528527511
iteration 0 batch 6560 trainingloss 0.6901793251455568
iteration 0 batch 6570 trainingloss 0.6931471805599453
iteration 0 batch 6580 trainingloss 0.6931471805599453
iteration 0 batch 6590 trainingloss 0.6916632528527511
iteration 0 batch 6600 trainingloss 0.6931471805599453
iteration 0 batch 6610 trainingloss 0.6931471805599453
iteration 0 batch 6620 trainingloss 0.6931471805599453
iteration 0 batch 6630 trainingloss 0.6931471805599453
iteration 0 batch 6640 trainingloss 0.6931471805599453
iteration 0 batch 6650 trainingloss 0.6931471805599453
iteration 0 batch 6660 trainingloss 0.6916632528527511
iteration 0 batch 6670 trainingloss 0.6931471805599453
iteration 0 batch 6680 trainingloss 0.6901793251455568
iteration 0 batch 6690 trainingloss 0.6931471805599453
iteration 0 batch 6700 trainingloss 0.6931471805599453
iteration 0 batch 6710 trainingloss 0.6931471805599453
iteration 0 batch 6720 trainingloss 0.6886953974383626
iteration 0 batch 6730 trainingloss 0.6872114697311684
iteration 0 batch 6740 trainingloss 0.6901793251455568
iteration 0 batch 6750 trainingloss 0.6931471805599453
iteration 0 batch 6760 trainingloss 0.6931471805599453
iteration 0 batch 6770 trainingloss 0.6886953974383626
iteration 0 batch 6780 trainingloss 0.6916632528527511
iteration 0 batch 6790 trainingloss 0.6931471805599453
iteration 0 batch 6800 trainingloss 0.6931471805599453
iteration 0 batch 6810 trainingloss 0.6916632528527511
iteration 0 batch 6820 trainingloss 0.6931471805599453
iteration 0 batch 6830 trainingloss 0.6916632528527511
iteration 0 batch 6840 trainingloss 0.6931471805599453
iteration 0 batch 6850 trainingloss 0.6916632528527511
iteration 0 batch 6860 trainingloss 0.6931471805599453
iteration 0 batch 6870 trainingloss 0.6931471805599453
iteration 0 batch 6880 trainingloss 0.6916632528527511
iteration 0 batch 6890 trainingloss 0.6931471805599453
iteration 0 batch 6900 trainingloss 0.6931471805599453
iteration 0 batch 6910 trainingloss 0.6886953974383626
iteration 0 batch 6920 trainingloss 0.6931471805599453
iteration 0 batch 6930 trainingloss 0.6916632528527511
iteration 0 batch 6940 trainingloss 0.6886953974383626
iteration 0 batch 6950 trainingloss 0.6916632528527511
iteration 0 batch 6960 trainingloss 0.6901793251455568
iteration 0 batch 6970 trainingloss 0.6901793251455568
iteration 0 batch 6980 trainingloss 0.6931471805599453
iteration 0 batch 6990 trainingloss 0.6916632528527511
iteration 0 batch 7000 trainingloss 0.6931471805599453
iteration 0 batch 7010 trainingloss 0.6916632528527511
iteration 0 batch 7020 trainingloss 0.6916632528527511
iteration 0 batch 7030 trainingloss 0.6916632528527511
iteration 0 batch 7040 trainingloss 0.6916632528527511
iteration 0 batch 7050 trainingloss 0.6916632528527511
iteration 0 batch 7060 trainingloss 0.6931471805599453
iteration 0 batch 7070 trainingloss 0.6931471805599453
iteration 0 batch 7080 trainingloss 0.6931471805599453
iteration 0 batch 7090 trainingloss 0.6916632528527511
iteration 0 batch 7100 trainingloss 0.6931471805599453
iteration 0 batch 7110 trainingloss 0.6901793251455568
iteration 0 batch 7120 trainingloss 0.6931471805599453
iteration 0 batch 7130 trainingloss 0.6901793251455568
iteration 0 batch 7140 trainingloss 0.6916632528527511
iteration 0 batch 7150 trainingloss 0.6931471805599453
iteration 0 batch 7160 trainingloss 0.6931471805599453
iteration 0 batch 7170 trainingloss 0.6931471805599453
iteration 0 batch 7180 trainingloss 0.6931471805599453
iteration 0 batch 7190 trainingloss 0.6916632528527511
iteration 0 batch 7200 trainingloss 0.6916632528527511
iteration 0 batch 7210 trainingloss 0.6931471805599453
iteration 0 batch 7220 trainingloss 0.6931471805599453
iteration 0 batch 7230 trainingloss 0.6931471805599453
iteration 0 batch 7240 trainingloss 0.6916632528527511
iteration 0 batch 7250 trainingloss 0.6901793251455568
iteration 0 batch 7260 trainingloss 0.6901793251455568
iteration 0 batch 7270 trainingloss 0.6916632528527511
iteration 0 batch 7280 trainingloss 0.6931471805599453
iteration 0 batch 7290 trainingloss 0.6931471805599453
iteration 0 batch 7300 trainingloss 0.6916632528527511
iteration 0 batch 7310 trainingloss 0.6931471805599453
iteration 0 batch 7320 trainingloss 0.6842436143167799
iteration 0 batch 7330 trainingloss 0.6916632528527511
iteration 0 batch 7340 trainingloss 0.6931471805599453
iteration 0 batch 7350 trainingloss 0.6931471805599453
iteration 0 batch 7360 trainingloss 0.6931471805599453
iteration 0 batch 7370 trainingloss 0.6872114697311684
iteration 0 batch 7380 trainingloss 0.6916632528527511
iteration 0 batch 7390 trainingloss 0.6931471805599453
iteration 0 batch 7400 trainingloss 0.6931471805599453
iteration 0 batch 7410 trainingloss 0.6916632528527511
iteration 0 batch 7420 trainingloss 0.6916632528527511
iteration 0 batch 7430 trainingloss 0.6931471805599453
iteration 0 batch 7440 trainingloss 0.6916632528527511
iteration 0 batch 7450 trainingloss 0.6916632528527511
iteration 0 batch 7460 trainingloss 0.6916632528527511
iteration 0 batch 7470 trainingloss 0.6916632528527511
iteration 0 batch 7480 trainingloss 0.6931471805599453
iteration 0 batch 7490 trainingloss 0.6931471805599453
iteration 0 batch 7500 trainingloss 0.6916632528527511
iteration 0 batch 7510 trainingloss 0.6931471805599453
iteration 0 batch 7520 trainingloss 0.6931471805599453
iteration 0 batch 7530 trainingloss 0.6916632528527511
iteration 0 batch 7540 trainingloss 0.6931471805599453
iteration 0 batch 7550 trainingloss 0.6931471805599453
iteration 0 batch 7560 trainingloss 0.6916632528527511
iteration 0 batch 7570 trainingloss 0.6931471805599453
iteration 0 batch 7580 trainingloss 0.6931471805599453
iteration 0 batch 7590 trainingloss 0.6931471805599453
iteration 0 batch 7600 trainingloss 0.6931471805599453
iteration 0 batch 7610 trainingloss 0.6931471805599453
iteration 0 batch 7620 trainingloss 0.6931471805599453
iteration 0 batch 7630 trainingloss 0.6916632528527511
iteration 0 batch 7640 trainingloss 0.6931471805599453
iteration 0 batch 7650 trainingloss 0.6931471805599453
iteration 0 batch 7660 trainingloss 0.6916632528527511
iteration 0 batch 7670 trainingloss 0.6916632528527511
iteration 0 batch 7680 trainingloss 0.6931471805599453
iteration 0 batch 7690 trainingloss 0.6916632528527511
iteration 0 batch 7700 trainingloss 0.6916632528527511
iteration 0 batch 7710 trainingloss 0.6931471805599453
iteration 0 batch 7720 trainingloss 0.6916632528527511
iteration 0 batch 7730 trainingloss 0.6931471805599453
iteration 0 batch 7740 trainingloss 0.6916632528527511
iteration 0 batch 7750 trainingloss 0.6916632528527511
iteration 0 batch 7760 trainingloss 0.6901793251455568
iteration 0 batch 7770 trainingloss 0.6931471805599453
iteration 0 batch 7780 trainingloss 0.6916632528527511
iteration 0 batch 7790 trainingloss 0.6916632528527511
iteration 0 batch 7800 trainingloss 0.6916632528527511
iteration 0 batch 7810 trainingloss 0.6931471805599453
iteration 0 batch 7820 trainingloss 0.6916632528527511
iteration 0 batch 7830 trainingloss 0.6901793251455568
iteration 0 batch 7840 trainingloss 0.6916632528527511
iteration 0 batch 7850 trainingloss 0.6931471805599453
iteration 0 batch 7860 trainingloss 0.6931471805599453
iteration 0 batch 7870 trainingloss 0.6931471805599453
iteration 0 batch 7880 trainingloss 0.6931471805599453
iteration 0 batch 7890 trainingloss 0.6931471805599453
iteration 0 batch 7900 trainingloss 0.6931471805599453
iteration 0 batch 7910 trainingloss 0.6931471805599453
iteration 0 batch 7920 trainingloss 0.6931471805599453
iteration 0 batch 7930 trainingloss 0.6916632528527511
iteration 0 batch 7940 trainingloss 0.6931471805599453
iteration 0 batch 7950 trainingloss 0.6931471805599453
iteration 0 batch 7960 trainingloss 0.6931471805599453
iteration 0 batch 7970 trainingloss 0.6931471805599453
iteration 0 batch 7980 trainingloss 0.6931471805599453
iteration 0 batch 7990 trainingloss 0.6916632528527511
iteration 0 batch 8000 trainingloss 0.6931471805599453
iteration 0 batch 8010 trainingloss 0.6916632528527511
iteration 0 batch 8020 trainingloss 0.6931471805599453
iteration 0 batch 8030 trainingloss 0.6916632528527511
iteration 0 batch 8040 trainingloss 0.6901793251455568
iteration 0 batch 8050 trainingloss 0.6916632528527511
iteration 0 batch 8060 trainingloss 0.6916632528527511
iteration 0 batch 8070 trainingloss 0.6916632528527511
iteration 0 batch 8080 trainingloss 0.6901793251455568
iteration 0 batch 8090 trainingloss 0.6931471805599453
iteration 0 batch 8100 trainingloss 0.6916632528527511
iteration 0 batch 8110 trainingloss 0.6931471805599453
iteration 0 batch 8120 trainingloss 0.6931471805599453
iteration 0 batch 8130 trainingloss 0.6931471805599453
iteration 0 batch 8140 trainingloss 0.6931471805599453
iteration 0 batch 8150 trainingloss 0.6886953974383626
iteration 0 batch 8160 trainingloss 0.6931471805599453
iteration 0 batch 8170 trainingloss 0.6931471805599453
iteration 0 batch 8180 trainingloss 0.6931471805599453
iteration 0 batch 8190 trainingloss 0.6916632528527511
iteration 0 batch 8200 trainingloss 0.6916632528527511
iteration 0 batch 8210 trainingloss 0.6931471805599453
iteration 0 batch 8220 trainingloss 0.6916632528527511
iteration 0 batch 8230 trainingloss 0.6931471805599453
iteration 0 batch 8240 trainingloss 0.6916632528527511
iteration 0 batch 8250 trainingloss 0.6901793251455568
iteration 0 batch 8260 trainingloss 0.6931471805599453
iteration 0 batch 8270 trainingloss 0.6916632528527511
iteration 0 batch 8280 trainingloss 0.6931471805599453
iteration 0 batch 8290 trainingloss 0.6901793251455568
iteration 0 batch 8300 trainingloss 0.6916632528527511
iteration 0 batch 8310 trainingloss 0.6931471805599453
iteration 0 batch 8320 trainingloss 0.6916632528527511
iteration 0 batch 8330 trainingloss 0.6931471805599453
iteration 0 batch 8340 trainingloss 0.6916632528527511
iteration 0 batch 8350 trainingloss 0.6931471805599453
iteration 0 batch 8360 trainingloss 0.6916632528527511
iteration 0 batch 8370 trainingloss 0.6901793251455568
iteration 0 batch 8380 trainingloss 0.6901793251455568
iteration 0 batch 8390 trainingloss 0.6931471805599453
iteration 0 batch 8400 trainingloss 0.6931471805599453
iteration 0 batch 8410 trainingloss 0.6916632528527511
iteration 0 batch 8420 trainingloss 0.6931471805599453
iteration 0 batch 8430 trainingloss 0.6931471805599453
iteration 0 batch 8440 trainingloss 0.6916632528527511
iteration 0 batch 8450 trainingloss 0.6931471805599453
iteration 0 batch 8460 trainingloss 0.6916632528527511
iteration 0 batch 8470 trainingloss 0.6916632528527511
iteration 0 batch 8480 trainingloss 0.6931471805599453
iteration 0 batch 8490 trainingloss 0.6931471805599453
iteration 0 batch 8500 trainingloss 0.6886953974383626
iteration 0 batch 8510 trainingloss 0.6931471805599453
iteration 0 batch 8520 trainingloss 0.6901793251455568
iteration 0 batch 8530 trainingloss 0.6916632528527511
iteration 0 batch 8540 trainingloss 0.6931471805599453
iteration 0 batch 8550 trainingloss 0.6931471805599453
iteration 0 batch 8560 trainingloss 0.6916632528527511
iteration 0 batch 8570 trainingloss 0.6931471805599453
iteration 0 batch 8580 trainingloss 0.6901793251455568
iteration 0 batch 8590 trainingloss 0.6931471805599453
iteration 0 batch 8600 trainingloss 0.6901793251455568
iteration 0 batch 8610 trainingloss 0.6901793251455568
iteration 0 batch 8620 trainingloss 0.6916632528527511
iteration 0 batch 8630 trainingloss 0.6931471805599453
iteration 0 batch 8640 trainingloss 0.6931471805599453
iteration 0 batch 8650 trainingloss 0.6931471805599453
iteration 0 batch 8660 trainingloss 0.6931471805599453
iteration 0 batch 8670 trainingloss 0.6916632528527511
iteration 0 batch 8680 trainingloss 0.6916632528527511
iteration 0 batch 8690 trainingloss 0.6931471805599453
iteration 0 batch 8700 trainingloss 0.6901793251455568
iteration 0 batch 8710 trainingloss 0.6931471805599453
iteration 0 batch 8720 trainingloss 0.6931471805599453
iteration 0 batch 8730 trainingloss 0.6931471805599453
iteration 0 batch 8740 trainingloss 0.6931471805599453
iteration 0 batch 8750 trainingloss 0.6931471805599453
iteration 0 batch 8760 trainingloss 0.6916632528527511
iteration 0 batch 8770 trainingloss 0.6916632528527511
iteration 0 batch 8780 trainingloss 0.6931471805599453
iteration 0 batch 8790 trainingloss 0.6931471805599453
iteration 0 batch 8800 trainingloss 0.6931471805599453
iteration 0 batch 8810 trainingloss 0.6931471805599453
iteration 0 batch 8820 trainingloss 0.6916632528527511
iteration 0 batch 8830 trainingloss 0.6916632528527511
iteration 0 batch 8840 trainingloss 0.6931471805599453
iteration 0 batch 8850 trainingloss 0.6931471805599453
iteration 0 batch 8860 trainingloss 0.6901793251455568
iteration 0 batch 8870 trainingloss 0.6901793251455568
iteration 0 batch 8880 trainingloss 0.6916632528527511
iteration 0 batch 8890 trainingloss 0.6931471805599453
iteration 0 batch 8900 trainingloss 0.6931471805599453
iteration 0 batch 8910 trainingloss 0.6916632528527511
iteration 0 batch 8920 trainingloss 0.6931471805599453
iteration 0 batch 8930 trainingloss 0.6931471805599453
iteration 0 batch 8940 trainingloss 0.6931471805599453
iteration 0 batch 8950 trainingloss 0.6931471805599453
iteration 0 batch 8960 trainingloss 0.6931471805599453
iteration 0 batch 8970 trainingloss 0.6931471805599453
iteration 0 batch 8980 trainingloss 0.6931471805599453
iteration 0 batch 8990 trainingloss 0.6931471805599453
iteration 0 batch 9000 trainingloss 0.6916632528527511
iteration 0 batch 9010 trainingloss 0.6931471805599453
iteration 0 batch 9020 trainingloss 0.6931471805599453
iteration 0 batch 9030 trainingloss 0.6916632528527511
iteration 0 batch 9040 trainingloss 0.6931471805599453
iteration 0 batch 9050 trainingloss 0.6931471805599453
iteration 0 batch 9060 trainingloss 0.6931471805599453
iteration 0 batch 9070 trainingloss 0.6931471805599453
iteration 0 batch 9080 trainingloss 0.6931471805599453
iteration 0 batch 9090 trainingloss 0.6916632528527511
iteration 0 batch 9100 trainingloss 0.6916632528527511
iteration 0 batch 9110 trainingloss 0.6901793251455568
iteration 0 batch 9120 trainingloss 0.6931471805599453
iteration 0 batch 9130 trainingloss 0.6931471805599453
iteration 0 batch 9140 trainingloss 0.6901793251455568
iteration 0 batch 9150 trainingloss 0.6916632528527511
iteration 0 batch 9160 trainingloss 0.6931471805599453
iteration 0 batch 9170 trainingloss 0.6931471805599453
iteration 0 batch 9180 trainingloss 0.6931471805599453
iteration 0 batch 9190 trainingloss 0.6901793251455568
iteration 0 batch 9200 trainingloss 0.6931471805599453
iteration 0 batch 9210 trainingloss 0.6931471805599453
iteration 0 batch 9220 trainingloss 0.6901793251455568
iteration 0 batch 9230 trainingloss 0.6901793251455568
iteration 0 batch 9240 trainingloss 0.6931471805599453
iteration 0 batch 9250 trainingloss 0.6916632528527511
iteration 0 batch 9260 trainingloss 0.6916632528527511
iteration 0 batch 9270 trainingloss 0.6931471805599453
iteration 0 batch 9280 trainingloss 0.6931471805599453
iteration 0 batch 9290 trainingloss 0.6931471805599453
iteration 0 batch 9300 trainingloss 0.6931471805599453
iteration 0 batch 9310 trainingloss 0.6931471805599453
iteration 0 batch 9320 trainingloss 0.6931471805599453
iteration 0 batch 9330 trainingloss 0.6916632528527511
iteration 0 batch 9340 trainingloss 0.6931471805599453
iteration 0 batch 9350 trainingloss 0.6931471805599453
iteration 0 batch 9360 trainingloss 0.6931471805599453
iteration 0 batch 9370 trainingloss 0.6901793251455568
iteration 0 batch 9380 trainingloss 0.6901793251455568
iteration 0 batch 9390 trainingloss 0.6931471805599453
iteration 0 batch 9400 trainingloss 0.6901793251455568
iteration 0 batch 9410 trainingloss 0.6916632528527511
iteration 0 batch 9420 trainingloss 0.6931471805599453
iteration 0 batch 9430 trainingloss 0.6931471805599453
iteration 0 batch 9440 trainingloss 0.6901793251455568
iteration 0 batch 9450 trainingloss 0.6916632528527511
iteration 0 batch 9460 trainingloss 0.6931471805599453
iteration 0 batch 9470 trainingloss 0.6931471805599453
iteration 0 batch 9480 trainingloss 0.6916632528527511
iteration 0 batch 9490 trainingloss 0.6931471805599453
iteration 0 batch 9500 trainingloss 0.6931471805599453
iteration 0 batch 9510 trainingloss 0.6931471805599453
iteration 0 batch 9520 trainingloss 0.6931471805599453
iteration 0 batch 9530 trainingloss 0.6931471805599453
iteration 0 batch 9540 trainingloss 0.6931471805599453
iteration 0 batch 9550 trainingloss 0.6916632528527511
iteration 0 batch 9560 trainingloss 0.6916632528527511
iteration 0 batch 9570 trainingloss 0.6886953974383626
iteration 0 batch 9580 trainingloss 0.6931471805599453
iteration 0 batch 9590 trainingloss 0.6931471805599453
iteration 0 batch 9600 trainingloss 0.6931471805599453
iteration 0 batch 9610 trainingloss 0.6916632528527511
iteration 0 batch 9620 trainingloss 0.6931471805599453
iteration 0 batch 9630 trainingloss 0.6931471805599453
iteration 0 batch 9640 trainingloss 0.6901793251455568
iteration 0 batch 9650 trainingloss 0.6931471805599453
iteration 0 batch 9660 trainingloss 0.6901793251455568
iteration 0 batch 9670 trainingloss 0.6931471805599453
iteration 0 batch 9680 trainingloss 0.6931471805599453
iteration 0 batch 9690 trainingloss 0.6901793251455568
iteration 0 batch 9700 trainingloss 0.6931471805599453
iteration 0 batch 9710 trainingloss 0.6901793251455568
iteration 0 batch 9720 trainingloss 0.6931471805599453
iteration 0 batch 9730 trainingloss 0.6872114697311684
iteration 0 batch 9740 trainingloss 0.6916632528527511
iteration 0 batch 9750 trainingloss 0.6931471805599453
iteration 0 batch 9760 trainingloss 0.6931471805599453
iteration 0 batch 9770 trainingloss 0.6916632528527511
iteration 0 batch 9780 trainingloss 0.6916632528527511
iteration 0 batch 9790 trainingloss 0.6931471805599453
iteration 0 batch 9800 trainingloss 0.6931471805599453
iteration 0 batch 9810 trainingloss 0.6931471805599453
iteration 0 batch 9820 trainingloss 0.6931471805599453
iteration 0 batch 9830 trainingloss 0.6931471805599453
iteration 0 batch 9840 trainingloss 0.6916632528527511
iteration 0 batch 9850 trainingloss 0.6916632528527511
iteration 0 batch 9860 trainingloss 0.6931471805599453
iteration 0 batch 9870 trainingloss 0.6931471805599453
iteration 0 batch 9880 trainingloss 0.6886953974383626
iteration 0 batch 9890 trainingloss 0.6931471805599453
iteration 0 batch 9900 trainingloss 0.6916632528527511
iteration 0 batch 9910 trainingloss 0.6931471805599453
iteration 0 batch 9920 trainingloss 0.6931471805599453
iteration 0 batch 9930 trainingloss 0.6931471805599453
iteration 0 batch 9940 trainingloss 0.6931471805599453
iteration 0 batch 9950 trainingloss 0.6931471805599453
iteration 0 batch 9960 trainingloss 0.6931471805599453
iteration 0 batch 9970 trainingloss 0.6916632528527511
iteration 0 batch 9980 trainingloss 0.6931471805599453
iteration 0 batch 9990 trainingloss 0.6931471805599453
iteration 0 batch 10000 trainingloss 0.6931471805599453
iteration 0 batch 10010 trainingloss 0.6931471805599453
iteration 0 batch 10020 trainingloss 0.6931471805599453
iteration 0 batch 10030 trainingloss 0.6916632528527511
iteration 0 batch 10040 trainingloss 0.6931471805599453
iteration 0 batch 10050 trainingloss 0.6931471805599453
iteration 0 batch 10060 trainingloss 0.6931471805599453
iteration 0 batch 10070 trainingloss 0.6931471805599453
iteration 0 batch 10080 trainingloss 0.6931471805599453
iteration 0 batch 10090 trainingloss 0.6931471805599453
iteration 0 batch 10100 trainingloss 0.6931471805599453
iteration 0 batch 10110 trainingloss 0.6901793251455568
iteration 0 batch 10120 trainingloss 0.6931471805599453
iteration 0 batch 10130 trainingloss 0.6931471805599453
iteration 0 batch 10140 trainingloss 0.6901793251455568
iteration 0 batch 10150 trainingloss 0.6916632528527511
iteration 0 batch 10160 trainingloss 0.6901793251455568
iteration 0 batch 10170 trainingloss 0.6931471805599453
iteration 0 batch 10180 trainingloss 0.6916632528527511
iteration 0 batch 10190 trainingloss 0.6916632528527511
iteration 0 batch 10200 trainingloss 0.6916632528527511
iteration 0 batch 10210 trainingloss 0.6916632528527511
iteration 0 batch 10220 trainingloss 0.6886953974383626
iteration 0 batch 10230 trainingloss 0.6916632528527511
iteration 0 batch 10240 trainingloss 0.6931471805599453
iteration 0 batch 10250 trainingloss 0.6916632528527511
iteration 0 batch 10260 trainingloss 0.6931471805599453
iteration 0 batch 10270 trainingloss 0.6916632528527511
iteration 0 batch 10280 trainingloss 0.6901793251455568
iteration 0 batch 10290 trainingloss 0.6886953974383626
iteration 0 batch 10300 trainingloss 0.6916632528527511
iteration 0 batch 10310 trainingloss 0.6916632528527511
iteration 0 batch 10320 trainingloss 0.6931471805599453
iteration 0 batch 10330 trainingloss 0.6931471805599453
iteration 0 batch 10340 trainingloss 0.6916632528527511
iteration 0 batch 10350 trainingloss 0.6916632528527511
iteration 0 batch 10360 trainingloss 0.6916632528527511
iteration 0 batch 10370 trainingloss 0.6901793251455568
iteration 0 batch 10380 trainingloss 0.6916632528527511
iteration 0 batch 10390 trainingloss 0.6916632528527511
iteration 0 batch 10400 trainingloss 0.6931471805599453
iteration 0 batch 10410 trainingloss 0.6916632528527511
iteration 0 batch 10420 trainingloss 0.6931471805599453
iteration 0 batch 10430 trainingloss 0.6916632528527511
iteration 0 batch 10440 trainingloss 0.6916632528527511
iteration 0 batch 10450 trainingloss 0.6931471805599453
iteration 0 batch 10460 trainingloss 0.6931471805599453
iteration 0 batch 10470 trainingloss 0.6916632528527511
iteration 0 batch 10480 trainingloss 0.6901793251455568
iteration 0 batch 10490 trainingloss 0.6931471805599453
iteration 0 batch 10500 trainingloss 0.6916632528527511
iteration 0 batch 10510 trainingloss 0.6931471805599453
iteration 0 batch 10520 trainingloss 0.6931471805599453
iteration 0 batch 10530 trainingloss 0.6916632528527511
iteration 0 batch 10540 trainingloss 0.6931471805599453
iteration 0 batch 10550 trainingloss 0.6916632528527511
iteration 0 batch 10560 trainingloss 0.6916632528527511
iteration 0 batch 10570 trainingloss 0.6931471805599453
iteration 0 batch 10580 trainingloss 0.6916632528527511
iteration 0 batch 10590 trainingloss 0.6931471805599453
iteration 0 batch 10600 trainingloss 0.6931471805599453
iteration 0 batch 10610 trainingloss 0.6931471805599453
iteration 0 batch 10620 trainingloss 0.6931471805599453
iteration 0 batch 10630 trainingloss 0.6901793251455568
iteration 0 batch 10640 trainingloss 0.6916632528527511
iteration 0 batch 10650 trainingloss 0.6931471805599453
iteration 0 batch 10660 trainingloss 0.6916632528527511
iteration 0 batch 10670 trainingloss 0.6931471805599453
iteration 0 batch 10680 trainingloss 0.6931471805599453
iteration 0 batch 10690 trainingloss 0.6931471805599453
iteration 0 batch 10700 trainingloss 0.6931471805599453
iteration 0 batch 10710 trainingloss 0.6916632528527511
iteration 0 batch 10720 trainingloss 0.6916632528527511
iteration 0 batch 10730 trainingloss 0.6931471805599453
iteration 0 batch 10740 trainingloss 0.6931471805599453
iteration 0 batch 10750 trainingloss 0.6931471805599453
iteration 0 batch 10760 trainingloss 0.6931471805599453
iteration 0 batch 10770 trainingloss 0.6931471805599453
iteration 0 batch 10780 trainingloss 0.6931471805599453
iteration 0 batch 10790 trainingloss 0.6931471805599453
iteration 0 batch 10800 trainingloss 0.6916632528527511
iteration 0 batch 10810 trainingloss 0.6931471805599453
iteration 0 batch 10820 trainingloss 0.6916632528527511
iteration 0 batch 10830 trainingloss 0.6916632528527511
iteration 0 batch 10840 trainingloss 0.6931471805599453
iteration 0 batch 10850 trainingloss 0.6916632528527511
iteration 0 batch 10860 trainingloss 0.6886953974383626
iteration 0 batch 10870 trainingloss 0.6931471805599453
iteration 0 batch 10880 trainingloss 0.6916632528527511
iteration 0 batch 10890 trainingloss 0.6931471805599453
iteration 0 batch 10900 trainingloss 0.6886953974383626
iteration 0 batch 10910 trainingloss 0.6916632528527511
iteration 0 batch 10920 trainingloss 0.6916632528527511
iteration 0 batch 10930 trainingloss 0.6931471805599453
iteration 0 batch 10940 trainingloss 0.6931471805599453
iteration 0 batch 10950 trainingloss 0.6931471805599453
iteration 0 batch 10960 trainingloss 0.6931471805599453
iteration 0 batch 10970 trainingloss 0.6931471805599453
iteration 0 batch 10980 trainingloss 0.6916632528527511
iteration 0 batch 10990 trainingloss 0.6916632528527511
iteration 0 batch 11000 trainingloss 0.6931471805599453
iteration 0 batch 11010 trainingloss 0.6931471805599453
iteration 0 batch 11020 trainingloss 0.6931471805599453
iteration 0 batch 11030 trainingloss 0.6931471805599453
iteration 0 batch 11040 trainingloss 0.6931471805599453
iteration 0 batch 11050 trainingloss 0.6901793251455568
iteration 0 batch 11060 trainingloss 0.6931471805599453
iteration 0 batch 11070 trainingloss 0.6916632528527511
iteration 0 batch 11080 trainingloss 0.6901793251455568
iteration 0 batch 11090 trainingloss 0.6916632528527511
iteration 0 batch 11100 trainingloss 0.6931471805599453
iteration 0 batch 11110 trainingloss 0.6916632528527511
iteration 0 batch 11120 trainingloss 0.6916632528527511
iteration 0 batch 11130 trainingloss 0.6916632528527511
iteration 0 batch 11140 trainingloss 0.6931471805599453
iteration 0 batch 11150 trainingloss 0.6931471805599453
iteration 0 batch 11160 trainingloss 0.6931471805599453
iteration 0 batch 11170 trainingloss 0.6916632528527511
iteration 0 batch 11180 trainingloss 0.6916632528527511
iteration 0 batch 11190 trainingloss 0.6901793251455568
iteration 0 batch 11200 trainingloss 0.6931471805599453
iteration 0 batch 11210 trainingloss 0.6916632528527511
iteration 0 batch 11220 trainingloss 0.6916632528527511
iteration 0 batch 11230 trainingloss 0.6931471805599453
iteration 0 batch 11240 trainingloss 0.6931471805599453
iteration 0 batch 11250 trainingloss 0.6931471805599453
iteration 0 batch 11260 trainingloss 0.6931471805599453
iteration 0 batch 11270 trainingloss 0.6931471805599453
iteration 0 batch 11280 trainingloss 0.6931471805599453
iteration 0 batch 11290 trainingloss 0.6931471805599453
iteration 0 batch 11300 trainingloss 0.6901793251455568
iteration 0 batch 11310 trainingloss 0.6901793251455568
iteration 0 batch 11320 trainingloss 0.6916632528527511
iteration 0 batch 11330 trainingloss 0.6931471805599453
iteration 0 batch 11340 trainingloss 0.6931471805599453
iteration 0 batch 11350 trainingloss 0.6916632528527511
iteration 0 batch 11360 trainingloss 0.6931471805599453
iteration 0 batch 11370 trainingloss 0.6931471805599453
iteration 0 batch 11380 trainingloss 0.6931471805599453
iteration 0 batch 11390 trainingloss 0.6901793251455568
iteration 0 batch 11400 trainingloss 0.6931471805599453
iteration 0 batch 11410 trainingloss 0.6901793251455568
iteration 0 batch 11420 trainingloss 0.6931471805599453
iteration 0 batch 11430 trainingloss 0.6916632528527511
iteration 0 batch 11440 trainingloss 0.6916632528527511
iteration 0 batch 11450 trainingloss 0.6916632528527511
iteration 0 batch 11460 trainingloss 0.6916632528527511
iteration 0 batch 11470 trainingloss 0.6931471805599453
iteration 0 batch 11480 trainingloss 0.6931471805599453
iteration 0 batch 11490 trainingloss 0.6931471805599453
iteration 0 batch 11500 trainingloss 0.6931471805599453
iteration 0 batch 11510 trainingloss 0.6931471805599453
iteration 0 batch 11520 trainingloss 0.6916632528527511
iteration 0 batch 11530 trainingloss 0.6931471805599453
iteration 0 batch 11540 trainingloss 0.6931471805599453
iteration 0 batch 11550 trainingloss 0.6916632528527511
iteration 0 batch 11560 trainingloss 0.6916632528527511
iteration 0 batch 11570 trainingloss 0.6931471805599453
iteration 0 batch 11580 trainingloss 0.6931471805599453
iteration 0 batch 11590 trainingloss 0.6916632528527511
iteration 0 batch 11600 trainingloss 0.6916632528527511
iteration 0 batch 11610 trainingloss 0.6931471805599453
iteration 0 batch 11620 trainingloss 0.6931471805599453
iteration 0 batch 11630 trainingloss 0.6931471805599453
iteration 0 batch 11640 trainingloss 0.6931471805599453
iteration 0 batch 11650 trainingloss 0.6901793251455568
iteration 0 batch 11660 trainingloss 0.6931471805599453
iteration 0 batch 11670 trainingloss 0.6931471805599453
iteration 0 batch 11680 trainingloss 0.6916632528527511
iteration 0 batch 11690 trainingloss 0.6916632528527511
iteration 0 batch 11700 trainingloss 0.6931471805599453
iteration 0 batch 11710 trainingloss 0.6916632528527511
iteration 0 batch 11720 trainingloss 0.6931471805599453
iteration 0 batch 11730 trainingloss 0.6916632528527511
iteration 0 batch 11740 trainingloss 0.6931471805599453
iteration 0 batch 11750 trainingloss 0.6901793251455568
iteration 0 batch 11760 trainingloss 0.6916632528527511
iteration 0 batch 11770 trainingloss 0.6931471805599453
iteration 0 batch 11780 trainingloss 0.6916632528527511
iteration 0 batch 11790 trainingloss 0.6931471805599453
iteration 0 batch 11800 trainingloss 0.6916632528527511
iteration 0 batch 11810 trainingloss 0.6931471805599453
iteration 0 batch 11820 trainingloss 0.6916632528527511
iteration 0 batch 11830 trainingloss 0.6931471805599453
iteration 0 batch 11840 trainingloss 0.6931471805599453
iteration 0 batch 11850 trainingloss 0.6931471805599453
iteration 0 batch 11860 trainingloss 0.6931471805599453
iteration 0 batch 11870 trainingloss 0.6886953974383626
iteration 0 batch 11880 trainingloss 0.6931471805599453
iteration 0 batch 11890 trainingloss 0.6931471805599453
iteration 0 batch 11900 trainingloss 0.6931471805599453
iteration 0 batch 11910 trainingloss 0.6931471805599453
iteration 0 batch 11920 trainingloss 0.6931471805599453
iteration 0 batch 11930 trainingloss 0.6931471805599453
iteration 0 batch 11940 trainingloss 0.6931471805599453
iteration 0 batch 11950 trainingloss 0.6916632528527511
iteration 0 batch 11960 trainingloss 0.6916632528527511
iteration 0 batch 11970 trainingloss 0.6916632528527511
iteration 0 batch 11980 trainingloss 0.6916632528527511
iteration 0 batch 11990 trainingloss 0.6916632528527511
iteration 0 batch 12000 trainingloss 0.6916632528527511
iteration 0 batch 12010 trainingloss 0.6901793251455568
iteration 0 batch 12020 trainingloss 0.6931471805599453
iteration 0 batch 12030 trainingloss 0.6916632528527511
iteration 0 batch 12040 trainingloss 0.6916632528527511
iteration 0 batch 12050 trainingloss 0.6931471805599453
iteration 0 batch 12060 trainingloss 0.6901793251455568
iteration 0 batch 12070 trainingloss 0.6931471805599453
iteration 0 batch 12080 trainingloss 0.6916632528527511
iteration 0 batch 12090 trainingloss 0.6931471805599453
iteration 0 batch 12100 trainingloss 0.6931471805599453
iteration 0 batch 12110 trainingloss 0.6931471805599453
iteration 0 batch 12120 trainingloss 0.6931471805599453
iteration 0 batch 12130 trainingloss 0.6916632528527511
iteration 0 batch 12140 trainingloss 0.6931471805599453
iteration 0 batch 12150 trainingloss 0.6931471805599453
iteration 0 batch 12160 trainingloss 0.6916632528527511
iteration 0 batch 12170 trainingloss 0.6931471805599453
iteration 0 batch 12180 trainingloss 0.6931471805599453
iteration 0 batch 12190 trainingloss 0.6931471805599453
iteration 0 batch 12200 trainingloss 0.6916632528527511
iteration 0 batch 12210 trainingloss 0.6901793251455568
iteration 0 batch 12220 trainingloss 0.6886953974383626
iteration 0 batch 12230 trainingloss 0.6931471805599453
iteration 0 batch 12240 trainingloss 0.6916632528527511
iteration 0 batch 12250 trainingloss 0.6901793251455568
iteration 0 batch 12260 trainingloss 0.6916632528527511
iteration 0 batch 12270 trainingloss 0.6931471805599453
iteration 0 batch 12280 trainingloss 0.6931471805599453
iteration 0 batch 12290 trainingloss 0.6931471805599453
iteration 0 batch 12300 trainingloss 0.6931471805599453
iteration 0 batch 12310 trainingloss 0.6931471805599453
iteration 0 batch 12320 trainingloss 0.6916632528527511
iteration 0 batch 12330 trainingloss 0.6916632528527511
iteration 0 batch 12340 trainingloss 0.6931471805599453
iteration 0 batch 12350 trainingloss 0.6931471805599453
iteration 0 batch 12360 trainingloss 0.6931471805599453
iteration 0 batch 12370 trainingloss 0.6901793251455568
iteration 0 batch 12380 trainingloss 0.6916632528527511
iteration 0 batch 12390 trainingloss 0.6931471805599453
iteration 0 batch 12400 trainingloss 0.6916632528527511
iteration 0 batch 12410 trainingloss 0.6916632528527511
iteration 0 batch 12420 trainingloss 0.6931471805599453
iteration 0 batch 12430 trainingloss 0.6916632528527511
iteration 0 batch 12440 trainingloss 0.6931471805599453
iteration 0 batch 12450 trainingloss 0.6931471805599453
iteration 0 batch 12460 trainingloss 0.6931471805599453
iteration 0 batch 12470 trainingloss 0.6931471805599453
iteration 0 batch 12480 trainingloss 0.6931471805599453
iteration 0 batch 12490 trainingloss 0.6916632528527511
iteration 0 batch 12500 trainingloss 0.6931471805599453
iteration 0 batch 12510 trainingloss 0.6916632528527511
iteration 0 batch 12520 trainingloss 0.6931471805599453
iteration 0 batch 12530 trainingloss 0.6931471805599453
iteration 0 batch 12540 trainingloss 0.6931471805599453
iteration 0 batch 12550 trainingloss 0.6931471805599453
iteration 0 batch 12560 trainingloss 0.6931471805599453
iteration 0 batch 12570 trainingloss 0.6931471805599453
iteration 0 batch 12580 trainingloss 0.6931471805599453
iteration 0 batch 12590 trainingloss 0.6931471805599453
iteration 0 batch 12600 trainingloss 0.6931471805599453
iteration 0 batch 12610 trainingloss 0.6931471805599453
iteration 0 batch 12620 trainingloss 0.6931471805599453
iteration 0 batch 12630 trainingloss 0.6931471805599453
iteration 0 batch 12640 trainingloss 0.6931471805599453
iteration 0 batch 12650 trainingloss 0.6931471805599453
iteration 0 batch 12660 trainingloss 0.6931471805599453
iteration 0 batch 12670 trainingloss 0.6931471805599453
iteration 0 batch 12680 trainingloss 0.6916632528527511
iteration 0 batch 12690 trainingloss 0.6916632528527511
iteration 0 batch 12700 trainingloss 0.6931471805599453
iteration 0 batch 12710 trainingloss 0.6931471805599453
iteration 0 batch 12720 trainingloss 0.6931471805599453
iteration 0 batch 12730 trainingloss 0.6931471805599453
iteration 0 batch 12740 trainingloss 0.6931471805599453
iteration 0 batch 12750 trainingloss 0.6931471805599453
iteration 0 batch 12760 trainingloss 0.6901793251455568
iteration 0 batch 12770 trainingloss 0.6916632528527511
iteration 0 batch 12780 trainingloss 0.6931471805599453
iteration 0 batch 12790 trainingloss 0.6931471805599453
iteration 0 batch 12800 trainingloss 0.6916632528527511
iteration 0 batch 12810 trainingloss 0.6931471805599453
iteration 0 batch 12820 trainingloss 0.6931471805599453
iteration 0 batch 12830 trainingloss 0.6931471805599453
iteration 0 batch 12840 trainingloss 0.6931471805599453
iteration 0 batch 12850 trainingloss 0.6931471805599453
iteration 0 batch 12860 trainingloss 0.6931471805599453
iteration 0 batch 12870 trainingloss 0.6901793251455568
iteration 0 batch 12880 trainingloss 0.6931471805599453
iteration 0 batch 12890 trainingloss 0.6931471805599453
iteration 0 batch 12900 trainingloss 0.6916632528527511
iteration 0 batch 12910 trainingloss 0.6931471805599453
iteration 0 batch 12920 trainingloss 0.6916632528527511
iteration 0 batch 12930 trainingloss 0.6931471805599453
iteration 0 batch 12940 trainingloss 0.6916632528527511
iteration 0 batch 12950 trainingloss 0.6901793251455568
iteration 0 batch 12960 trainingloss 0.6931471805599453
iteration 0 batch 12970 trainingloss 0.6901793251455568
iteration 0 batch 12980 trainingloss 0.6931471805599453
iteration 0 batch 12990 trainingloss 0.6931471805599453
iteration 0 batch 13000 trainingloss 0.6931471805599453
iteration 0 batch 13010 trainingloss 0.6931471805599453
iteration 0 batch 13020 trainingloss 0.6931471805599453
iteration 0 batch 13030 trainingloss 0.6931471805599453
iteration 0 batch 13040 trainingloss 0.6931471805599453
iteration 0 batch 13050 trainingloss 0.6931471805599453
iteration 0 batch 13060 trainingloss 0.6931471805599453
iteration 0 batch 13070 trainingloss 0.6931471805599453
iteration 0 batch 13080 trainingloss 0.6931471805599453
iteration 0 batch 13090 trainingloss 0.6931471805599453
iteration 0 batch 13100 trainingloss 0.6931471805599453
iteration 0 batch 13110 trainingloss 0.6916632528527511
iteration 0 batch 13120 trainingloss 0.6886953974383626
iteration 0 batch 13130 trainingloss 0.6931471805599453
iteration 0 batch 13140 trainingloss 0.6931471805599453
iteration 0 batch 13150 trainingloss 0.6916632528527511
iteration 0 batch 13160 trainingloss 0.6931471805599453
iteration 0 batch 13170 trainingloss 0.6931471805599453
iteration 0 batch 13180 trainingloss 0.6916632528527511
iteration 0 batch 13190 trainingloss 0.6931471805599453
iteration 0 batch 13200 trainingloss 0.6931471805599453
iteration 0 batch 13210 trainingloss 0.6901793251455568
iteration 0 batch 13220 trainingloss 0.6931471805599453
iteration 0 batch 13230 trainingloss 0.6916632528527511
iteration 0 batch 13240 trainingloss 0.6931471805599453
iteration 0 batch 13250 trainingloss 0.6931471805599453
iteration 0 batch 13260 trainingloss 0.6931471805599453
iteration 0 batch 13270 trainingloss 0.6931471805599453
iteration 0 batch 13280 trainingloss 0.6931471805599453
iteration 0 batch 13290 trainingloss 0.6931471805599453
iteration 0 batch 13300 trainingloss 0.6931471805599453
iteration 0 batch 13310 trainingloss 0.6931471805599453
iteration 0 batch 13320 trainingloss 0.6916632528527511
iteration 0 batch 13330 trainingloss 0.6931471805599453
iteration 0 batch 13340 trainingloss 0.6901793251455568
iteration 0 batch 13350 trainingloss 0.6931471805599453
iteration 0 batch 13360 trainingloss 0.6931471805599453
iteration 0 batch 13370 trainingloss 0.6916632528527511
iteration 0 batch 13380 trainingloss 0.6931471805599453
iteration 0 batch 13390 trainingloss 0.6916632528527511
iteration 0 batch 13400 trainingloss 0.6901793251455568
iteration 0 batch 13410 trainingloss 0.6931471805599453
iteration 0 batch 13420 trainingloss 0.6931471805599453
iteration 0 batch 13430 trainingloss 0.6931471805599453
iteration 0 batch 13440 trainingloss 0.6916632528527511
iteration 0 batch 13450 trainingloss 0.6931471805599453
iteration 0 batch 13460 trainingloss 0.6916632528527511
iteration 0 batch 13470 trainingloss 0.6916632528527511
iteration 0 batch 13480 trainingloss 0.6931471805599453
iteration 0 batch 13490 trainingloss 0.6916632528527511
iteration 0 batch 13500 trainingloss 0.6916632528527511
iteration 0 batch 13510 trainingloss 0.6916632528527511
iteration 0 batch 13520 trainingloss 0.6931471805599453
iteration 0 batch 13530 trainingloss 0.6931471805599453
iteration 0 batch 13540 trainingloss 0.6916632528527511
iteration 0 batch 13550 trainingloss 0.6931471805599453
iteration 0 batch 13560 trainingloss 0.6931471805599453
iteration 0 batch 13570 trainingloss 0.6931471805599453
iteration 0 batch 13580 trainingloss 0.6931471805599453
iteration 0 batch 13590 trainingloss 0.6916632528527511
iteration 0 batch 13600 trainingloss 0.6916632528527511
iteration 0 batch 13610 trainingloss 0.6916632528527511
iteration 0 batch 13620 trainingloss 0.6916632528527511
iteration 0 batch 13630 trainingloss 0.6931471805599453
iteration 0 batch 13640 trainingloss 0.6931471805599453
iteration 0 batch 13650 trainingloss 0.6931471805599453
iteration 0 batch 13660 trainingloss 0.6931471805599453
iteration 0 batch 13670 trainingloss 0.6916632528527511
iteration 0 batch 13680 trainingloss 0.6916632528527511
iteration 0 batch 13690 trainingloss 0.6931471805599453
iteration 0 batch 13700 trainingloss 0.6931471805599453
iteration 0 batch 13710 trainingloss 0.6931471805599453
iteration 0 batch 13720 trainingloss 0.6931471805599453
iteration 0 batch 13730 trainingloss 0.6931471805599453
iteration 0 batch 13740 trainingloss 0.6931471805599453
iteration 0 batch 13750 trainingloss 0.6916632528527511
iteration 0 batch 13760 trainingloss 0.6931471805599453
iteration 0 batch 13770 trainingloss 0.6931471805599453
iteration 0 batch 13780 trainingloss 0.6916632528527511
iteration 0 batch 13790 trainingloss 0.6931471805599453
iteration 0 batch 13800 trainingloss 0.6916632528527511
iteration 0 batch 13810 trainingloss 0.6931471805599453
iteration 0 batch 13820 trainingloss 0.6931471805599453
iteration 0 batch 13830 trainingloss 0.6901793251455568
iteration 0 batch 13840 trainingloss 0.6931471805599453
iteration 0 batch 13850 trainingloss 0.6916632528527511
iteration 0 batch 13860 trainingloss 0.6916632528527511
iteration 0 batch 13870 trainingloss 0.6916632528527511
iteration 0 batch 13880 trainingloss 0.6931471805599453
iteration 0 batch 13890 trainingloss 0.6931471805599453
iteration 0 batch 13900 trainingloss 0.6916632528527511
iteration 0 batch 13910 trainingloss 0.6931471805599453
iteration 0 batch 13920 trainingloss 0.6931471805599453
iteration 0 batch 13930 trainingloss 0.6916632528527511
iteration 0 batch 13940 trainingloss 0.6931471805599453
iteration 0 batch 13950 trainingloss 0.6931471805599453
iteration 0 batch 13960 trainingloss 0.6931471805599453
iteration 0 batch 13970 trainingloss 0.6916632528527511
iteration 0 batch 13980 trainingloss 0.6916632528527511
iteration 0 batch 13990 trainingloss 0.6931471805599453
iteration 0 batch 14000 trainingloss 0.6931471805599453
iteration 0 batch 14010 trainingloss 0.6916632528527511
iteration 0 batch 14020 trainingloss 0.6916632528527511
iteration 0 batch 14030 trainingloss 0.6931471805599453
iteration 0 batch 14040 trainingloss 0.6931471805599453
iteration 0 batch 14050 trainingloss 0.6916632528527511
iteration 0 batch 14060 trainingloss 0.6916632528527511
iteration 0 batch 14070 trainingloss 0.6931471805599453
iteration 0 batch 14080 trainingloss 0.6931471805599453
iteration 0 batch 14090 trainingloss 0.6916632528527511
iteration 0 batch 14100 trainingloss 0.6931471805599453
iteration 0 batch 14110 trainingloss 0.6931471805599453
iteration 0 batch 14120 trainingloss 0.6931471805599453
iteration 0 batch 14130 trainingloss 0.6931471805599453
iteration 0 batch 14140 trainingloss 0.6901793251455568
iteration 0 batch 14150 trainingloss 0.6916632528527511
iteration 0 batch 14160 trainingloss 0.6931471805599453
iteration 0 batch 14170 trainingloss 0.6901793251455568
iteration 0 batch 14180 trainingloss 0.6916632528527511
iteration 0 batch 14190 trainingloss 0.6931471805599453
iteration 0 batch 14200 trainingloss 0.6916632528527511
iteration 0 batch 14210 trainingloss 0.6931471805599453
iteration 0 batch 14220 trainingloss 0.6916632528527511
iteration 0 batch 14230 trainingloss 0.6931471805599453
iteration 0 batch 14240 trainingloss 0.6931471805599453
iteration 0 batch 14250 trainingloss 0.6931471805599453
iteration 0 batch 14260 trainingloss 0.6931471805599453
iteration 0 batch 14270 trainingloss 0.6931471805599453
iteration 0 batch 14280 trainingloss 0.6916632528527511
iteration 0 batch 14290 trainingloss 0.6931471805599453
iteration 0 batch 14300 trainingloss 0.6931471805599453
iteration 0 batch 14310 trainingloss 0.6931471805599453
iteration 0 batch 14320 trainingloss 0.6916632528527511
iteration 0 batch 14330 trainingloss 0.6931471805599453
iteration 0 batch 14340 trainingloss 0.6931471805599453
iteration 0 batch 14350 trainingloss 0.6901793251455568
iteration 0 batch 14360 trainingloss 0.6886953974383626
iteration 0 batch 14370 trainingloss 0.6886953974383626
iteration 0 batch 14380 trainingloss 0.6931471805599453
iteration 0 batch 14390 trainingloss 0.6916632528527511
iteration 0 batch 14400 trainingloss 0.6931471805599453
iteration 0 batch 14410 trainingloss 0.6931471805599453
iteration 0 batch 14420 trainingloss 0.6931471805599453
iteration 0 batch 14430 trainingloss 0.6931471805599453
iteration 0 batch 14440 trainingloss 0.6931471805599453
iteration 0 batch 14450 trainingloss 0.6931471805599453
iteration 0 batch 14460 trainingloss 0.6931471805599453
iteration 0 batch 14470 trainingloss 0.6931471805599453
iteration 0 batch 14480 trainingloss 0.6931471805599453
iteration 0 batch 14490 trainingloss 0.6916632528527511
iteration 0 batch 14500 trainingloss 0.6931471805599453
iteration 0 batch 14510 trainingloss 0.6931471805599453
iteration 0 batch 14520 trainingloss 0.6931471805599453
iteration 0 batch 14530 trainingloss 0.6931471805599453
iteration 0 batch 14540 trainingloss 0.6931471805599453
iteration 0 batch 14550 trainingloss 0.6931471805599453
iteration 0 batch 14560 trainingloss 0.6931471805599453
iteration 0 batch 14570 trainingloss 0.6931471805599453
iteration 0 batch 14580 trainingloss 0.6916632528527511
iteration 0 batch 14590 trainingloss 0.6931471805599453
iteration 0 batch 14600 trainingloss 0.6901793251455568
iteration 0 batch 14610 trainingloss 0.6931471805599453
iteration 0 batch 14620 trainingloss 0.6931471805599453
iteration 0 batch 14630 trainingloss 0.6916632528527511
iteration 0 batch 14640 trainingloss 0.6931471805599453
iteration 0 batch 14650 trainingloss 0.6931471805599453
iteration 0 batch 14660 trainingloss 0.6931471805599453
iteration 0 batch 14670 trainingloss 0.6931471805599453
iteration 0 batch 14680 trainingloss 0.6901793251455568
iteration 0 batch 14690 trainingloss 0.6916632528527511
iteration 0 batch 14700 trainingloss 0.6931471805599453
iteration 0 batch 14710 trainingloss 0.6931471805599453
iteration 0 batch 14720 trainingloss 0.6931471805599453
iteration 0 batch 14730 trainingloss 0.6931471805599453
iteration 0 batch 14740 trainingloss 0.6931471805599453
iteration 0 batch 14750 trainingloss 0.6916632528527511
iteration 0 batch 14760 trainingloss 0.6916632528527511
iteration 0 batch 14770 trainingloss 0.6931471805599453
iteration 0 batch 14780 trainingloss 0.6931471805599453
iteration 0 batch 14790 trainingloss 0.6931471805599453
iteration 0 batch 14800 trainingloss 0.6931471805599453
iteration 0 batch 14810 trainingloss 0.6931471805599453
iteration 0 batch 14820 trainingloss 0.6916632528527511
iteration 0 batch 14830 trainingloss 0.6931471805599453
iteration 0 batch 14840 trainingloss 0.6916632528527511
iteration 0 batch 14850 trainingloss 0.6931471805599453
iteration 0 batch 14860 trainingloss 0.6916632528527511
iteration 0 batch 14870 trainingloss 0.6931471805599453
iteration 0 batch 14880 trainingloss 0.6916632528527511
iteration 0 batch 14890 trainingloss 0.6886953974383626
iteration 0 batch 14900 trainingloss 0.6931471805599453
iteration 0 batch 14910 trainingloss 0.6931471805599453
iteration 0 batch 14920 trainingloss 0.6916632528527511
iteration 0 batch 14930 trainingloss 0.6916632528527511
iteration 0 batch 14940 trainingloss 0.6931471805599453
iteration 0 batch 14950 trainingloss 0.6931471805599453
iteration 0 batch 14960 trainingloss 0.6931471805599453
iteration 0 batch 14970 trainingloss 0.6931471805599453
iteration 0 batch 14980 trainingloss 0.6901793251455568
iteration 0 batch 14990 trainingloss 0.6931471805599453
iteration 0 batch 15000 trainingloss 0.6931471805599453
iteration 0 batch 15010 trainingloss 0.6916632528527511
iteration 0 batch 15020 trainingloss 0.6931471805599453
iteration 0 batch 15030 trainingloss 0.6916632528527511
iteration 0 batch 15040 trainingloss 0.6931471805599453
iteration 0 batch 15050 trainingloss 0.6931471805599453
iteration 0 batch 15060 trainingloss 0.6916632528527511
iteration 0 batch 15070 trainingloss 0.6916632528527511
iteration 0 batch 15080 trainingloss 0.6916632528527511
iteration 0 batch 15090 trainingloss 0.6931471805599453
iteration 0 batch 15100 trainingloss 0.6916632528527511
iteration 0 batch 15110 trainingloss 0.6931471805599453
iteration 0 batch 15120 trainingloss 0.6931471805599453
iteration 0 batch 15130 trainingloss 0.6931471805599453
iteration 0 batch 15140 trainingloss 0.6931471805599453
iteration 0 batch 15150 trainingloss 0.6916632528527511
iteration 0 batch 15160 trainingloss 0.6931471805599453
iteration 0 batch 15170 trainingloss 0.6931471805599453
iteration 0 batch 15180 trainingloss 0.6931471805599453
iteration 0 batch 15190 trainingloss 0.6931471805599453
iteration 0 batch 15200 trainingloss 0.6931471805599453
iteration 0 batch 15210 trainingloss 0.6931471805599453
iteration 0 batch 15220 trainingloss 0.6931471805599453
iteration 0 batch 15230 trainingloss 0.6916632528527511
iteration 0 batch 15240 trainingloss 0.6931471805599453
iteration 0 batch 15250 trainingloss 0.6931471805599453
iteration 0 batch 15260 trainingloss 0.6931471805599453
iteration 0 batch 15270 trainingloss 0.6931471805599453
iteration 0 batch 15280 trainingloss 0.6931471805599453
iteration 0 batch 15290 trainingloss 0.6931471805599453
iteration 0 batch 15300 trainingloss 0.6916632528527511
iteration 0 batch 15310 trainingloss 0.6931471805599453
iteration 0 batch 15320 trainingloss 0.6931471805599453
iteration 0 batch 15330 trainingloss 0.6931471805599453
iteration 0 batch 15340 trainingloss 0.6931471805599453
iteration 0 batch 15350 trainingloss 0.6931471805599453
iteration 0 batch 15360 trainingloss 0.6901793251455568
iteration 0 batch 15370 trainingloss 0.6916632528527511
iteration 0 batch 15380 trainingloss 0.6931471805599453
iteration 0 batch 15390 trainingloss 0.6916632528527511
iteration 0 batch 15400 trainingloss 0.6931471805599453
iteration 0 batch 15410 trainingloss 0.6931471805599453
iteration 0 batch 15420 trainingloss 0.6916632528527511
iteration 0 batch 15430 trainingloss 0.6931471805599453
iteration 0 batch 15440 trainingloss 0.6931471805599453
iteration 0 batch 15450 trainingloss 0.6931471805599453
iteration 0 batch 15460 trainingloss 0.6931471805599453
iteration 0 batch 15470 trainingloss 0.6931471805599453
iteration 0 batch 15480 trainingloss 0.6931471805599453
iteration 0 batch 15490 trainingloss 0.6916632528527511
iteration 0 batch 15500 trainingloss 0.6931471805599453
iteration 0 batch 15510 trainingloss 0.6931471805599453
iteration 0 batch 15520 trainingloss 0.6931471805599453
iteration 0 batch 15530 trainingloss 0.6931471805599453
iteration 0 batch 15540 trainingloss 0.6901793251455568
iteration 0 batch 15550 trainingloss 0.6931471805599453
iteration 0 batch 15560 trainingloss 0.6931471805599453
iteration 0 batch 15570 trainingloss 0.6916632528527511
iteration 0 batch 15580 trainingloss 0.6931471805599453
iteration 0 batch 15590 trainingloss 0.6931471805599453
iteration 0 batch 15600 trainingloss 0.6931471805599453
iteration 0 batch 15610 trainingloss 0.6916632528527511
iteration 0 batch 15620 trainingloss 0.6931471805599453
iteration 0 batch 15630 trainingloss 0.6931471805599453
iteration 0 batch 15640 trainingloss 0.6931471805599453
iteration 0 batch 15650 trainingloss 0.6931471805599453
iteration 0 batch 15660 trainingloss 0.6916632528527511
iteration 0 batch 15670 trainingloss 0.6916632528527511
iteration 0 batch 15680 trainingloss 0.6931471805599453
iteration 0 batch 15690 trainingloss 0.6931471805599453
iteration 0 batch 15700 trainingloss 0.6931471805599453
iteration 0 batch 15710 trainingloss 0.6931471805599453
iteration 0 batch 15720 trainingloss 0.6916632528527511
iteration 0 batch 15730 trainingloss 0.6931471805599453
iteration 0 batch 15740 trainingloss 0.6931471805599453
iteration 0 batch 15750 trainingloss 0.6931471805599453
iteration 0 batch 15760 trainingloss 0.6916632528527511
iteration 0 batch 15770 trainingloss 0.6931471805599453
iteration 0 batch 15780 trainingloss 0.6931471805599453
iteration 0 batch 15790 trainingloss 0.6931471805599453
iteration 0 batch 15800 trainingloss 0.6916632528527511
iteration 0 batch 15810 trainingloss 0.6931471805599453
iteration 0 batch 15820 trainingloss 0.6931471805599453
iteration 0 batch 15830 trainingloss 0.6931471805599453
iteration 0 batch 15840 trainingloss 0.6931471805599453
iteration 0 batch 15850 trainingloss 0.6931471805599453
iteration 0 batch 15860 trainingloss 0.6931471805599453
iteration 0 batch 15870 trainingloss 0.6931471805599453
iteration 0 batch 15880 trainingloss 0.6931471805599453
iteration 0 batch 15890 trainingloss 0.6931471805599453
iteration 0 batch 15900 trainingloss 0.6901793251455568
iteration 0 batch 15910 trainingloss 0.6931471805599453
iteration 0 batch 15920 trainingloss 0.6931471805599453
iteration 0 batch 15930 trainingloss 0.6916632528527511
iteration 0 batch 15940 trainingloss 0.6931471805599453
iteration 0 batch 15950 trainingloss 0.6931471805599453
iteration 0 batch 15960 trainingloss 0.6931471805599453
iteration 0 batch 15970 trainingloss 0.6931471805599453
iteration 0 batch 15980 trainingloss 0.6916632528527511
iteration 0 batch 15990 trainingloss 0.6931471805599453
iteration 0 batch 16000 trainingloss 0.6931471805599453
iteration 0 batch 16010 trainingloss 0.6931471805599453
iteration 0 batch 16020 trainingloss 0.6916632528527511
iteration 0 batch 16030 trainingloss 0.6916632528527511
iteration 0 batch 16040 trainingloss 0.6931471805599453
iteration 0 batch 16050 trainingloss 0.6916632528527511
iteration 0 batch 16060 trainingloss 0.6916632528527511
iteration 0 batch 16070 trainingloss 0.6916632528527511
iteration 0 batch 16080 trainingloss 0.6931471805599453
iteration 0 batch 16090 trainingloss 0.6931471805599453
iteration 0 batch 16100 trainingloss 0.6931471805599453
iteration 0 batch 16110 trainingloss 0.6901793251455568
iteration 0 batch 16120 trainingloss 0.6916632528527511
iteration 0 batch 16130 trainingloss 0.6916632528527511
iteration 0 batch 16140 trainingloss 0.6931471805599453
iteration 0 batch 16150 trainingloss 0.6931471805599453
iteration 0 batch 16160 trainingloss 0.6916632528527511
iteration 0 batch 16170 trainingloss 0.6931471805599453
iteration 0 batch 16180 trainingloss 0.6931471805599453
iteration 0 batch 16190 trainingloss 0.6931471805599453
iteration 0 batch 16200 trainingloss 0.6931471805599453
iteration 0 batch 16210 trainingloss 0.6916632528527511
iteration 0 batch 16220 trainingloss 0.6901793251455568
iteration 0 batch 16230 trainingloss 0.6931471805599453
iteration 0 batch 16240 trainingloss 0.6931471805599453
iteration 0 batch 16250 trainingloss 0.6931471805599453
iteration 0 batch 16260 trainingloss 0.6931471805599453
iteration 0 batch 16270 trainingloss 0.6916632528527511
iteration 0 batch 16280 trainingloss 0.6931471805599453
iteration 0 batch 16290 trainingloss 0.6931471805599453
iteration 0 batch 16300 trainingloss 0.6931471805599453
iteration 0 batch 16310 trainingloss 0.6916632528527511
iteration 0 batch 16320 trainingloss 0.6931471805599453
iteration 0 batch 16330 trainingloss 0.6931471805599453
iteration 0 batch 16340 trainingloss 0.6901793251455568
iteration 0 batch 16350 trainingloss 0.6931471805599453
iteration 0 batch 16360 trainingloss 0.6931471805599453
iteration 0 batch 16370 trainingloss 0.6916632528527511
iteration 0 batch 16380 trainingloss 0.6931471805599453
iteration 0 batch 16390 trainingloss 0.6916632528527511
iteration 0 batch 16400 trainingloss 0.6931471805599453
iteration 0 batch 16410 trainingloss 0.6931471805599453
iteration 0 batch 16420 trainingloss 0.6931471805599453
iteration 0 batch 16430 trainingloss 0.6916632528527511
iteration 0 batch 16440 trainingloss 0.6931471805599453
iteration 0 batch 16450 trainingloss 0.6931471805599453
iteration 0 batch 16460 trainingloss 0.6931471805599453
iteration 0 batch 16470 trainingloss 0.6916632528527511
iteration 0 batch 16480 trainingloss 0.6916632528527511
iteration 0 batch 16490 trainingloss 0.6916632528527511
iteration 0 batch 16500 trainingloss 0.6931471805599453
iteration 0 batch 16510 trainingloss 0.6931471805599453
iteration 0 batch 16520 trainingloss 0.6931471805599453
iteration 0 batch 16530 trainingloss 0.6931471805599453
iteration 0 batch 16540 trainingloss 0.6931471805599453
iteration 0 batch 16550 trainingloss 0.6931471805599453
iteration 0 batch 16560 trainingloss 0.6916632528527511
iteration 0 batch 16570 trainingloss 0.6931471805599453
iteration 0 batch 16580 trainingloss 0.6931471805599453
iteration 0 batch 16590 trainingloss 0.6916632528527511
iteration 0 batch 16600 trainingloss 0.6931471805599453
iteration 0 batch 16610 trainingloss 0.6916632528527511
iteration 0 batch 16620 trainingloss 0.6931471805599453
iteration 0 batch 16630 trainingloss 0.6931471805599453
iteration 0 batch 16640 trainingloss 0.6886953974383626
iteration 0 batch 16650 trainingloss 0.6931471805599453
iteration 0 batch 16660 trainingloss 0.6931471805599453
iteration 0 batch 16670 trainingloss 0.6931471805599453
iteration 0 batch 16680 trainingloss 0.6931471805599453
iteration 0 batch 16690 trainingloss 0.6916632528527511
iteration 0 batch 16700 trainingloss 0.6931471805599453
iteration 0 batch 16710 trainingloss 0.6931471805599453
iteration 0 batch 16720 trainingloss 0.6931471805599453
iteration 0 batch 16730 trainingloss 0.6931471805599453
iteration 0 batch 16740 trainingloss 0.6931471805599453
iteration 0 batch 16750 trainingloss 0.6916632528527511
iteration 0 batch 16760 trainingloss 0.6931471805599453
iteration 0 batch 16770 trainingloss 0.6931471805599453
iteration 0 batch 16780 trainingloss 0.6931471805599453
iteration 0 batch 16790 trainingloss 0.6931471805599453
iteration 0 batch 16800 trainingloss 0.6931471805599453
iteration 0 batch 16810 trainingloss 0.6931471805599453
iteration 0 batch 16820 trainingloss 0.6931471805599453
iteration 0 batch 16830 trainingloss 0.6931471805599453
iteration 0 batch 16840 trainingloss 0.6931471805599453
iteration 0 batch 16850 trainingloss 0.6931471805599453
iteration 0 batch 16860 trainingloss 0.6931471805599453
iteration 0 batch 16870 trainingloss 0.6931471805599453
iteration 0 batch 16880 trainingloss 0.6916632528527511
iteration 0 batch 16890 trainingloss 0.6901793251455568
iteration 0 batch 16900 trainingloss 0.6916632528527511
iteration 0 batch 16910 trainingloss 0.6931471805599453
iteration 0 batch 16920 trainingloss 0.6931471805599453
iteration 0 batch 16930 trainingloss 0.6901793251455568
iteration 0 batch 16940 trainingloss 0.6931471805599453
iteration 0 batch 16950 trainingloss 0.6931471805599453
iteration 0 batch 16960 trainingloss 0.6931471805599453
iteration 0 batch 16970 trainingloss 0.6916632528527511
iteration 0 batch 16980 trainingloss 0.6931471805599453
iteration 0 batch 16990 trainingloss 0.6916632528527511
iteration 0 batch 17000 trainingloss 0.6931471805599453
iteration 0 batch 17010 trainingloss 0.6916632528527511
iteration 0 batch 17020 trainingloss 0.6916632528527511
iteration 0 batch 17030 trainingloss 0.6931471805599453
iteration 0 batch 17040 trainingloss 0.6931471805599453
iteration 0 batch 17050 trainingloss 0.6931471805599453
iteration 0 batch 17060 trainingloss 0.6931471805599453
iteration 0 batch 17070 trainingloss 0.6916632528527511
iteration 0 batch 17080 trainingloss 0.6931471805599453
iteration 0 batch 17090 trainingloss 0.6916632528527511
iteration 0 batch 17100 trainingloss 0.6916632528527511
iteration 0 batch 17110 trainingloss 0.6916632528527511
iteration 0 batch 17120 trainingloss 0.6931471805599453
iteration 0 batch 17130 trainingloss 0.6916632528527511
iteration 0 batch 17140 trainingloss 0.6931471805599453
iteration 0 batch 17150 trainingloss 0.6931471805599453
iteration 0 batch 17160 trainingloss 0.6931471805599453
iteration 0 batch 17170 trainingloss 0.6931471805599453
iteration 0 batch 17180 trainingloss 0.6931471805599453
iteration 0 batch 17190 trainingloss 0.6901793251455568
iteration 0 batch 17200 trainingloss 0.6931471805599453
iteration 0 batch 17210 trainingloss 0.6931471805599453
iteration 0 batch 17220 trainingloss 0.6916632528527511
iteration 0 batch 17230 trainingloss 0.6931471805599453
iteration 0 batch 17240 trainingloss 0.6931471805599453
iteration 0 batch 17250 trainingloss 0.6916632528527511
iteration 0 batch 17260 trainingloss 0.6931471805599453
iteration 0 batch 17270 trainingloss 0.6916632528527511
iteration 0 batch 17280 trainingloss 0.6931471805599453
iteration 0 batch 17290 trainingloss 0.6931471805599453
iteration 0 batch 17300 trainingloss 0.6931471805599453
iteration 0 batch 17310 trainingloss 0.6931471805599453
iteration 0 batch 17320 trainingloss 0.6931471805599453
iteration 0 batch 17330 trainingloss 0.6916632528527511
iteration 0 batch 17340 trainingloss 0.6931471805599453
iteration 0 batch 17350 trainingloss 0.6931471805599453
iteration 0 batch 17360 trainingloss 0.6931471805599453
iteration 0 batch 17370 trainingloss 0.6931471805599453
iteration 0 batch 17380 trainingloss 0.6916632528527511
iteration 0 batch 17390 trainingloss 0.6931471805599453
iteration 0 batch 17400 trainingloss 0.6931471805599453
iteration 0 batch 17410 trainingloss 0.6916632528527511
iteration 0 batch 17420 trainingloss 0.6901793251455568
iteration 0 batch 17430 trainingloss 0.6931471805599453
iteration 0 batch 17440 trainingloss 0.6931471805599453
iteration 0 batch 17450 trainingloss 0.6916632528527511
iteration 0 batch 17460 trainingloss 0.6931471805599453
iteration 0 batch 17470 trainingloss 0.6931471805599453
iteration 0 batch 17480 trainingloss 0.6931471805599453
iteration 0 batch 17490 trainingloss 0.6931471805599453
iteration 0 batch 17500 trainingloss 0.6916632528527511
iteration 0 batch 17510 trainingloss 0.6916632528527511
iteration 0 batch 17520 trainingloss 0.6931471805599453
iteration 0 batch 17530 trainingloss 0.6916632528527511
iteration 0 batch 17540 trainingloss 0.6931471805599453
iteration 0 batch 17550 trainingloss 0.6931471805599453
iteration 0 batch 17560 trainingloss 0.6931471805599453
iteration 0 batch 17570 trainingloss 0.6931471805599453
iteration 0 batch 17580 trainingloss 0.6916632528527511
iteration 0 batch 17590 trainingloss 0.6931471805599453
iteration 0 batch 17600 trainingloss 0.6931471805599453
iteration 0 batch 17610 trainingloss 0.6931471805599453
iteration 0 batch 17620 trainingloss 0.6931471805599453
iteration 0 batch 17630 trainingloss 0.6931471805599453
iteration 0 batch 17640 trainingloss 0.6931471805599453
iteration 0 batch 17650 trainingloss 0.6931471805599453
iteration 0 batch 17660 trainingloss 0.6931471805599453
iteration 0 batch 17670 trainingloss 0.6931471805599453
iteration 0 batch 17680 trainingloss 0.6931471805599453
iteration 0 batch 17690 trainingloss 0.6916632528527511
iteration 0 batch 17700 trainingloss 0.6916632528527511
iteration 0 batch 17710 trainingloss 0.6931471805599453
iteration 0 batch 17720 trainingloss 0.6931471805599453
iteration 0 batch 17730 trainingloss 0.6916632528527511
iteration 0 batch 17740 trainingloss 0.6931471805599453
iteration 0 batch 17750 trainingloss 0.6931471805599453
iteration 0 batch 17760 trainingloss 0.6931471805599453
iteration 0 batch 17770 trainingloss 0.6931471805599453
iteration 0 batch 17780 trainingloss 0.6931471805599453
iteration 0 batch 17790 trainingloss 0.6931471805599453
iteration 0 batch 17800 trainingloss 0.6931471805599453
iteration 0 batch 17810 trainingloss 0.6931471805599453
iteration 0 batch 17820 trainingloss 0.6931471805599453
iteration 0 batch 17830 trainingloss 0.6916632528527511
iteration 0 batch 17840 trainingloss 0.6916632528527511
iteration 0 batch 17850 trainingloss 0.6931471805599453
iteration 0 batch 17860 trainingloss 0.6931471805599453
iteration 0 batch 17870 trainingloss 0.6931471805599453
iteration 0 batch 17880 trainingloss 0.6931471805599453
iteration 0 batch 17890 trainingloss 0.6931471805599453
iteration 0 batch 17900 trainingloss 0.6916632528527511
iteration 0 batch 17910 trainingloss 0.6931471805599453
iteration 0 batch 17920 trainingloss 0.6931471805599453
iteration 0 batch 17930 trainingloss 0.6931471805599453
iteration 0 batch 17940 trainingloss 0.6931471805599453
iteration 0 batch 17950 trainingloss 0.6931471805599453
iteration 0 batch 17960 trainingloss 0.6931471805599453
iteration 0 batch 17970 trainingloss 0.6931471805599453
iteration 0 batch 17980 trainingloss 0.6931471805599453
iteration 0 batch 17990 trainingloss 0.6916632528527511
iteration 0 batch 18000 trainingloss 0.6931471805599453
iteration 0 batch 18010 trainingloss 0.6916632528527511
iteration 0 batch 18020 trainingloss 0.6931471805599453
iteration 0 batch 18030 trainingloss 0.6931471805599453
iteration 0 batch 18040 trainingloss 0.6916632528527511
iteration 0 batch 18050 trainingloss 0.6931471805599453
iteration 0 batch 18060 trainingloss 0.6916632528527511
iteration 0 batch 18070 trainingloss 0.6931471805599453
iteration 0 batch 18080 trainingloss 0.6916632528527511
iteration 0 batch 18090 trainingloss 0.6931471805599453
iteration 0 batch 18100 trainingloss 0.6931471805599453
iteration 0 batch 18110 trainingloss 0.6931471805599453
iteration 0 batch 18120 trainingloss 0.6916632528527511
iteration 0 batch 18130 trainingloss 0.6931471805599453
iteration 0 batch 18140 trainingloss 0.6916632528527511
iteration 0 batch 18150 trainingloss 0.6901793251455568
iteration 0 batch 18160 trainingloss 0.6931471805599453
iteration 0 batch 18170 trainingloss 0.6931471805599453
iteration 0 batch 18180 trainingloss 0.6916632528527511
iteration 0 batch 18190 trainingloss 0.6931471805599453
iteration 0 batch 18200 trainingloss 0.6931471805599453
iteration 0 batch 18210 trainingloss 0.6931471805599453
iteration 0 batch 18220 trainingloss 0.6931471805599453
iteration 0 batch 18230 trainingloss 0.6931471805599453
iteration 0 batch 18240 trainingloss 0.6916632528527511
iteration 0 batch 18250 trainingloss 0.6931471805599453
iteration 0 batch 18260 trainingloss 0.6916632528527511
iteration 0 batch 18270 trainingloss 0.6931471805599453
iteration 0 batch 18280 trainingloss 0.6916632528527511
iteration 0 batch 18290 trainingloss 0.6931471805599453
iteration 0 batch 18300 trainingloss 0.6931471805599453
iteration 0 batch 18310 trainingloss 0.6931471805599453
iteration 0 batch 18320 trainingloss 0.6931471805599453
iteration 0 batch 18330 trainingloss 0.6931471805599453
iteration 0 batch 18340 trainingloss 0.6916632528527511
iteration 0 batch 18350 trainingloss 0.6931471805599453
iteration 0 batch 18360 trainingloss 0.6931471805599453
iteration 0 batch 18370 trainingloss 0.6916632528527511
iteration 0 batch 18380 trainingloss 0.6931471805599453
iteration 0 batch 18390 trainingloss 0.6901793251455568
iteration 0 batch 18400 trainingloss 0.6931471805599453
iteration 0 batch 18410 trainingloss 0.6916632528527511
iteration 0 batch 18420 trainingloss 0.6931471805599453
iteration 0 batch 18430 trainingloss 0.6931471805599453
iteration 0 batch 18440 trainingloss 0.6931471805599453
iteration 0 batch 18450 trainingloss 0.6931471805599453
iteration 0 batch 18460 trainingloss 0.6931471805599453
iteration 0 batch 18470 trainingloss 0.6931471805599453
iteration 0 batch 18480 trainingloss 0.6931471805599453
iteration 0 batch 18490 trainingloss 0.6931471805599453
iteration 0 batch 18500 trainingloss 0.6901793251455568
iteration 0 batch 18510 trainingloss 0.6931471805599453
iteration 0 batch 18520 trainingloss 0.6916632528527511
iteration 0 batch 18530 trainingloss 0.6931471805599453
iteration 0 batch 18540 trainingloss 0.6931471805599453
iteration 0 batch 18550 trainingloss 0.6916632528527511
iteration 0 batch 18560 trainingloss 0.6931471805599453
iteration 0 batch 18570 trainingloss 0.6931471805599453
iteration 0 batch 18580 trainingloss 0.6916632528527511
iteration 0 batch 18590 trainingloss 0.6916632528527511
iteration 0 batch 18600 trainingloss 0.6901793251455568
iteration 0 batch 18610 trainingloss 0.6931471805599453
iteration 1 batch 0 trainingloss 0.6931471805599453
iteration 1 batch 10 trainingloss 0.6931471805599453
iteration 1 batch 20 trainingloss 0.6916632528527511
iteration 1 batch 30 trainingloss 0.6916632528527511
iteration 1 batch 40 trainingloss 0.6931471805599453
iteration 1 batch 50 trainingloss 0.6916632528527511
iteration 1 batch 60 trainingloss 0.6931471805599453
iteration 1 batch 70 trainingloss 0.6931471805599453
iteration 1 batch 80 trainingloss 0.6931471805599453
iteration 1 batch 90 trainingloss 0.6931471805599453
iteration 1 batch 100 trainingloss 0.6916632528527511
iteration 1 batch 110 trainingloss 0.6931471805599453
iteration 1 batch 120 trainingloss 0.6931471805599453
iteration 1 batch 130 trainingloss 0.6931471805599453
iteration 1 batch 140 trainingloss 0.6931471805599453
iteration 1 batch 150 trainingloss 0.6931471805599453
iteration 1 batch 160 trainingloss 0.6931471805599453
iteration 1 batch 170 trainingloss 0.6931471805599453
iteration 1 batch 180 trainingloss 0.6931471805599453
iteration 1 batch 190 trainingloss 0.6931471805599453
iteration 1 batch 200 trainingloss 0.6931471805599453
iteration 1 batch 210 trainingloss 0.6931471805599453
iteration 1 batch 220 trainingloss 0.6931471805599453
iteration 1 batch 230 trainingloss 0.6931471805599453
iteration 1 batch 240 trainingloss 0.6931471805599453
iteration 1 batch 250 trainingloss 0.6931471805599453
iteration 1 batch 260 trainingloss 0.6931471805599453
iteration 1 batch 270 trainingloss 0.6931471805599453
iteration 1 batch 280 trainingloss 0.6931471805599453
iteration 1 batch 290 trainingloss 0.6916632528527511
iteration 1 batch 300 trainingloss 0.6931471805599453
iteration 1 batch 310 trainingloss 0.6931471805599453
iteration 1 batch 320 trainingloss 0.6931471805599453
iteration 1 batch 330 trainingloss 0.6931471805599453
iteration 1 batch 340 trainingloss 0.6916632528527511
iteration 1 batch 350 trainingloss 0.6931471805599453
iteration 1 batch 360 trainingloss 0.6916632528527511
iteration 1 batch 370 trainingloss 0.6931471805599453
iteration 1 batch 380 trainingloss 0.6931471805599453
iteration 1 batch 390 trainingloss 0.6931471805599453
iteration 1 batch 400 trainingloss 0.6916632528527511
iteration 1 batch 410 trainingloss 0.6931471805599453
iteration 1 batch 420 trainingloss 0.6931471805599453
iteration 1 batch 430 trainingloss 0.6916632528527511
iteration 1 batch 440 trainingloss 0.6916632528527511
iteration 1 batch 450 trainingloss 0.6916632528527511
iteration 1 batch 460 trainingloss 0.6916632528527511
iteration 1 batch 470 trainingloss 0.6931471805599453
iteration 1 batch 480 trainingloss 0.6931471805599453
iteration 1 batch 490 trainingloss 0.6916632528527511
iteration 1 batch 500 trainingloss 0.6931471805599453
iteration 1 batch 510 trainingloss 0.6931471805599453
iteration 1 batch 520 trainingloss 0.6931471805599453
iteration 1 batch 530 trainingloss 0.6931471805599453
iteration 1 batch 540 trainingloss 0.6931471805599453
iteration 1 batch 550 trainingloss 0.6916632528527511
iteration 1 batch 560 trainingloss 0.6931471805599453
iteration 1 batch 570 trainingloss 0.6931471805599453
iteration 1 batch 580 trainingloss 0.6916632528527511
iteration 1 batch 590 trainingloss 0.6931471805599453
iteration 1 batch 600 trainingloss 0.6931471805599453
iteration 1 batch 610 trainingloss 0.6931471805599453
iteration 1 batch 620 trainingloss 0.6931471805599453
iteration 1 batch 630 trainingloss 0.6931471805599453
iteration 1 batch 640 trainingloss 0.6916632528527511
iteration 1 batch 650 trainingloss 0.6931471805599453
iteration 1 batch 660 trainingloss 0.6931471805599453
iteration 1 batch 670 trainingloss 0.6916632528527511
iteration 1 batch 680 trainingloss 0.6931471805599453
iteration 1 batch 690 trainingloss 0.6886953974383626
iteration 1 batch 700 trainingloss 0.6931471805599453
iteration 1 batch 710 trainingloss 0.6916632528527511
iteration 1 batch 720 trainingloss 0.6916632528527511
iteration 1 batch 730 trainingloss 0.6931471805599453
iteration 1 batch 740 trainingloss 0.6931471805599453
iteration 1 batch 750 trainingloss 0.6931471805599453
iteration 1 batch 760 trainingloss 0.6931471805599453
iteration 1 batch 770 trainingloss 0.6916632528527511
iteration 1 batch 780 trainingloss 0.6916632528527511
iteration 1 batch 790 trainingloss 0.6931471805599453
iteration 1 batch 800 trainingloss 0.6916632528527511
iteration 1 batch 810 trainingloss 0.6931471805599453
iteration 1 batch 820 trainingloss 0.6916632528527511
iteration 1 batch 830 trainingloss 0.6916632528527511
iteration 1 batch 840 trainingloss 0.6931471805599453
iteration 1 batch 850 trainingloss 0.6931471805599453
iteration 1 batch 860 trainingloss 0.6916632528527511
iteration 1 batch 870 trainingloss 0.6931471805599453
iteration 1 batch 880 trainingloss 0.6916632528527511
iteration 1 batch 890 trainingloss 0.6931471805599453
iteration 1 batch 900 trainingloss 0.6916632528527511
iteration 1 batch 910 trainingloss 0.6931471805599453
iteration 1 batch 920 trainingloss 0.6916632528527511
iteration 1 batch 930 trainingloss 0.6931471805599453
iteration 1 batch 940 trainingloss 0.6931471805599453
iteration 1 batch 950 trainingloss 0.6931471805599453
iteration 1 batch 960 trainingloss 0.6931471805599453
iteration 1 batch 970 trainingloss 0.6931471805599453
iteration 1 batch 980 trainingloss 0.6916632528527511
iteration 1 batch 990 trainingloss 0.6931471805599453
iteration 1 batch 1000 trainingloss 0.6931471805599453
iteration 1 batch 1010 trainingloss 0.6931471805599453
iteration 1 batch 1020 trainingloss 0.6916632528527511
iteration 1 batch 1030 trainingloss 0.6931471805599453
iteration 1 batch 1040 trainingloss 0.6916632528527511
iteration 1 batch 1050 trainingloss 0.6931471805599453
iteration 1 batch 1060 trainingloss 0.6931471805599453
iteration 1 batch 1070 trainingloss 0.6916632528527511
iteration 1 batch 1080 trainingloss 0.6931471805599453
iteration 1 batch 1090 trainingloss 0.6931471805599453
iteration 1 batch 1100 trainingloss 0.6931471805599453
iteration 1 batch 1110 trainingloss 0.6916632528527511
iteration 1 batch 1120 trainingloss 0.6931471805599453
iteration 1 batch 1130 trainingloss 0.6931471805599453
iteration 1 batch 1140 trainingloss 0.6916632528527511
iteration 1 batch 1150 trainingloss 0.6931471805599453
iteration 1 batch 1160 trainingloss 0.6916632528527511
iteration 1 batch 1170 trainingloss 0.6931471805599453
iteration 1 batch 1180 trainingloss 0.6931471805599453
iteration 1 batch 1190 trainingloss 0.6931471805599453
iteration 1 batch 1200 trainingloss 0.6931471805599453
iteration 1 batch 1210 trainingloss 0.6931471805599453
iteration 1 batch 1220 trainingloss 0.6931471805599453
iteration 1 batch 1230 trainingloss 0.6901793251455568
iteration 1 batch 1240 trainingloss 0.6916632528527511
iteration 1 batch 1250 trainingloss 0.6931471805599453
iteration 1 batch 1260 trainingloss 0.6931471805599453
iteration 1 batch 1270 trainingloss 0.6931471805599453
iteration 1 batch 1280 trainingloss 0.6916632528527511
iteration 1 batch 1290 trainingloss 0.6931471805599453
iteration 1 batch 1300 trainingloss 0.6931471805599453
iteration 1 batch 1310 trainingloss 0.6931471805599453
iteration 1 batch 1320 trainingloss 0.6916632528527511
iteration 1 batch 1330 trainingloss 0.6931471805599453
iteration 1 batch 1340 trainingloss 0.6931471805599453
iteration 1 batch 1350 trainingloss 0.6931471805599453
iteration 1 batch 1360 trainingloss 0.6931471805599453
iteration 1 batch 1370 trainingloss 0.6931471805599453
iteration 1 batch 1380 trainingloss 0.6931471805599453
iteration 1 batch 1390 trainingloss 0.6931471805599453
iteration 1 batch 1400 trainingloss 0.6931471805599453
iteration 1 batch 1410 trainingloss 0.6931471805599453
iteration 1 batch 1420 trainingloss 0.6901793251455568
iteration 1 batch 1430 trainingloss 0.6931471805599453
iteration 1 batch 1440 trainingloss 0.6931471805599453
iteration 1 batch 1450 trainingloss 0.6901793251455568
iteration 1 batch 1460 trainingloss 0.6931471805599453
iteration 1 batch 1470 trainingloss 0.6931471805599453
iteration 1 batch 1480 trainingloss 0.6931471805599453
iteration 1 batch 1490 trainingloss 0.6916632528527511
iteration 1 batch 1500 trainingloss 0.6931471805599453
iteration 1 batch 1510 trainingloss 0.6916632528527511
iteration 1 batch 1520 trainingloss 0.6931471805599453
iteration 1 batch 1530 trainingloss 0.6931471805599453
iteration 1 batch 1540 trainingloss 0.6931471805599453
iteration 1 batch 1550 trainingloss 0.6931471805599453
iteration 1 batch 1560 trainingloss 0.6931471805599453
iteration 1 batch 1570 trainingloss 0.6931471805599453
iteration 1 batch 1580 trainingloss 0.6931471805599453
iteration 1 batch 1590 trainingloss 0.6931471805599453
iteration 1 batch 1600 trainingloss 0.6931471805599453
iteration 1 batch 1610 trainingloss 0.6931471805599453
iteration 1 batch 1620 trainingloss 0.6931471805599453
iteration 1 batch 1630 trainingloss 0.6931471805599453
iteration 1 batch 1640 trainingloss 0.6931471805599453
iteration 1 batch 1650 trainingloss 0.6931471805599453
iteration 1 batch 1660 trainingloss 0.6916632528527511
iteration 1 batch 1670 trainingloss 0.6916632528527511
iteration 1 batch 1680 trainingloss 0.6916632528527511
iteration 1 batch 1690 trainingloss 0.6931471805599453
iteration 1 batch 1700 trainingloss 0.6931471805599453
iteration 1 batch 1710 trainingloss 0.6931471805599453
iteration 1 batch 1720 trainingloss 0.6931471805599453
iteration 1 batch 1730 trainingloss 0.6931471805599453
iteration 1 batch 1740 trainingloss 0.6916632528527511
iteration 1 batch 1750 trainingloss 0.6931471805599453
iteration 1 batch 1760 trainingloss 0.6931471805599453
iteration 1 batch 1770 trainingloss 0.6916632528527511
iteration 1 batch 1780 trainingloss 0.6931471805599453
iteration 1 batch 1790 trainingloss 0.6931471805599453
iteration 1 batch 1800 trainingloss 0.6931471805599453
iteration 1 batch 1810 trainingloss 0.6931471805599453
iteration 1 batch 1820 trainingloss 0.6916632528527511
iteration 1 batch 1830 trainingloss 0.6931471805599453
iteration 1 batch 1840 trainingloss 0.6931471805599453
iteration 1 batch 1850 trainingloss 0.6886953974383626
iteration 1 batch 1860 trainingloss 0.6931471805599453
iteration 1 batch 1870 trainingloss 0.6931471805599453
iteration 1 batch 1880 trainingloss 0.6931471805599453
iteration 1 batch 1890 trainingloss 0.6931471805599453
iteration 1 batch 1900 trainingloss 0.6931471805599453
iteration 1 batch 1910 trainingloss 0.6931471805599453
iteration 1 batch 1920 trainingloss 0.6931471805599453
iteration 1 batch 1930 trainingloss 0.6931471805599453
iteration 1 batch 1940 trainingloss 0.6931471805599453
iteration 1 batch 1950 trainingloss 0.6916632528527511
iteration 1 batch 1960 trainingloss 0.6931471805599453
iteration 1 batch 1970 trainingloss 0.6931471805599453
iteration 1 batch 1980 trainingloss 0.6931471805599453
iteration 1 batch 1990 trainingloss 0.6931471805599453
iteration 1 batch 2000 trainingloss 0.6931471805599453
iteration 1 batch 2010 trainingloss 0.6931471805599453
iteration 1 batch 2020 trainingloss 0.6931471805599453
iteration 1 batch 2030 trainingloss 0.6931471805599453
iteration 1 batch 2040 trainingloss 0.6931471805599453
iteration 1 batch 2050 trainingloss 0.6931471805599453
iteration 1 batch 2060 trainingloss 0.6931471805599453
iteration 1 batch 2070 trainingloss 0.6931471805599453
iteration 1 batch 2080 trainingloss 0.6916632528527511
iteration 1 batch 2090 trainingloss 0.6916632528527511
iteration 1 batch 2100 trainingloss 0.6931471805599453
iteration 1 batch 2110 trainingloss 0.6931471805599453
iteration 1 batch 2120 trainingloss 0.6931471805599453
iteration 1 batch 2130 trainingloss 0.6931471805599453
iteration 1 batch 2140 trainingloss 0.6916632528527511
iteration 1 batch 2150 trainingloss 0.6931471805599453
iteration 1 batch 2160 trainingloss 0.6931471805599453
iteration 1 batch 2170 trainingloss 0.6931471805599453
iteration 1 batch 2180 trainingloss 0.6931471805599453
iteration 1 batch 2190 trainingloss 0.6931471805599453
iteration 1 batch 2200 trainingloss 0.6931471805599453
iteration 1 batch 2210 trainingloss 0.6931471805599453
iteration 1 batch 2220 trainingloss 0.6931471805599453
iteration 1 batch 2230 trainingloss 0.6916632528527511
iteration 1 batch 2240 trainingloss 0.6931471805599453
iteration 1 batch 2250 trainingloss 0.6931471805599453
iteration 1 batch 2260 trainingloss 0.6916632528527511
iteration 1 batch 2270 trainingloss 0.6931471805599453
iteration 1 batch 2280 trainingloss 0.6931471805599453
iteration 1 batch 2290 trainingloss 0.6916632528527511
iteration 1 batch 2300 trainingloss 0.6931471805599453
iteration 1 batch 2310 trainingloss 0.6931471805599453
iteration 1 batch 2320 trainingloss 0.6901793251455568
iteration 1 batch 2330 trainingloss 0.6931471805599453
iteration 1 batch 2340 trainingloss 0.6916632528527511
iteration 1 batch 2350 trainingloss 0.6931471805599453
iteration 1 batch 2360 trainingloss 0.6916632528527511
iteration 1 batch 2370 trainingloss 0.6931471805599453
iteration 1 batch 2380 trainingloss 0.6931471805599453
iteration 1 batch 2390 trainingloss 0.6931471805599453
iteration 1 batch 2400 trainingloss 0.6931471805599453
iteration 1 batch 2410 trainingloss 0.6916632528527511
iteration 1 batch 2420 trainingloss 0.6916632528527511
iteration 1 batch 2430 trainingloss 0.6931471805599453
iteration 1 batch 2440 trainingloss 0.6931471805599453
iteration 1 batch 2450 trainingloss 0.6931471805599453
iteration 1 batch 2460 trainingloss 0.6931471805599453
iteration 1 batch 2470 trainingloss 0.6931471805599453
iteration 1 batch 2480 trainingloss 0.6901793251455568
iteration 1 batch 2490 trainingloss 0.6916632528527511
iteration 1 batch 2500 trainingloss 0.6931471805599453
iteration 1 batch 2510 trainingloss 0.6931471805599453
iteration 1 batch 2520 trainingloss 0.6931471805599453
iteration 1 batch 2530 trainingloss 0.6931471805599453
iteration 1 batch 2540 trainingloss 0.6931471805599453
iteration 1 batch 2550 trainingloss 0.6931471805599453
iteration 1 batch 2560 trainingloss 0.6931471805599453
iteration 1 batch 2570 trainingloss 0.6931471805599453
iteration 1 batch 2580 trainingloss 0.6931471805599453
iteration 1 batch 2590 trainingloss 0.6931471805599453
iteration 1 batch 2600 trainingloss 0.6931471805599453
iteration 1 batch 2610 trainingloss 0.6916632528527511
iteration 1 batch 2620 trainingloss 0.6931471805599453
iteration 1 batch 2630 trainingloss 0.6931471805599453
iteration 1 batch 2640 trainingloss 0.6916632528527511
iteration 1 batch 2650 trainingloss 0.6931471805599453
iteration 1 batch 2660 trainingloss 0.6931471805599453
iteration 1 batch 2670 trainingloss 0.6931471805599453
iteration 1 batch 2680 trainingloss 0.6931471805599453
iteration 1 batch 2690 trainingloss 0.6916632528527511
iteration 1 batch 2700 trainingloss 0.6931471805599453
iteration 1 batch 2710 trainingloss 0.6931471805599453
iteration 1 batch 2720 trainingloss 0.6931471805599453
iteration 1 batch 2730 trainingloss 0.6916632528527511
iteration 1 batch 2740 trainingloss 0.6931471805599453
iteration 1 batch 2750 trainingloss 0.6931471805599453
iteration 1 batch 2760 trainingloss 0.6931471805599453
iteration 1 batch 2770 trainingloss 0.6931471805599453
iteration 1 batch 2780 trainingloss 0.6931471805599453
iteration 1 batch 2790 trainingloss 0.6916632528527511
iteration 1 batch 2800 trainingloss 0.6931471805599453
iteration 1 batch 2810 trainingloss 0.6931471805599453
iteration 1 batch 2820 trainingloss 0.6931471805599453
iteration 1 batch 2830 trainingloss 0.6931471805599453
iteration 1 batch 2840 trainingloss 0.6931471805599453
iteration 1 batch 2850 trainingloss 0.6931471805599453
iteration 1 batch 2860 trainingloss 0.6916632528527511
iteration 1 batch 2870 trainingloss 0.6931471805599453
iteration 1 batch 2880 trainingloss 0.6931471805599453
iteration 1 batch 2890 trainingloss 0.6916632528527511
iteration 1 batch 2900 trainingloss 0.6931471805599453
iteration 1 batch 2910 trainingloss 0.6931471805599453
iteration 1 batch 2920 trainingloss 0.6916632528527511
iteration 1 batch 2930 trainingloss 0.6916632528527511
iteration 1 batch 2940 trainingloss 0.6931471805599453
iteration 1 batch 2950 trainingloss 0.6931471805599453
iteration 1 batch 2960 trainingloss 0.6931471805599453
iteration 1 batch 2970 trainingloss 0.6931471805599453
iteration 1 batch 2980 trainingloss 0.6931471805599453
iteration 1 batch 2990 trainingloss 0.6931471805599453
iteration 1 batch 3000 trainingloss 0.6931471805599453
iteration 1 batch 3010 trainingloss 0.6916632528527511
iteration 1 batch 3020 trainingloss 0.6931471805599453
iteration 1 batch 3030 trainingloss 0.6931471805599453
iteration 1 batch 3040 trainingloss 0.6931471805599453
iteration 1 batch 3050 trainingloss 0.6931471805599453
iteration 1 batch 3060 trainingloss 0.6931471805599453
iteration 1 batch 3070 trainingloss 0.6931471805599453
iteration 1 batch 3080 trainingloss 0.6931471805599453
iteration 1 batch 3090 trainingloss 0.6916632528527511
iteration 1 batch 3100 trainingloss 0.6931471805599453
iteration 1 batch 3110 trainingloss 0.6931471805599453
iteration 1 batch 3120 trainingloss 0.6931471805599453
iteration 1 batch 3130 trainingloss 0.6931471805599453
iteration 1 batch 3140 trainingloss 0.6916632528527511
iteration 1 batch 3150 trainingloss 0.6931471805599453
iteration 1 batch 3160 trainingloss 0.6931471805599453
iteration 1 batch 3170 trainingloss 0.6931471805599453
iteration 1 batch 3180 trainingloss 0.6931471805599453
iteration 1 batch 3190 trainingloss 0.6931471805599453
iteration 1 batch 3200 trainingloss 0.6931471805599453
iteration 1 batch 3210 trainingloss 0.6931471805599453
iteration 1 batch 3220 trainingloss 0.6916632528527511
iteration 1 batch 3230 trainingloss 0.6931471805599453
iteration 1 batch 3240 trainingloss 0.6916632528527511
iteration 1 batch 3250 trainingloss 0.6931471805599453
iteration 1 batch 3260 trainingloss 0.6931471805599453
iteration 1 batch 3270 trainingloss 0.6931471805599453
iteration 1 batch 3280 trainingloss 0.6901793251455568
iteration 1 batch 3290 trainingloss 0.6931471805599453
iteration 1 batch 3300 trainingloss 0.6931471805599453
iteration 1 batch 3310 trainingloss 0.6931471805599453
iteration 1 batch 3320 trainingloss 0.6916632528527511
iteration 1 batch 3330 trainingloss 0.6916632528527511
iteration 1 batch 3340 trainingloss 0.6916632528527511
iteration 1 batch 3350 trainingloss 0.6931471805599453
iteration 1 batch 3360 trainingloss 0.6901793251455568
iteration 1 batch 3370 trainingloss 0.6931471805599453
iteration 1 batch 3380 trainingloss 0.6931471805599453
iteration 1 batch 3390 trainingloss 0.6931471805599453
iteration 1 batch 3400 trainingloss 0.6931471805599453
iteration 1 batch 3410 trainingloss 0.6931471805599453
iteration 1 batch 3420 trainingloss 0.6916632528527511
iteration 1 batch 3430 trainingloss 0.6931471805599453
iteration 1 batch 3440 trainingloss 0.6931471805599453
iteration 1 batch 3450 trainingloss 0.6931471805599453
iteration 1 batch 3460 trainingloss 0.6931471805599453
iteration 1 batch 3470 trainingloss 0.6931471805599453
iteration 1 batch 3480 trainingloss 0.6931471805599453
iteration 1 batch 3490 trainingloss 0.6931471805599453
iteration 1 batch 3500 trainingloss 0.6931471805599453
iteration 1 batch 3510 trainingloss 0.6931471805599453
iteration 1 batch 3520 trainingloss 0.6931471805599453
iteration 1 batch 3530 trainingloss 0.6931471805599453
iteration 1 batch 3540 trainingloss 0.6931471805599453
iteration 1 batch 3550 trainingloss 0.6931471805599453
iteration 1 batch 3560 trainingloss 0.6931471805599453
iteration 1 batch 3570 trainingloss 0.6916632528527511
iteration 1 batch 3580 trainingloss 0.6931471805599453
iteration 1 batch 3590 trainingloss 0.6901793251455568
iteration 1 batch 3600 trainingloss 0.6931471805599453
iteration 1 batch 3610 trainingloss 0.6931471805599453
iteration 1 batch 3620 trainingloss 0.6931471805599453
iteration 1 batch 3630 trainingloss 0.6901793251455568
iteration 1 batch 3640 trainingloss 0.6901793251455568
iteration 1 batch 3650 trainingloss 0.6931471805599453
iteration 1 batch 3660 trainingloss 0.6931471805599453
iteration 1 batch 3670 trainingloss 0.6931471805599453
iteration 1 batch 3680 trainingloss 0.6931471805599453
iteration 1 batch 3690 trainingloss 0.6931471805599453
iteration 1 batch 3700 trainingloss 0.6931471805599453
iteration 1 batch 3710 trainingloss 0.6901793251455568
iteration 1 batch 3720 trainingloss 0.6931471805599453
iteration 1 batch 3730 trainingloss 0.6931471805599453
iteration 1 batch 3740 trainingloss 0.6931471805599453
iteration 1 batch 3750 trainingloss 0.6931471805599453
iteration 1 batch 3760 trainingloss 0.6916632528527511
iteration 1 batch 3770 trainingloss 0.6931471805599453
iteration 1 batch 3780 trainingloss 0.6931471805599453
iteration 1 batch 3790 trainingloss 0.6916632528527511
iteration 1 batch 3800 trainingloss 0.6931471805599453
iteration 1 batch 3810 trainingloss 0.6931471805599453
iteration 1 batch 3820 trainingloss 0.6931471805599453
iteration 1 batch 3830 trainingloss 0.6931471805599453
iteration 1 batch 3840 trainingloss 0.6931471805599453
iteration 1 batch 3850 trainingloss 0.6931471805599453
iteration 1 batch 3860 trainingloss 0.6931471805599453
iteration 1 batch 3870 trainingloss 0.6931471805599453
iteration 1 batch 3880 trainingloss 0.6931471805599453
iteration 1 batch 3890 trainingloss 0.6931471805599453
iteration 1 batch 3900 trainingloss 0.6916632528527511
iteration 1 batch 3910 trainingloss 0.6931471805599453
iteration 1 batch 3920 trainingloss 0.6931471805599453
iteration 1 batch 3930 trainingloss 0.6931471805599453
iteration 1 batch 3940 trainingloss 0.6931471805599453
iteration 1 batch 3950 trainingloss 0.6931471805599453
iteration 1 batch 3960 trainingloss 0.6931471805599453
iteration 1 batch 3970 trainingloss 0.6931471805599453
iteration 1 batch 3980 trainingloss 0.6931471805599453
iteration 1 batch 3990 trainingloss 0.6931471805599453
iteration 1 batch 4000 trainingloss 0.6931471805599453
iteration 1 batch 4010 trainingloss 0.6931471805599453
iteration 1 batch 4020 trainingloss 0.6931471805599453
iteration 1 batch 4030 trainingloss 0.6931471805599453
iteration 1 batch 4040 trainingloss 0.6931471805599453
iteration 1 batch 4050 trainingloss 0.6931471805599453
iteration 1 batch 4060 trainingloss 0.6931471805599453
iteration 1 batch 4070 trainingloss 0.6931471805599453
iteration 1 batch 4080 trainingloss 0.6931471805599453
iteration 1 batch 4090 trainingloss 0.6931471805599453
iteration 1 batch 4100 trainingloss 0.6931471805599453
iteration 1 batch 4110 trainingloss 0.6931471805599453
iteration 1 batch 4120 trainingloss 0.6931471805599453
iteration 1 batch 4130 trainingloss 0.6931471805599453
iteration 1 batch 4140 trainingloss 0.6931471805599453
iteration 1 batch 4150 trainingloss 0.6916632528527511
iteration 1 batch 4160 trainingloss 0.6916632528527511
iteration 1 batch 4170 trainingloss 0.6931471805599453
iteration 1 batch 4180 trainingloss 0.6931471805599453
iteration 1 batch 4190 trainingloss 0.6931471805599453
iteration 1 batch 4200 trainingloss 0.6931471805599453
iteration 1 batch 4210 trainingloss 0.6931471805599453
iteration 1 batch 4220 trainingloss 0.6931471805599453
iteration 1 batch 4230 trainingloss 0.6931471805599453
iteration 1 batch 4240 trainingloss 0.6931471805599453
iteration 1 batch 4250 trainingloss 0.6931471805599453
iteration 1 batch 4260 trainingloss 0.6931471805599453
iteration 1 batch 4270 trainingloss 0.6916632528527511
iteration 1 batch 4280 trainingloss 0.6931471805599453
iteration 1 batch 4290 trainingloss 0.6901793251455568
iteration 1 batch 4300 trainingloss 0.6931471805599453
iteration 1 batch 4310 trainingloss 0.6931471805599453
iteration 1 batch 4320 trainingloss 0.6931471805599453
iteration 1 batch 4330 trainingloss 0.6931471805599453
iteration 1 batch 4340 trainingloss 0.6931471805599453
iteration 1 batch 4350 trainingloss 0.6931471805599453
iteration 1 batch 4360 trainingloss 0.6931471805599453
iteration 1 batch 4370 trainingloss 0.6931471805599453
iteration 1 batch 4380 trainingloss 0.6931471805599453
iteration 1 batch 4390 trainingloss 0.6931471805599453
iteration 1 batch 4400 trainingloss 0.6931471805599453
iteration 1 batch 4410 trainingloss 0.6931471805599453
iteration 1 batch 4420 trainingloss 0.6931471805599453
iteration 1 batch 4430 trainingloss 0.6901793251455568
iteration 1 batch 4440 trainingloss 0.6931471805599453
iteration 1 batch 4450 trainingloss 0.6916632528527511
iteration 1 batch 4460 trainingloss 0.6931471805599453
iteration 1 batch 4470 trainingloss 0.6916632528527511
iteration 1 batch 4480 trainingloss 0.6916632528527511
iteration 1 batch 4490 trainingloss 0.6931471805599453
iteration 1 batch 4500 trainingloss 0.6931471805599453
iteration 1 batch 4510 trainingloss 0.6931471805599453
iteration 1 batch 4520 trainingloss 0.6916632528527511
iteration 1 batch 4530 trainingloss 0.6931471805599453
iteration 1 batch 4540 trainingloss 0.6931471805599453
iteration 1 batch 4550 trainingloss 0.6916632528527511
iteration 1 batch 4560 trainingloss 0.6931471805599453
iteration 1 batch 4570 trainingloss 0.6931471805599453
iteration 1 batch 4580 trainingloss 0.6931471805599453
iteration 1 batch 4590 trainingloss 0.6916632528527511
iteration 1 batch 4600 trainingloss 0.6931471805599453
iteration 1 batch 4610 trainingloss 0.6916632528527511
iteration 1 batch 4620 trainingloss 0.6931471805599453
iteration 1 batch 4630 trainingloss 0.6931471805599453
iteration 1 batch 4640 trainingloss 0.6931471805599453
iteration 1 batch 4650 trainingloss 0.6931471805599453
iteration 1 batch 4660 trainingloss 0.6916632528527511
iteration 1 batch 4670 trainingloss 0.6931471805599453
iteration 1 batch 4680 trainingloss 0.6931471805599453
iteration 1 batch 4690 trainingloss 0.6931471805599453
iteration 1 batch 4700 trainingloss 0.6931471805599453
iteration 1 batch 4710 trainingloss 0.6931471805599453
iteration 1 batch 4720 trainingloss 0.6931471805599453
iteration 1 batch 4730 trainingloss 0.6916632528527511
iteration 1 batch 4740 trainingloss 0.6916632528527511
iteration 1 batch 4750 trainingloss 0.6916632528527511
iteration 1 batch 4760 trainingloss 0.6931471805599453
iteration 1 batch 4770 trainingloss 0.6931471805599453
iteration 1 batch 4780 trainingloss 0.6931471805599453
iteration 1 batch 4790 trainingloss 0.6916632528527511
iteration 1 batch 4800 trainingloss 0.6916632528527511
iteration 1 batch 4810 trainingloss 0.6931471805599453
iteration 1 batch 4820 trainingloss 0.6886953974383626
iteration 1 batch 4830 trainingloss 0.6931471805599453
iteration 1 batch 4840 trainingloss 0.6931471805599453
iteration 1 batch 4850 trainingloss 0.6931471805599453
iteration 1 batch 4860 trainingloss 0.6931471805599453
iteration 1 batch 4870 trainingloss 0.6931471805599453
iteration 1 batch 4880 trainingloss 0.6916632528527511
iteration 1 batch 4890 trainingloss 0.6916632528527511
iteration 1 batch 4900 trainingloss 0.6901793251455568
iteration 1 batch 4910 trainingloss 0.6931471805599453
iteration 1 batch 4920 trainingloss 0.6931471805599453
iteration 1 batch 4930 trainingloss 0.6901793251455568
iteration 1 batch 4940 trainingloss 0.6916632528527511
iteration 1 batch 4950 trainingloss 0.6931471805599453
iteration 1 batch 4960 trainingloss 0.6916632528527511
iteration 1 batch 4970 trainingloss 0.6931471805599453
iteration 1 batch 4980 trainingloss 0.6931471805599453
iteration 1 batch 4990 trainingloss 0.6916632528527511
iteration 1 batch 5000 trainingloss 0.6901793251455568
iteration 1 batch 5010 trainingloss 0.6931471805599453
iteration 1 batch 5020 trainingloss 0.6931471805599453
iteration 1 batch 5030 trainingloss 0.6931471805599453
iteration 1 batch 5040 trainingloss 0.6916632528527511
iteration 1 batch 5050 trainingloss 0.6901793251455568
iteration 1 batch 5060 trainingloss 0.6931471805599453
iteration 1 batch 5070 trainingloss 0.6916632528527511
iteration 1 batch 5080 trainingloss 0.6901793251455568
iteration 1 batch 5090 trainingloss 0.6916632528527511
iteration 1 batch 5100 trainingloss 0.6931471805599453
iteration 1 batch 5110 trainingloss 0.6931471805599453
iteration 1 batch 5120 trainingloss 0.6916632528527511
iteration 1 batch 5130 trainingloss 0.6931471805599453
iteration 1 batch 5140 trainingloss 0.6931471805599453
iteration 1 batch 5150 trainingloss 0.6931471805599453
iteration 1 batch 5160 trainingloss 0.6931471805599453
iteration 1 batch 5170 trainingloss 0.6931471805599453
iteration 1 batch 5180 trainingloss 0.6931471805599453
iteration 1 batch 5190 trainingloss 0.6901793251455568
iteration 1 batch 5200 trainingloss 0.6931471805599453
iteration 1 batch 5210 trainingloss 0.6916632528527511
iteration 1 batch 5220 trainingloss 0.6916632528527511
iteration 1 batch 5230 trainingloss 0.6916632528527511
iteration 1 batch 5240 trainingloss 0.6931471805599453
iteration 1 batch 5250 trainingloss 0.6916632528527511
iteration 1 batch 5260 trainingloss 0.6916632528527511
iteration 1 batch 5270 trainingloss 0.6931471805599453
iteration 1 batch 5280 trainingloss 0.6916632528527511
iteration 1 batch 5290 trainingloss 0.6916632528527511
iteration 1 batch 5300 trainingloss 0.6931471805599453
iteration 1 batch 5310 trainingloss 0.6931471805599453
iteration 1 batch 5320 trainingloss 0.6931471805599453
iteration 1 batch 5330 trainingloss 0.6931471805599453
iteration 1 batch 5340 trainingloss 0.6916632528527511
iteration 1 batch 5350 trainingloss 0.6931471805599453
iteration 1 batch 5360 trainingloss 0.6901793251455568
iteration 1 batch 5370 trainingloss 0.6931471805599453
iteration 1 batch 5380 trainingloss 0.6931471805599453
iteration 1 batch 5390 trainingloss 0.6931471805599453
iteration 1 batch 5400 trainingloss 0.6931471805599453
iteration 1 batch 5410 trainingloss 0.6916632528527511
iteration 1 batch 5420 trainingloss 0.6931471805599453
iteration 1 batch 5430 trainingloss 0.6931471805599453
iteration 1 batch 5440 trainingloss 0.6931471805599453
iteration 1 batch 5450 trainingloss 0.6931471805599453
iteration 1 batch 5460 trainingloss 0.6931471805599453
iteration 1 batch 5470 trainingloss 0.6931471805599453
iteration 1 batch 5480 trainingloss 0.6931471805599453
iteration 1 batch 5490 trainingloss 0.6916632528527511
iteration 1 batch 5500 trainingloss 0.6931471805599453
iteration 1 batch 5510 trainingloss 0.6931471805599453
iteration 1 batch 5520 trainingloss 0.6916632528527511
iteration 1 batch 5530 trainingloss 0.6931471805599453
iteration 1 batch 5540 trainingloss 0.6931471805599453
iteration 1 batch 5550 trainingloss 0.6931471805599453
iteration 1 batch 5560 trainingloss 0.6916632528527511
iteration 1 batch 5570 trainingloss 0.6931471805599453
iteration 1 batch 5580 trainingloss 0.6916632528527511
iteration 1 batch 5590 trainingloss 0.6931471805599453
iteration 1 batch 5600 trainingloss 0.6931471805599453
iteration 1 batch 5610 trainingloss 0.6931471805599453
iteration 1 batch 5620 trainingloss 0.6931471805599453
iteration 1 batch 5630 trainingloss 0.6931471805599453
iteration 1 batch 5640 trainingloss 0.6931471805599453
iteration 1 batch 5650 trainingloss 0.6916632528527511
iteration 1 batch 5660 trainingloss 0.6931471805599453
iteration 1 batch 5670 trainingloss 0.6931471805599453
iteration 1 batch 5680 trainingloss 0.6931471805599453
iteration 1 batch 5690 trainingloss 0.6931471805599453
iteration 1 batch 5700 trainingloss 0.6931471805599453
iteration 1 batch 5710 trainingloss 0.6931471805599453
iteration 1 batch 5720 trainingloss 0.6931471805599453
iteration 1 batch 5730 trainingloss 0.6931471805599453
iteration 1 batch 5740 trainingloss 0.6916632528527511
iteration 1 batch 5750 trainingloss 0.6931471805599453
iteration 1 batch 5760 trainingloss 0.6931471805599453
iteration 1 batch 5770 trainingloss 0.6931471805599453
iteration 1 batch 5780 trainingloss 0.6916632528527511
iteration 1 batch 5790 trainingloss 0.6931471805599453
iteration 1 batch 5800 trainingloss 0.6931471805599453
iteration 1 batch 5810 trainingloss 0.6931471805599453
iteration 1 batch 5820 trainingloss 0.6931471805599453
iteration 1 batch 5830 trainingloss 0.6931471805599453
iteration 1 batch 5840 trainingloss 0.6931471805599453
iteration 1 batch 5850 trainingloss 0.6931471805599453
iteration 1 batch 5860 trainingloss 0.6931471805599453
iteration 1 batch 5870 trainingloss 0.6931471805599453
iteration 1 batch 5880 trainingloss 0.6931471805599453
iteration 1 batch 5890 trainingloss 0.6931471805599453
iteration 1 batch 5900 trainingloss 0.6931471805599453
iteration 1 batch 5910 trainingloss 0.6931471805599453
iteration 1 batch 5920 trainingloss 0.6931471805599453
iteration 1 batch 5930 trainingloss 0.6916632528527511
iteration 1 batch 5940 trainingloss 0.6931471805599453
iteration 1 batch 5950 trainingloss 0.6931471805599453
iteration 1 batch 5960 trainingloss 0.6916632528527511
iteration 1 batch 5970 trainingloss 0.6931471805599453
iteration 1 batch 5980 trainingloss 0.6931471805599453
iteration 1 batch 5990 trainingloss 0.6931471805599453
iteration 1 batch 6000 trainingloss 0.6931471805599453
iteration 1 batch 6010 trainingloss 0.6931471805599453
iteration 1 batch 6020 trainingloss 0.6931471805599453
iteration 1 batch 6030 trainingloss 0.6931471805599453
iteration 1 batch 6040 trainingloss 0.6931471805599453
iteration 1 batch 6050 trainingloss 0.6931471805599453
iteration 1 batch 6060 trainingloss 0.6931471805599453
iteration 1 batch 6070 trainingloss 0.6931471805599453
iteration 1 batch 6080 trainingloss 0.6931471805599453
iteration 1 batch 6090 trainingloss 0.6916632528527511
iteration 1 batch 6100 trainingloss 0.6916632528527511
iteration 1 batch 6110 trainingloss 0.6931471805599453
iteration 1 batch 6120 trainingloss 0.6931471805599453
iteration 1 batch 6130 trainingloss 0.6931471805599453
iteration 1 batch 6140 trainingloss 0.6931471805599453
iteration 1 batch 6150 trainingloss 0.6931471805599453
iteration 1 batch 6160 trainingloss 0.6931471805599453
iteration 1 batch 6170 trainingloss 0.6916632528527511
iteration 1 batch 6180 trainingloss 0.6916632528527511
iteration 1 batch 6190 trainingloss 0.6931471805599453
iteration 1 batch 6200 trainingloss 0.6931471805599453
iteration 1 batch 6210 trainingloss 0.6931471805599453
iteration 1 batch 6220 trainingloss 0.6931471805599453
iteration 1 batch 6230 trainingloss 0.6916632528527511
iteration 1 batch 6240 trainingloss 0.6886953974383626
iteration 1 batch 6250 trainingloss 0.6931471805599453
iteration 1 batch 6260 trainingloss 0.6931471805599453
iteration 1 batch 6270 trainingloss 0.6931471805599453
iteration 1 batch 6280 trainingloss 0.6916632528527511
iteration 1 batch 6290 trainingloss 0.6901793251455568
iteration 1 batch 6300 trainingloss 0.6931471805599453
iteration 1 batch 6310 trainingloss 0.6916632528527511
iteration 1 batch 6320 trainingloss 0.6931471805599453
iteration 1 batch 6330 trainingloss 0.6931471805599453
iteration 1 batch 6340 trainingloss 0.6931471805599453
iteration 1 batch 6350 trainingloss 0.6931471805599453
iteration 1 batch 6360 trainingloss 0.6931471805599453
iteration 1 batch 6370 trainingloss 0.6916632528527511
iteration 1 batch 6380 trainingloss 0.6931471805599453
iteration 1 batch 6390 trainingloss 0.6931471805599453
iteration 1 batch 6400 trainingloss 0.6931471805599453
iteration 1 batch 6410 trainingloss 0.6916632528527511
iteration 1 batch 6420 trainingloss 0.6931471805599453
iteration 1 batch 6430 trainingloss 0.6931471805599453
iteration 1 batch 6440 trainingloss 0.6931471805599453
iteration 1 batch 6450 trainingloss 0.6931471805599453
iteration 1 batch 6460 trainingloss 0.6931471805599453
iteration 1 batch 6470 trainingloss 0.6916632528527511
iteration 1 batch 6480 trainingloss 0.6916632528527511
iteration 1 batch 6490 trainingloss 0.6931471805599453
iteration 1 batch 6500 trainingloss 0.6916632528527511
iteration 1 batch 6510 trainingloss 0.6931471805599453
iteration 1 batch 6520 trainingloss 0.6931471805599453
iteration 1 batch 6530 trainingloss 0.6916632528527511
iteration 1 batch 6540 trainingloss 0.6916632528527511
iteration 1 batch 6550 trainingloss 0.6931471805599453
iteration 1 batch 6560 trainingloss 0.6901793251455568
iteration 1 batch 6570 trainingloss 0.6931471805599453
iteration 1 batch 6580 trainingloss 0.6931471805599453
iteration 1 batch 6590 trainingloss 0.6931471805599453
iteration 1 batch 6600 trainingloss 0.6931471805599453
iteration 1 batch 6610 trainingloss 0.6931471805599453
iteration 1 batch 6620 trainingloss 0.6931471805599453
iteration 1 batch 6630 trainingloss 0.6931471805599453
iteration 1 batch 6640 trainingloss 0.6931471805599453
iteration 1 batch 6650 trainingloss 0.6931471805599453
iteration 1 batch 6660 trainingloss 0.6931471805599453
iteration 1 batch 6670 trainingloss 0.6931471805599453
iteration 1 batch 6680 trainingloss 0.6916632528527511
iteration 1 batch 6690 trainingloss 0.6931471805599453
iteration 1 batch 6700 trainingloss 0.6931471805599453
iteration 1 batch 6710 trainingloss 0.6931471805599453
iteration 1 batch 6720 trainingloss 0.6916632528527511
iteration 1 batch 6730 trainingloss 0.6931471805599453
iteration 1 batch 6740 trainingloss 0.6931471805599453
iteration 1 batch 6750 trainingloss 0.6931471805599453
iteration 1 batch 6760 trainingloss 0.6931471805599453
iteration 1 batch 6770 trainingloss 0.6916632528527511
iteration 1 batch 6780 trainingloss 0.6931471805599453
iteration 1 batch 6790 trainingloss 0.6931471805599453
iteration 1 batch 6800 trainingloss 0.6931471805599453
iteration 1 batch 6810 trainingloss 0.6916632528527511
iteration 1 batch 6820 trainingloss 0.6931471805599453
iteration 1 batch 6830 trainingloss 0.6931471805599453
iteration 1 batch 6840 trainingloss 0.6931471805599453
iteration 1 batch 6850 trainingloss 0.6931471805599453
iteration 1 batch 6860 trainingloss 0.6931471805599453
iteration 1 batch 6870 trainingloss 0.6931471805599453
iteration 1 batch 6880 trainingloss 0.6916632528527511
iteration 1 batch 6890 trainingloss 0.6931471805599453
iteration 1 batch 6900 trainingloss 0.6931471805599453
iteration 1 batch 6910 trainingloss 0.6931471805599453
iteration 1 batch 6920 trainingloss 0.6931471805599453
iteration 1 batch 6930 trainingloss 0.6931471805599453
iteration 1 batch 6940 trainingloss 0.6931471805599453
iteration 1 batch 6950 trainingloss 0.6916632528527511
iteration 1 batch 6960 trainingloss 0.6931471805599453
iteration 1 batch 6970 trainingloss 0.6931471805599453
iteration 1 batch 6980 trainingloss 0.6931471805599453
iteration 1 batch 6990 trainingloss 0.6931471805599453
iteration 1 batch 7000 trainingloss 0.6931471805599453
iteration 1 batch 7010 trainingloss 0.6931471805599453
iteration 1 batch 7020 trainingloss 0.6931471805599453
iteration 1 batch 7030 trainingloss 0.6916632528527511
iteration 1 batch 7040 trainingloss 0.6916632528527511
iteration 1 batch 7050 trainingloss 0.6931471805599453
iteration 1 batch 7060 trainingloss 0.6931471805599453
iteration 1 batch 7070 trainingloss 0.6931471805599453
iteration 1 batch 7080 trainingloss 0.6931471805599453
iteration 1 batch 7090 trainingloss 0.6931471805599453
iteration 1 batch 7100 trainingloss 0.6931471805599453
iteration 1 batch 7110 trainingloss 0.6931471805599453
iteration 1 batch 7120 trainingloss 0.6931471805599453
iteration 1 batch 7130 trainingloss 0.6916632528527511
iteration 1 batch 7140 trainingloss 0.6916632528527511
iteration 1 batch 7150 trainingloss 0.6931471805599453
iteration 1 batch 7160 trainingloss 0.6931471805599453
iteration 1 batch 7170 trainingloss 0.6931471805599453
iteration 1 batch 7180 trainingloss 0.6931471805599453
iteration 1 batch 7190 trainingloss 0.6931471805599453
iteration 1 batch 7200 trainingloss 0.6931471805599453
iteration 1 batch 7210 trainingloss 0.6931471805599453
iteration 1 batch 7220 trainingloss 0.6931471805599453
iteration 1 batch 7230 trainingloss 0.6931471805599453
iteration 1 batch 7240 trainingloss 0.6931471805599453
iteration 1 batch 7250 trainingloss 0.6901793251455568
iteration 1 batch 7260 trainingloss 0.6931471805599453
iteration 1 batch 7270 trainingloss 0.6931471805599453
iteration 1 batch 7280 trainingloss 0.6931471805599453
iteration 1 batch 7290 trainingloss 0.6931471805599453
iteration 1 batch 7300 trainingloss 0.6916632528527511
iteration 1 batch 7310 trainingloss 0.6931471805599453
iteration 1 batch 7320 trainingloss 0.6916632528527511
iteration 1 batch 7330 trainingloss 0.6916632528527511
iteration 1 batch 7340 trainingloss 0.6931471805599453
iteration 1 batch 7350 trainingloss 0.6931471805599453
iteration 1 batch 7360 trainingloss 0.6931471805599453
iteration 1 batch 7370 trainingloss 0.6886953974383626
iteration 1 batch 7380 trainingloss 0.6931471805599453
iteration 1 batch 7390 trainingloss 0.6931471805599453
iteration 1 batch 7400 trainingloss 0.6931471805599453
iteration 1 batch 7410 trainingloss 0.6931471805599453
iteration 1 batch 7420 trainingloss 0.6931471805599453
iteration 1 batch 7430 trainingloss 0.6931471805599453
iteration 1 batch 7440 trainingloss 0.6931471805599453
iteration 1 batch 7450 trainingloss 0.6916632528527511
iteration 1 batch 7460 trainingloss 0.6916632528527511
iteration 1 batch 7470 trainingloss 0.6931471805599453
iteration 1 batch 7480 trainingloss 0.6931471805599453
iteration 1 batch 7490 trainingloss 0.6931471805599453
iteration 1 batch 7500 trainingloss 0.6916632528527511
iteration 1 batch 7510 trainingloss 0.6931471805599453
iteration 1 batch 7520 trainingloss 0.6931471805599453
iteration 1 batch 7530 trainingloss 0.6931471805599453
iteration 1 batch 7540 trainingloss 0.6931471805599453
iteration 1 batch 7550 trainingloss 0.6931471805599453
iteration 1 batch 7560 trainingloss 0.6931471805599453
iteration 1 batch 7570 trainingloss 0.6931471805599453
iteration 1 batch 7580 trainingloss 0.6931471805599453
iteration 1 batch 7590 trainingloss 0.6931471805599453
iteration 1 batch 7600 trainingloss 0.6931471805599453
iteration 1 batch 7610 trainingloss 0.6931471805599453
iteration 1 batch 7620 trainingloss 0.6931471805599453
iteration 1 batch 7630 trainingloss 0.6931471805599453
iteration 1 batch 7640 trainingloss 0.6931471805599453
iteration 1 batch 7650 trainingloss 0.6931471805599453
iteration 1 batch 7660 trainingloss 0.6931471805599453
iteration 1 batch 7670 trainingloss 0.6931471805599453
iteration 1 batch 7680 trainingloss 0.6931471805599453
iteration 1 batch 7690 trainingloss 0.6916632528527511
iteration 1 batch 7700 trainingloss 0.6931471805599453
iteration 1 batch 7710 trainingloss 0.6931471805599453
iteration 1 batch 7720 trainingloss 0.6916632528527511
iteration 1 batch 7730 trainingloss 0.6931471805599453
iteration 1 batch 7740 trainingloss 0.6931471805599453
iteration 1 batch 7750 trainingloss 0.6931471805599453
iteration 1 batch 7760 trainingloss 0.6916632528527511
iteration 1 batch 7770 trainingloss 0.6931471805599453
iteration 1 batch 7780 trainingloss 0.6916632528527511
iteration 1 batch 7790 trainingloss 0.6916632528527511
iteration 1 batch 7800 trainingloss 0.6916632528527511
iteration 1 batch 7810 trainingloss 0.6931471805599453
iteration 1 batch 7820 trainingloss 0.6916632528527511
iteration 1 batch 7830 trainingloss 0.6916632528527511
iteration 1 batch 7840 trainingloss 0.6931471805599453
iteration 1 batch 7850 trainingloss 0.6931471805599453
iteration 1 batch 7860 trainingloss 0.6931471805599453
iteration 1 batch 7870 trainingloss 0.6931471805599453
iteration 1 batch 7880 trainingloss 0.6931471805599453
iteration 1 batch 7890 trainingloss 0.6931471805599453
iteration 1 batch 7900 trainingloss 0.6931471805599453
iteration 1 batch 7910 trainingloss 0.6931471805599453
iteration 1 batch 7920 trainingloss 0.6931471805599453
iteration 1 batch 7930 trainingloss 0.6931471805599453
iteration 1 batch 7940 trainingloss 0.6931471805599453
iteration 1 batch 7950 trainingloss 0.6931471805599453
iteration 1 batch 7960 trainingloss 0.6931471805599453
iteration 1 batch 7970 trainingloss 0.6931471805599453
iteration 1 batch 7980 trainingloss 0.6931471805599453
iteration 1 batch 7990 trainingloss 0.6931471805599453
iteration 1 batch 8000 trainingloss 0.6931471805599453
iteration 1 batch 8010 trainingloss 0.6931471805599453
iteration 1 batch 8020 trainingloss 0.6931471805599453
iteration 1 batch 8030 trainingloss 0.6931471805599453
iteration 1 batch 8040 trainingloss 0.6931471805599453
iteration 1 batch 8050 trainingloss 0.6931471805599453
iteration 1 batch 8060 trainingloss 0.6931471805599453
iteration 1 batch 8070 trainingloss 0.6931471805599453
iteration 1 batch 8080 trainingloss 0.6901793251455568
iteration 1 batch 8090 trainingloss 0.6931471805599453
iteration 1 batch 8100 trainingloss 0.6931471805599453
iteration 1 batch 8110 trainingloss 0.6931471805599453
iteration 1 batch 8120 trainingloss 0.6931471805599453
iteration 1 batch 8130 trainingloss 0.6931471805599453
iteration 1 batch 8140 trainingloss 0.6931471805599453
iteration 1 batch 8150 trainingloss 0.6916632528527511
iteration 1 batch 8160 trainingloss 0.6931471805599453
iteration 1 batch 8170 trainingloss 0.6931471805599453
iteration 1 batch 8180 trainingloss 0.6931471805599453
iteration 1 batch 8190 trainingloss 0.6931471805599453
iteration 1 batch 8200 trainingloss 0.6931471805599453
iteration 1 batch 8210 trainingloss 0.6931471805599453
iteration 1 batch 8220 trainingloss 0.6931471805599453
iteration 1 batch 8230 trainingloss 0.6931471805599453
iteration 1 batch 8240 trainingloss 0.6931471805599453
iteration 1 batch 8250 trainingloss 0.6916632528527511
iteration 1 batch 8260 trainingloss 0.6931471805599453
iteration 1 batch 8270 trainingloss 0.6931471805599453
iteration 1 batch 8280 trainingloss 0.6931471805599453
iteration 1 batch 8290 trainingloss 0.6901793251455568
iteration 1 batch 8300 trainingloss 0.6931471805599453
iteration 1 batch 8310 trainingloss 0.6931471805599453
iteration 1 batch 8320 trainingloss 0.6931471805599453
iteration 1 batch 8330 trainingloss 0.6931471805599453
iteration 1 batch 8340 trainingloss 0.6916632528527511
iteration 1 batch 8350 trainingloss 0.6931471805599453
iteration 1 batch 8360 trainingloss 0.6931471805599453
iteration 1 batch 8370 trainingloss 0.6931471805599453
iteration 1 batch 8380 trainingloss 0.6931471805599453
iteration 1 batch 8390 trainingloss 0.6931471805599453
iteration 1 batch 8400 trainingloss 0.6931471805599453
iteration 1 batch 8410 trainingloss 0.6931471805599453
iteration 1 batch 8420 trainingloss 0.6931471805599453
iteration 1 batch 8430 trainingloss 0.6931471805599453
iteration 1 batch 8440 trainingloss 0.6931471805599453
iteration 1 batch 8450 trainingloss 0.6931471805599453
iteration 1 batch 8460 trainingloss 0.6916632528527511
iteration 1 batch 8470 trainingloss 0.6931471805599453
iteration 1 batch 8480 trainingloss 0.6931471805599453
iteration 1 batch 8490 trainingloss 0.6931471805599453
iteration 1 batch 8500 trainingloss 0.6916632528527511
iteration 1 batch 8510 trainingloss 0.6931471805599453
iteration 1 batch 8520 trainingloss 0.6916632528527511
iteration 1 batch 8530 trainingloss 0.6931471805599453
iteration 1 batch 8540 trainingloss 0.6931471805599453
iteration 1 batch 8550 trainingloss 0.6931471805599453
iteration 1 batch 8560 trainingloss 0.6931471805599453
iteration 1 batch 8570 trainingloss 0.6931471805599453
iteration 1 batch 8580 trainingloss 0.6916632528527511
iteration 1 batch 8590 trainingloss 0.6931471805599453
iteration 1 batch 8600 trainingloss 0.6931471805599453
iteration 1 batch 8610 trainingloss 0.6901793251455568
iteration 1 batch 8620 trainingloss 0.6931471805599453
iteration 1 batch 8630 trainingloss 0.6931471805599453
iteration 1 batch 8640 trainingloss 0.6931471805599453
iteration 1 batch 8650 trainingloss 0.6931471805599453
iteration 1 batch 8660 trainingloss 0.6931471805599453
iteration 1 batch 8670 trainingloss 0.6931471805599453
iteration 1 batch 8680 trainingloss 0.6916632528527511
iteration 1 batch 8690 trainingloss 0.6931471805599453
iteration 1 batch 8700 trainingloss 0.6916632528527511
iteration 1 batch 8710 trainingloss 0.6931471805599453
iteration 1 batch 8720 trainingloss 0.6931471805599453
iteration 1 batch 8730 trainingloss 0.6931471805599453
iteration 1 batch 8740 trainingloss 0.6931471805599453
iteration 1 batch 8750 trainingloss 0.6931471805599453
iteration 1 batch 8760 trainingloss 0.6931471805599453
iteration 1 batch 8770 trainingloss 0.6931471805599453
iteration 1 batch 8780 trainingloss 0.6931471805599453
iteration 1 batch 8790 trainingloss 0.6931471805599453
iteration 1 batch 8800 trainingloss 0.6931471805599453
iteration 1 batch 8810 trainingloss 0.6931471805599453
iteration 1 batch 8820 trainingloss 0.6931471805599453
iteration 1 batch 8830 trainingloss 0.6931471805599453
iteration 1 batch 8840 trainingloss 0.6931471805599453
iteration 1 batch 8850 trainingloss 0.6931471805599453
iteration 1 batch 8860 trainingloss 0.6916632528527511
iteration 1 batch 8870 trainingloss 0.6916632528527511
iteration 1 batch 8880 trainingloss 0.6931471805599453
iteration 1 batch 8890 trainingloss 0.6931471805599453
iteration 1 batch 8900 trainingloss 0.6931471805599453
iteration 1 batch 8910 trainingloss 0.6931471805599453
iteration 1 batch 8920 trainingloss 0.6931471805599453
iteration 1 batch 8930 trainingloss 0.6931471805599453
iteration 1 batch 8940 trainingloss 0.6931471805599453
iteration 1 batch 8950 trainingloss 0.6931471805599453
iteration 1 batch 8960 trainingloss 0.6931471805599453
iteration 1 batch 8970 trainingloss 0.6931471805599453
iteration 1 batch 8980 trainingloss 0.6931471805599453
iteration 1 batch 8990 trainingloss 0.6931471805599453
iteration 1 batch 9000 trainingloss 0.6931471805599453
iteration 1 batch 9010 trainingloss 0.6931471805599453
iteration 1 batch 9020 trainingloss 0.6931471805599453
iteration 1 batch 9030 trainingloss 0.6916632528527511
iteration 1 batch 9040 trainingloss 0.6931471805599453
iteration 1 batch 9050 trainingloss 0.6931471805599453
iteration 1 batch 9060 trainingloss 0.6931471805599453
iteration 1 batch 9070 trainingloss 0.6931471805599453
iteration 1 batch 9080 trainingloss 0.6931471805599453
iteration 1 batch 9090 trainingloss 0.6931471805599453
iteration 1 batch 9100 trainingloss 0.6931471805599453
iteration 1 batch 9110 trainingloss 0.6931471805599453
iteration 1 batch 9120 trainingloss 0.6931471805599453
iteration 1 batch 9130 trainingloss 0.6931471805599453
iteration 1 batch 9140 trainingloss 0.6931471805599453
iteration 1 batch 9150 trainingloss 0.6916632528527511
iteration 1 batch 9160 trainingloss 0.6931471805599453
iteration 1 batch 9170 trainingloss 0.6931471805599453
iteration 1 batch 9180 trainingloss 0.6931471805599453
iteration 1 batch 9190 trainingloss 0.6916632528527511
iteration 1 batch 9200 trainingloss 0.6931471805599453
iteration 1 batch 9210 trainingloss 0.6931471805599453
iteration 1 batch 9220 trainingloss 0.6901793251455568
iteration 1 batch 9230 trainingloss 0.6916632528527511
iteration 1 batch 9240 trainingloss 0.6931471805599453
iteration 1 batch 9250 trainingloss 0.6931471805599453
iteration 1 batch 9260 trainingloss 0.6931471805599453
iteration 1 batch 9270 trainingloss 0.6931471805599453
iteration 1 batch 9280 trainingloss 0.6931471805599453
iteration 1 batch 9290 trainingloss 0.6931471805599453
iteration 1 batch 9300 trainingloss 0.6931471805599453
iteration 1 batch 9310 trainingloss 0.6931471805599453
iteration 1 batch 9320 trainingloss 0.6931471805599453
iteration 1 batch 9330 trainingloss 0.6931471805599453
iteration 1 batch 9340 trainingloss 0.6931471805599453
iteration 1 batch 9350 trainingloss 0.6931471805599453
iteration 1 batch 9360 trainingloss 0.6931471805599453
iteration 1 batch 9370 trainingloss 0.6901793251455568
iteration 1 batch 9380 trainingloss 0.6916632528527511
iteration 1 batch 9390 trainingloss 0.6931471805599453
iteration 1 batch 9400 trainingloss 0.6916632528527511
iteration 1 batch 9410 trainingloss 0.6931471805599453
iteration 1 batch 9420 trainingloss 0.6931471805599453
iteration 1 batch 9430 trainingloss 0.6931471805599453
iteration 1 batch 9440 trainingloss 0.6901793251455568
iteration 1 batch 9450 trainingloss 0.6931471805599453
iteration 1 batch 9460 trainingloss 0.6931471805599453
iteration 1 batch 9470 trainingloss 0.6931471805599453
iteration 1 batch 9480 trainingloss 0.6916632528527511
iteration 1 batch 9490 trainingloss 0.6931471805599453
iteration 1 batch 9500 trainingloss 0.6931471805599453
iteration 1 batch 9510 trainingloss 0.6931471805599453
iteration 1 batch 9520 trainingloss 0.6931471805599453
iteration 1 batch 9530 trainingloss 0.6931471805599453
iteration 1 batch 9540 trainingloss 0.6931471805599453
iteration 1 batch 9550 trainingloss 0.6916632528527511
iteration 1 batch 9560 trainingloss 0.6931471805599453
iteration 1 batch 9570 trainingloss 0.6901793251455568
iteration 1 batch 9580 trainingloss 0.6931471805599453
iteration 1 batch 9590 trainingloss 0.6931471805599453
iteration 1 batch 9600 trainingloss 0.6931471805599453
iteration 1 batch 9610 trainingloss 0.6931471805599453
iteration 1 batch 9620 trainingloss 0.6931471805599453
iteration 1 batch 9630 trainingloss 0.6931471805599453
iteration 1 batch 9640 trainingloss 0.6931471805599453
iteration 1 batch 9650 trainingloss 0.6931471805599453
iteration 1 batch 9660 trainingloss 0.6916632528527511
iteration 1 batch 9670 trainingloss 0.6931471805599453
iteration 1 batch 9680 trainingloss 0.6931471805599453
iteration 1 batch 9690 trainingloss 0.6901793251455568
iteration 1 batch 9700 trainingloss 0.6931471805599453
iteration 1 batch 9710 trainingloss 0.6931471805599453
iteration 1 batch 9720 trainingloss 0.6931471805599453
iteration 1 batch 9730 trainingloss 0.6916632528527511
iteration 1 batch 9740 trainingloss 0.6931471805599453
iteration 1 batch 9750 trainingloss 0.6931471805599453
iteration 1 batch 9760 trainingloss 0.6931471805599453
iteration 1 batch 9770 trainingloss 0.6931471805599453
iteration 1 batch 9780 trainingloss 0.6931471805599453
iteration 1 batch 9790 trainingloss 0.6931471805599453
iteration 1 batch 9800 trainingloss 0.6931471805599453
iteration 1 batch 9810 trainingloss 0.6931471805599453
iteration 1 batch 9820 trainingloss 0.6931471805599453
iteration 1 batch 9830 trainingloss 0.6931471805599453
iteration 1 batch 9840 trainingloss 0.6931471805599453
iteration 1 batch 9850 trainingloss 0.6931471805599453
iteration 1 batch 9860 trainingloss 0.6931471805599453
iteration 1 batch 9870 trainingloss 0.6931471805599453
iteration 1 batch 9880 trainingloss 0.6916632528527511
iteration 1 batch 9890 trainingloss 0.6931471805599453
iteration 1 batch 9900 trainingloss 0.6931471805599453
iteration 1 batch 9910 trainingloss 0.6931471805599453
iteration 1 batch 9920 trainingloss 0.6931471805599453
iteration 1 batch 9930 trainingloss 0.6931471805599453
iteration 1 batch 9940 trainingloss 0.6931471805599453
iteration 1 batch 9950 trainingloss 0.6931471805599453
iteration 1 batch 9960 trainingloss 0.6931471805599453
iteration 1 batch 9970 trainingloss 0.6916632528527511
iteration 1 batch 9980 trainingloss 0.6931471805599453
iteration 1 batch 9990 trainingloss 0.6931471805599453
iteration 1 batch 10000 trainingloss 0.6931471805599453
iteration 1 batch 10010 trainingloss 0.6931471805599453
iteration 1 batch 10020 trainingloss 0.6931471805599453
iteration 1 batch 10030 trainingloss 0.6931471805599453
iteration 1 batch 10040 trainingloss 0.6931471805599453
iteration 1 batch 10050 trainingloss 0.6931471805599453
iteration 1 batch 10060 trainingloss 0.6931471805599453
iteration 1 batch 10070 trainingloss 0.6931471805599453
iteration 1 batch 10080 trainingloss 0.6931471805599453
iteration 1 batch 10090 trainingloss 0.6931471805599453
iteration 1 batch 10100 trainingloss 0.6931471805599453
iteration 1 batch 10110 trainingloss 0.6916632528527511
iteration 1 batch 10120 trainingloss 0.6931471805599453
iteration 1 batch 10130 trainingloss 0.6931471805599453
iteration 1 batch 10140 trainingloss 0.6931471805599453
iteration 1 batch 10150 trainingloss 0.6916632528527511
iteration 1 batch 10160 trainingloss 0.6931471805599453
iteration 1 batch 10170 trainingloss 0.6931471805599453
iteration 1 batch 10180 trainingloss 0.6916632528527511
iteration 1 batch 10190 trainingloss 0.6931471805599453
iteration 1 batch 10200 trainingloss 0.6931471805599453
iteration 1 batch 10210 trainingloss 0.6916632528527511
iteration 1 batch 10220 trainingloss 0.6901793251455568
iteration 1 batch 10230 trainingloss 0.6916632528527511
iteration 1 batch 10240 trainingloss 0.6931471805599453
iteration 1 batch 10250 trainingloss 0.6931471805599453
iteration 1 batch 10260 trainingloss 0.6931471805599453
iteration 1 batch 10270 trainingloss 0.6916632528527511
iteration 1 batch 10280 trainingloss 0.6931471805599453
iteration 1 batch 10290 trainingloss 0.6916632528527511
iteration 1 batch 10300 trainingloss 0.6931471805599453
iteration 1 batch 10310 trainingloss 0.6931471805599453
iteration 1 batch 10320 trainingloss 0.6931471805599453
iteration 1 batch 10330 trainingloss 0.6931471805599453
iteration 1 batch 10340 trainingloss 0.6916632528527511
iteration 1 batch 10350 trainingloss 0.6916632528527511
iteration 1 batch 10360 trainingloss 0.6931471805599453
iteration 1 batch 10370 trainingloss 0.6916632528527511
iteration 1 batch 10380 trainingloss 0.6931471805599453
iteration 1 batch 10390 trainingloss 0.6916632528527511
iteration 1 batch 10400 trainingloss 0.6931471805599453
iteration 1 batch 10410 trainingloss 0.6931471805599453
iteration 1 batch 10420 trainingloss 0.6931471805599453
iteration 1 batch 10430 trainingloss 0.6931471805599453
iteration 1 batch 10440 trainingloss 0.6916632528527511
iteration 1 batch 10450 trainingloss 0.6931471805599453
iteration 1 batch 10460 trainingloss 0.6931471805599453
iteration 1 batch 10470 trainingloss 0.6931471805599453
iteration 1 batch 10480 trainingloss 0.6931471805599453
iteration 1 batch 10490 trainingloss 0.6931471805599453
iteration 1 batch 10500 trainingloss 0.6931471805599453
iteration 1 batch 10510 trainingloss 0.6931471805599453
iteration 1 batch 10520 trainingloss 0.6931471805599453
iteration 1 batch 10530 trainingloss 0.6931471805599453
iteration 1 batch 10540 trainingloss 0.6931471805599453
iteration 1 batch 10550 trainingloss 0.6931471805599453
iteration 1 batch 10560 trainingloss 0.6916632528527511
iteration 1 batch 10570 trainingloss 0.6931471805599453
iteration 1 batch 10580 trainingloss 0.6916632528527511
iteration 1 batch 10590 trainingloss 0.6931471805599453
iteration 1 batch 10600 trainingloss 0.6931471805599453
iteration 1 batch 10610 trainingloss 0.6931471805599453
iteration 1 batch 10620 trainingloss 0.6931471805599453
iteration 1 batch 10630 trainingloss 0.6931471805599453
iteration 1 batch 10640 trainingloss 0.6931471805599453
iteration 1 batch 10650 trainingloss 0.6931471805599453
iteration 1 batch 10660 trainingloss 0.6916632528527511
iteration 1 batch 10670 trainingloss 0.6931471805599453
iteration 1 batch 10680 trainingloss 0.6931471805599453
iteration 1 batch 10690 trainingloss 0.6931471805599453
iteration 1 batch 10700 trainingloss 0.6931471805599453
iteration 1 batch 10710 trainingloss 0.6931471805599453
iteration 1 batch 10720 trainingloss 0.6916632528527511
iteration 1 batch 10730 trainingloss 0.6931471805599453
iteration 1 batch 10740 trainingloss 0.6931471805599453
iteration 1 batch 10750 trainingloss 0.6931471805599453
iteration 1 batch 10760 trainingloss 0.6931471805599453
iteration 1 batch 10770 trainingloss 0.6931471805599453
iteration 1 batch 10780 trainingloss 0.6931471805599453
iteration 1 batch 10790 trainingloss 0.6931471805599453
iteration 1 batch 10800 trainingloss 0.6931471805599453
iteration 1 batch 10810 trainingloss 0.6931471805599453
iteration 1 batch 10820 trainingloss 0.6931471805599453
iteration 1 batch 10830 trainingloss 0.6931471805599453
iteration 1 batch 10840 trainingloss 0.6931471805599453
iteration 1 batch 10850 trainingloss 0.6916632528527511
iteration 1 batch 10860 trainingloss 0.6901793251455568
iteration 1 batch 10870 trainingloss 0.6931471805599453
iteration 1 batch 10880 trainingloss 0.6916632528527511
iteration 1 batch 10890 trainingloss 0.6931471805599453
iteration 1 batch 10900 trainingloss 0.6901793251455568
iteration 1 batch 10910 trainingloss 0.6916632528527511
iteration 1 batch 10920 trainingloss 0.6931471805599453
iteration 1 batch 10930 trainingloss 0.6931471805599453
iteration 1 batch 10940 trainingloss 0.6931471805599453
iteration 1 batch 10950 trainingloss 0.6931471805599453
iteration 1 batch 10960 trainingloss 0.6931471805599453
iteration 1 batch 10970 trainingloss 0.6931471805599453
iteration 1 batch 10980 trainingloss 0.6916632528527511
iteration 1 batch 10990 trainingloss 0.6931471805599453
iteration 1 batch 11000 trainingloss 0.6931471805599453
iteration 1 batch 11010 trainingloss 0.6931471805599453
iteration 1 batch 11020 trainingloss 0.6931471805599453
iteration 1 batch 11030 trainingloss 0.6931471805599453
iteration 1 batch 11040 trainingloss 0.6931471805599453
iteration 1 batch 11050 trainingloss 0.6916632528527511
iteration 1 batch 11060 trainingloss 0.6931471805599453
iteration 1 batch 11070 trainingloss 0.6916632528527511
iteration 1 batch 11080 trainingloss 0.6916632528527511
iteration 1 batch 11090 trainingloss 0.6931471805599453
iteration 1 batch 11100 trainingloss 0.6931471805599453
iteration 1 batch 11110 trainingloss 0.6916632528527511
iteration 1 batch 11120 trainingloss 0.6931471805599453
iteration 1 batch 11130 trainingloss 0.6931471805599453
iteration 1 batch 11140 trainingloss 0.6931471805599453
iteration 1 batch 11150 trainingloss 0.6931471805599453
iteration 1 batch 11160 trainingloss 0.6931471805599453
iteration 1 batch 11170 trainingloss 0.6916632528527511
iteration 1 batch 11180 trainingloss 0.6916632528527511
iteration 1 batch 11190 trainingloss 0.6931471805599453
iteration 1 batch 11200 trainingloss 0.6931471805599453
iteration 1 batch 11210 trainingloss 0.6931471805599453
iteration 1 batch 11220 trainingloss 0.6931471805599453
iteration 1 batch 11230 trainingloss 0.6931471805599453
iteration 1 batch 11240 trainingloss 0.6931471805599453
iteration 1 batch 11250 trainingloss 0.6931471805599453
iteration 1 batch 11260 trainingloss 0.6931471805599453
iteration 1 batch 11270 trainingloss 0.6931471805599453
iteration 1 batch 11280 trainingloss 0.6931471805599453
iteration 1 batch 11290 trainingloss 0.6931471805599453
iteration 1 batch 11300 trainingloss 0.6916632528527511
iteration 1 batch 11310 trainingloss 0.6931471805599453
iteration 1 batch 11320 trainingloss 0.6916632528527511
iteration 1 batch 11330 trainingloss 0.6931471805599453
iteration 1 batch 11340 trainingloss 0.6931471805599453
iteration 1 batch 11350 trainingloss 0.6931471805599453
iteration 1 batch 11360 trainingloss 0.6931471805599453
iteration 1 batch 11370 trainingloss 0.6931471805599453
iteration 1 batch 11380 trainingloss 0.6931471805599453
iteration 1 batch 11390 trainingloss 0.6901793251455568
iteration 1 batch 11400 trainingloss 0.6931471805599453
iteration 1 batch 11410 trainingloss 0.6901793251455568
iteration 1 batch 11420 trainingloss 0.6931471805599453
iteration 1 batch 11430 trainingloss 0.6931471805599453
iteration 1 batch 11440 trainingloss 0.6931471805599453
iteration 1 batch 11450 trainingloss 0.6931471805599453
iteration 1 batch 11460 trainingloss 0.6931471805599453
iteration 1 batch 11470 trainingloss 0.6931471805599453
iteration 1 batch 11480 trainingloss 0.6931471805599453
iteration 1 batch 11490 trainingloss 0.6931471805599453
iteration 1 batch 11500 trainingloss 0.6931471805599453
iteration 1 batch 11510 trainingloss 0.6931471805599453
iteration 1 batch 11520 trainingloss 0.6916632528527511
iteration 1 batch 11530 trainingloss 0.6931471805599453
iteration 1 batch 11540 trainingloss 0.6931471805599453
iteration 1 batch 11550 trainingloss 0.6931471805599453
iteration 1 batch 11560 trainingloss 0.6916632528527511
iteration 1 batch 11570 trainingloss 0.6931471805599453
iteration 1 batch 11580 trainingloss 0.6931471805599453
iteration 1 batch 11590 trainingloss 0.6931471805599453
iteration 1 batch 11600 trainingloss 0.6916632528527511
iteration 1 batch 11610 trainingloss 0.6931471805599453
iteration 1 batch 11620 trainingloss 0.6931471805599453
iteration 1 batch 11630 trainingloss 0.6931471805599453
iteration 1 batch 11640 trainingloss 0.6931471805599453
iteration 1 batch 11650 trainingloss 0.6916632528527511
iteration 1 batch 11660 trainingloss 0.6931471805599453
iteration 1 batch 11670 trainingloss 0.6931471805599453
iteration 1 batch 11680 trainingloss 0.6931471805599453
iteration 1 batch 11690 trainingloss 0.6931471805599453
iteration 1 batch 11700 trainingloss 0.6931471805599453
iteration 1 batch 11710 trainingloss 0.6916632528527511
iteration 1 batch 11720 trainingloss 0.6931471805599453
iteration 1 batch 11730 trainingloss 0.6916632528527511
iteration 1 batch 11740 trainingloss 0.6931471805599453
iteration 1 batch 11750 trainingloss 0.6916632528527511
iteration 1 batch 11760 trainingloss 0.6931471805599453
iteration 1 batch 11770 trainingloss 0.6931471805599453
iteration 1 batch 11780 trainingloss 0.6901793251455568
iteration 1 batch 11790 trainingloss 0.6931471805599453
iteration 1 batch 11800 trainingloss 0.6931471805599453
iteration 1 batch 11810 trainingloss 0.6931471805599453
iteration 1 batch 11820 trainingloss 0.6916632528527511
iteration 1 batch 11830 trainingloss 0.6931471805599453
iteration 1 batch 11840 trainingloss 0.6931471805599453
iteration 1 batch 11850 trainingloss 0.6931471805599453
iteration 1 batch 11860 trainingloss 0.6931471805599453
iteration 1 batch 11870 trainingloss 0.6916632528527511
iteration 1 batch 11880 trainingloss 0.6931471805599453
iteration 1 batch 11890 trainingloss 0.6931471805599453
iteration 1 batch 11900 trainingloss 0.6931471805599453
iteration 1 batch 11910 trainingloss 0.6931471805599453
iteration 1 batch 11920 trainingloss 0.6931471805599453
iteration 1 batch 11930 trainingloss 0.6931471805599453
iteration 1 batch 11940 trainingloss 0.6931471805599453
iteration 1 batch 11950 trainingloss 0.6931471805599453
iteration 1 batch 11960 trainingloss 0.6931471805599453
iteration 1 batch 11970 trainingloss 0.6916632528527511
iteration 1 batch 11980 trainingloss 0.6931471805599453
iteration 1 batch 11990 trainingloss 0.6931471805599453
iteration 1 batch 12000 trainingloss 0.6916632528527511
iteration 1 batch 12010 trainingloss 0.6916632528527511
iteration 1 batch 12020 trainingloss 0.6931471805599453
iteration 1 batch 12030 trainingloss 0.6931471805599453
iteration 1 batch 12040 trainingloss 0.6931471805599453
iteration 1 batch 12050 trainingloss 0.6931471805599453
iteration 1 batch 12060 trainingloss 0.6931471805599453
iteration 1 batch 12070 trainingloss 0.6931471805599453
iteration 1 batch 12080 trainingloss 0.6931471805599453
iteration 1 batch 12090 trainingloss 0.6931471805599453
iteration 1 batch 12100 trainingloss 0.6931471805599453
iteration 1 batch 12110 trainingloss 0.6931471805599453
iteration 1 batch 12120 trainingloss 0.6931471805599453
iteration 1 batch 12130 trainingloss 0.6931471805599453
iteration 1 batch 12140 trainingloss 0.6931471805599453
iteration 1 batch 12150 trainingloss 0.6931471805599453
iteration 1 batch 12160 trainingloss 0.6931471805599453
iteration 1 batch 12170 trainingloss 0.6931471805599453
iteration 1 batch 12180 trainingloss 0.6931471805599453
iteration 1 batch 12190 trainingloss 0.6931471805599453
iteration 1 batch 12200 trainingloss 0.6931471805599453
iteration 1 batch 12210 trainingloss 0.6931471805599453
iteration 1 batch 12220 trainingloss 0.6901793251455568
iteration 1 batch 12230 trainingloss 0.6931471805599453
iteration 1 batch 12240 trainingloss 0.6931471805599453
iteration 1 batch 12250 trainingloss 0.6916632528527511
iteration 1 batch 12260 trainingloss 0.6931471805599453
iteration 1 batch 12270 trainingloss 0.6931471805599453
iteration 1 batch 12280 trainingloss 0.6931471805599453
iteration 1 batch 12290 trainingloss 0.6931471805599453
iteration 1 batch 12300 trainingloss 0.6931471805599453
iteration 1 batch 12310 trainingloss 0.6931471805599453
iteration 1 batch 12320 trainingloss 0.6931471805599453
iteration 1 batch 12330 trainingloss 0.6931471805599453
iteration 1 batch 12340 trainingloss 0.6931471805599453
iteration 1 batch 12350 trainingloss 0.6931471805599453
iteration 1 batch 12360 trainingloss 0.6931471805599453
iteration 1 batch 12370 trainingloss 0.6931471805599453
iteration 1 batch 12380 trainingloss 0.6916632528527511
iteration 1 batch 12390 trainingloss 0.6931471805599453
iteration 1 batch 12400 trainingloss 0.6931471805599453
iteration 1 batch 12410 trainingloss 0.6916632528527511
iteration 1 batch 12420 trainingloss 0.6931471805599453
iteration 1 batch 12430 trainingloss 0.6931471805599453
iteration 1 batch 12440 trainingloss 0.6931471805599453
iteration 1 batch 12450 trainingloss 0.6931471805599453
iteration 1 batch 12460 trainingloss 0.6931471805599453
iteration 1 batch 12470 trainingloss 0.6931471805599453
iteration 1 batch 12480 trainingloss 0.6931471805599453
iteration 1 batch 12490 trainingloss 0.6916632528527511
iteration 1 batch 12500 trainingloss 0.6931471805599453
iteration 1 batch 12510 trainingloss 0.6931471805599453
iteration 1 batch 12520 trainingloss 0.6931471805599453
iteration 1 batch 12530 trainingloss 0.6931471805599453
iteration 1 batch 12540 trainingloss 0.6931471805599453
iteration 1 batch 12550 trainingloss 0.6931471805599453
iteration 1 batch 12560 trainingloss 0.6931471805599453
iteration 1 batch 12570 trainingloss 0.6931471805599453
iteration 1 batch 12580 trainingloss 0.6931471805599453
iteration 1 batch 12590 trainingloss 0.6931471805599453
iteration 1 batch 12600 trainingloss 0.6931471805599453
iteration 1 batch 12610 trainingloss 0.6931471805599453
iteration 1 batch 12620 trainingloss 0.6931471805599453
iteration 1 batch 12630 trainingloss 0.6931471805599453
iteration 1 batch 12640 trainingloss 0.6931471805599453
iteration 1 batch 12650 trainingloss 0.6931471805599453
iteration 1 batch 12660 trainingloss 0.6931471805599453
iteration 1 batch 12670 trainingloss 0.6931471805599453
iteration 1 batch 12680 trainingloss 0.6916632528527511
iteration 1 batch 12690 trainingloss 0.6916632528527511
iteration 1 batch 12700 trainingloss 0.6931471805599453
iteration 1 batch 12710 trainingloss 0.6931471805599453
iteration 1 batch 12720 trainingloss 0.6931471805599453
iteration 1 batch 12730 trainingloss 0.6931471805599453
iteration 1 batch 12740 trainingloss 0.6931471805599453
iteration 1 batch 12750 trainingloss 0.6931471805599453
iteration 1 batch 12760 trainingloss 0.6916632528527511
iteration 1 batch 12770 trainingloss 0.6931471805599453
iteration 1 batch 12780 trainingloss 0.6931471805599453
iteration 1 batch 12790 trainingloss 0.6931471805599453
iteration 1 batch 12800 trainingloss 0.6916632528527511
iteration 1 batch 12810 trainingloss 0.6931471805599453
iteration 1 batch 12820 trainingloss 0.6931471805599453
iteration 1 batch 12830 trainingloss 0.6931471805599453
iteration 1 batch 12840 trainingloss 0.6931471805599453
iteration 1 batch 12850 trainingloss 0.6931471805599453
iteration 1 batch 12860 trainingloss 0.6931471805599453
iteration 1 batch 12870 trainingloss 0.6916632528527511
iteration 1 batch 12880 trainingloss 0.6931471805599453
iteration 1 batch 12890 trainingloss 0.6931471805599453
iteration 1 batch 12900 trainingloss 0.6931471805599453
iteration 1 batch 12910 trainingloss 0.6931471805599453
iteration 1 batch 12920 trainingloss 0.6916632528527511
iteration 1 batch 12930 trainingloss 0.6931471805599453
iteration 1 batch 12940 trainingloss 0.6916632528527511
iteration 1 batch 12950 trainingloss 0.6901793251455568
iteration 1 batch 12960 trainingloss 0.6931471805599453
iteration 1 batch 12970 trainingloss 0.6931471805599453
iteration 1 batch 12980 trainingloss 0.6931471805599453
iteration 1 batch 12990 trainingloss 0.6931471805599453
iteration 1 batch 13000 trainingloss 0.6931471805599453
iteration 1 batch 13010 trainingloss 0.6931471805599453
iteration 1 batch 13020 trainingloss 0.6931471805599453
iteration 1 batch 13030 trainingloss 0.6931471805599453
iteration 1 batch 13040 trainingloss 0.6931471805599453
iteration 1 batch 13050 trainingloss 0.6931471805599453
iteration 1 batch 13060 trainingloss 0.6931471805599453
iteration 1 batch 13070 trainingloss 0.6931471805599453
iteration 1 batch 13080 trainingloss 0.6931471805599453
iteration 1 batch 13090 trainingloss 0.6931471805599453
iteration 1 batch 13100 trainingloss 0.6931471805599453
iteration 1 batch 13110 trainingloss 0.6916632528527511
iteration 1 batch 13120 trainingloss 0.6931471805599453
iteration 1 batch 13130 trainingloss 0.6931471805599453
iteration 1 batch 13140 trainingloss 0.6931471805599453
iteration 1 batch 13150 trainingloss 0.6931471805599453
iteration 1 batch 13160 trainingloss 0.6931471805599453
iteration 1 batch 13170 trainingloss 0.6931471805599453
iteration 1 batch 13180 trainingloss 0.6916632528527511
iteration 1 batch 13190 trainingloss 0.6931471805599453
iteration 1 batch 13200 trainingloss 0.6931471805599453
iteration 1 batch 13210 trainingloss 0.6901793251455568
iteration 1 batch 13220 trainingloss 0.6931471805599453
iteration 1 batch 13230 trainingloss 0.6931471805599453
iteration 1 batch 13240 trainingloss 0.6931471805599453
iteration 1 batch 13250 trainingloss 0.6931471805599453
iteration 1 batch 13260 trainingloss 0.6931471805599453
iteration 1 batch 13270 trainingloss 0.6931471805599453
iteration 1 batch 13280 trainingloss 0.6931471805599453
iteration 1 batch 13290 trainingloss 0.6931471805599453
iteration 1 batch 13300 trainingloss 0.6931471805599453
iteration 1 batch 13310 trainingloss 0.6931471805599453
iteration 1 batch 13320 trainingloss 0.6916632528527511
iteration 1 batch 13330 trainingloss 0.6931471805599453
iteration 1 batch 13340 trainingloss 0.6931471805599453
iteration 1 batch 13350 trainingloss 0.6931471805599453
iteration 1 batch 13360 trainingloss 0.6931471805599453
iteration 1 batch 13370 trainingloss 0.6931471805599453
iteration 1 batch 13380 trainingloss 0.6931471805599453
iteration 1 batch 13390 trainingloss 0.6931471805599453
iteration 1 batch 13400 trainingloss 0.6901793251455568
iteration 1 batch 13410 trainingloss 0.6931471805599453
iteration 1 batch 13420 trainingloss 0.6931471805599453
iteration 1 batch 13430 trainingloss 0.6931471805599453
iteration 1 batch 13440 trainingloss 0.6916632528527511
iteration 1 batch 13450 trainingloss 0.6931471805599453
iteration 1 batch 13460 trainingloss 0.6931471805599453
iteration 1 batch 13470 trainingloss 0.6931471805599453
iteration 1 batch 13480 trainingloss 0.6931471805599453
iteration 1 batch 13490 trainingloss 0.6931471805599453
iteration 1 batch 13500 trainingloss 0.6931471805599453
iteration 1 batch 13510 trainingloss 0.6931471805599453
iteration 1 batch 13520 trainingloss 0.6931471805599453
iteration 1 batch 13530 trainingloss 0.6931471805599453
iteration 1 batch 13540 trainingloss 0.6931471805599453
iteration 1 batch 13550 trainingloss 0.6931471805599453
iteration 1 batch 13560 trainingloss 0.6931471805599453
iteration 1 batch 13570 trainingloss 0.6931471805599453
iteration 1 batch 13580 trainingloss 0.6931471805599453
iteration 1 batch 13590 trainingloss 0.6931471805599453
iteration 1 batch 13600 trainingloss 0.6931471805599453
iteration 1 batch 13610 trainingloss 0.6916632528527511
iteration 1 batch 13620 trainingloss 0.6916632528527511
iteration 1 batch 13630 trainingloss 0.6931471805599453
iteration 1 batch 13640 trainingloss 0.6931471805599453
iteration 1 batch 13650 trainingloss 0.6931471805599453
iteration 1 batch 13660 trainingloss 0.6931471805599453
iteration 1 batch 13670 trainingloss 0.6931471805599453
iteration 1 batch 13680 trainingloss 0.6916632528527511
iteration 1 batch 13690 trainingloss 0.6931471805599453
iteration 1 batch 13700 trainingloss 0.6931471805599453
iteration 1 batch 13710 trainingloss 0.6931471805599453
iteration 1 batch 13720 trainingloss 0.6931471805599453
iteration 1 batch 13730 trainingloss 0.6931471805599453
iteration 1 batch 13740 trainingloss 0.6931471805599453
iteration 1 batch 13750 trainingloss 0.6916632528527511
iteration 1 batch 13760 trainingloss 0.6931471805599453
iteration 1 batch 13770 trainingloss 0.6931471805599453
iteration 1 batch 13780 trainingloss 0.6931471805599453
iteration 1 batch 13790 trainingloss 0.6931471805599453
iteration 1 batch 13800 trainingloss 0.6916632528527511
iteration 1 batch 13810 trainingloss 0.6931471805599453
iteration 1 batch 13820 trainingloss 0.6931471805599453
iteration 1 batch 13830 trainingloss 0.6901793251455568
iteration 1 batch 13840 trainingloss 0.6931471805599453
iteration 1 batch 13850 trainingloss 0.6916632528527511
iteration 1 batch 13860 trainingloss 0.6931471805599453
iteration 1 batch 13870 trainingloss 0.6916632528527511
iteration 1 batch 13880 trainingloss 0.6931471805599453
iteration 1 batch 13890 trainingloss 0.6931471805599453
iteration 1 batch 13900 trainingloss 0.6916632528527511
iteration 1 batch 13910 trainingloss 0.6931471805599453
iteration 1 batch 13920 trainingloss 0.6931471805599453
iteration 1 batch 13930 trainingloss 0.6916632528527511
iteration 1 batch 13940 trainingloss 0.6931471805599453
iteration 1 batch 13950 trainingloss 0.6931471805599453
iteration 1 batch 13960 trainingloss 0.6931471805599453
iteration 1 batch 13970 trainingloss 0.6916632528527511
iteration 1 batch 13980 trainingloss 0.6931471805599453
iteration 1 batch 13990 trainingloss 0.6931471805599453
iteration 1 batch 14000 trainingloss 0.6931471805599453
iteration 1 batch 14010 trainingloss 0.6916632528527511
iteration 1 batch 14020 trainingloss 0.6916632528527511
iteration 1 batch 14030 trainingloss 0.6931471805599453
iteration 1 batch 14040 trainingloss 0.6931471805599453
iteration 1 batch 14050 trainingloss 0.6916632528527511
iteration 1 batch 14060 trainingloss 0.6916632528527511
iteration 1 batch 14070 trainingloss 0.6931471805599453
iteration 1 batch 14080 trainingloss 0.6931471805599453
iteration 1 batch 14090 trainingloss 0.6931471805599453
iteration 1 batch 14100 trainingloss 0.6931471805599453
iteration 1 batch 14110 trainingloss 0.6931471805599453
iteration 1 batch 14120 trainingloss 0.6931471805599453
iteration 1 batch 14130 trainingloss 0.6931471805599453
iteration 1 batch 14140 trainingloss 0.6916632528527511
iteration 1 batch 14150 trainingloss 0.6916632528527511
iteration 1 batch 14160 trainingloss 0.6931471805599453
iteration 1 batch 14170 trainingloss 0.6901793251455568
iteration 1 batch 14180 trainingloss 0.6931471805599453
iteration 1 batch 14190 trainingloss 0.6931471805599453
iteration 1 batch 14200 trainingloss 0.6931471805599453
iteration 1 batch 14210 trainingloss 0.6931471805599453
iteration 1 batch 14220 trainingloss 0.6931471805599453
iteration 1 batch 14230 trainingloss 0.6931471805599453
iteration 1 batch 14240 trainingloss 0.6931471805599453
iteration 1 batch 14250 trainingloss 0.6931471805599453
iteration 1 batch 14260 trainingloss 0.6931471805599453
iteration 1 batch 14270 trainingloss 0.6931471805599453
iteration 1 batch 14280 trainingloss 0.6916632528527511
iteration 1 batch 14290 trainingloss 0.6931471805599453
iteration 1 batch 14300 trainingloss 0.6931471805599453
iteration 1 batch 14310 trainingloss 0.6931471805599453
iteration 1 batch 14320 trainingloss 0.6916632528527511
iteration 1 batch 14330 trainingloss 0.6931471805599453
iteration 1 batch 14340 trainingloss 0.6931471805599453
iteration 1 batch 14350 trainingloss 0.6916632528527511
iteration 1 batch 14360 trainingloss 0.6916632528527511
iteration 1 batch 14370 trainingloss 0.6901793251455568
iteration 1 batch 14380 trainingloss 0.6931471805599453
iteration 1 batch 14390 trainingloss 0.6931471805599453
iteration 1 batch 14400 trainingloss 0.6931471805599453
iteration 1 batch 14410 trainingloss 0.6931471805599453
iteration 1 batch 14420 trainingloss 0.6931471805599453
iteration 1 batch 14430 trainingloss 0.6931471805599453
iteration 1 batch 14440 trainingloss 0.6931471805599453
iteration 1 batch 14450 trainingloss 0.6931471805599453
iteration 1 batch 14460 trainingloss 0.6931471805599453
iteration 1 batch 14470 trainingloss 0.6931471805599453
iteration 1 batch 14480 trainingloss 0.6931471805599453
iteration 1 batch 14490 trainingloss 0.6931471805599453
iteration 1 batch 14500 trainingloss 0.6931471805599453
iteration 1 batch 14510 trainingloss 0.6931471805599453
iteration 1 batch 14520 trainingloss 0.6931471805599453
iteration 1 batch 14530 trainingloss 0.6931471805599453
iteration 1 batch 14540 trainingloss 0.6931471805599453
iteration 1 batch 14550 trainingloss 0.6931471805599453
iteration 1 batch 14560 trainingloss 0.6931471805599453
iteration 1 batch 14570 trainingloss 0.6931471805599453
iteration 1 batch 14580 trainingloss 0.6916632528527511
iteration 1 batch 14590 trainingloss 0.6931471805599453
iteration 1 batch 14600 trainingloss 0.6916632528527511
iteration 1 batch 14610 trainingloss 0.6931471805599453
iteration 1 batch 14620 trainingloss 0.6931471805599453
iteration 1 batch 14630 trainingloss 0.6916632528527511
iteration 1 batch 14640 trainingloss 0.6931471805599453
iteration 1 batch 14650 trainingloss 0.6931471805599453
iteration 1 batch 14660 trainingloss 0.6931471805599453
iteration 1 batch 14670 trainingloss 0.6931471805599453
iteration 1 batch 14680 trainingloss 0.6901793251455568
iteration 1 batch 14690 trainingloss 0.6916632528527511
iteration 1 batch 14700 trainingloss 0.6931471805599453
iteration 1 batch 14710 trainingloss 0.6931471805599453
iteration 1 batch 14720 trainingloss 0.6931471805599453
iteration 1 batch 14730 trainingloss 0.6931471805599453
iteration 1 batch 14740 trainingloss 0.6931471805599453
iteration 1 batch 14750 trainingloss 0.6931471805599453
iteration 1 batch 14760 trainingloss 0.6931471805599453
iteration 1 batch 14770 trainingloss 0.6931471805599453
iteration 1 batch 14780 trainingloss 0.6931471805599453
iteration 1 batch 14790 trainingloss 0.6931471805599453
iteration 1 batch 14800 trainingloss 0.6931471805599453
iteration 1 batch 14810 trainingloss 0.6931471805599453
iteration 1 batch 14820 trainingloss 0.6931471805599453
iteration 1 batch 14830 trainingloss 0.6931471805599453
iteration 1 batch 14840 trainingloss 0.6931471805599453
iteration 1 batch 14850 trainingloss 0.6931471805599453
iteration 1 batch 14860 trainingloss 0.6931471805599453
iteration 1 batch 14870 trainingloss 0.6931471805599453
iteration 1 batch 14880 trainingloss 0.6931471805599453
iteration 1 batch 14890 trainingloss 0.6931471805599453
iteration 1 batch 14900 trainingloss 0.6931471805599453
iteration 1 batch 14910 trainingloss 0.6931471805599453
iteration 1 batch 14920 trainingloss 0.6931471805599453
iteration 1 batch 14930 trainingloss 0.6916632528527511
iteration 1 batch 14940 trainingloss 0.6931471805599453
iteration 1 batch 14950 trainingloss 0.6931471805599453
iteration 1 batch 14960 trainingloss 0.6931471805599453
iteration 1 batch 14970 trainingloss 0.6931471805599453
iteration 1 batch 14980 trainingloss 0.6916632528527511
iteration 1 batch 14990 trainingloss 0.6931471805599453
iteration 1 batch 15000 trainingloss 0.6931471805599453
iteration 1 batch 15010 trainingloss 0.6931471805599453
iteration 1 batch 15020 trainingloss 0.6931471805599453
iteration 1 batch 15030 trainingloss 0.6931471805599453
iteration 1 batch 15040 trainingloss 0.6931471805599453
iteration 1 batch 15050 trainingloss 0.6931471805599453
iteration 1 batch 15060 trainingloss 0.6931471805599453
iteration 1 batch 15070 trainingloss 0.6931471805599453
iteration 1 batch 15080 trainingloss 0.6931471805599453
iteration 1 batch 15090 trainingloss 0.6931471805599453
iteration 1 batch 15100 trainingloss 0.6916632528527511
iteration 1 batch 15110 trainingloss 0.6931471805599453
iteration 1 batch 15120 trainingloss 0.6931471805599453
iteration 1 batch 15130 trainingloss 0.6931471805599453
iteration 1 batch 15140 trainingloss 0.6931471805599453
iteration 1 batch 15150 trainingloss 0.6916632528527511
iteration 1 batch 15160 trainingloss 0.6931471805599453
iteration 1 batch 15170 trainingloss 0.6931471805599453
iteration 1 batch 15180 trainingloss 0.6931471805599453
iteration 1 batch 15190 trainingloss 0.6931471805599453
iteration 1 batch 15200 trainingloss 0.6931471805599453
iteration 1 batch 15210 trainingloss 0.6931471805599453
iteration 1 batch 15220 trainingloss 0.6931471805599453
iteration 1 batch 15230 trainingloss 0.6931471805599453
iteration 1 batch 15240 trainingloss 0.6931471805599453
iteration 1 batch 15250 trainingloss 0.6931471805599453
iteration 1 batch 15260 trainingloss 0.6931471805599453
iteration 1 batch 15270 trainingloss 0.6931471805599453
iteration 1 batch 15280 trainingloss 0.6931471805599453
iteration 1 batch 15290 trainingloss 0.6931471805599453
iteration 1 batch 15300 trainingloss 0.6931471805599453
iteration 1 batch 15310 trainingloss 0.6931471805599453
iteration 1 batch 15320 trainingloss 0.6931471805599453
iteration 1 batch 15330 trainingloss 0.6931471805599453
iteration 1 batch 15340 trainingloss 0.6931471805599453
iteration 1 batch 15350 trainingloss 0.6931471805599453
iteration 1 batch 15360 trainingloss 0.6916632528527511
iteration 1 batch 15370 trainingloss 0.6916632528527511
iteration 1 batch 15380 trainingloss 0.6931471805599453
iteration 1 batch 15390 trainingloss 0.6916632528527511
iteration 1 batch 15400 trainingloss 0.6931471805599453
iteration 1 batch 15410 trainingloss 0.6931471805599453
iteration 1 batch 15420 trainingloss 0.6931471805599453
iteration 1 batch 15430 trainingloss 0.6931471805599453
iteration 1 batch 15440 trainingloss 0.6931471805599453
iteration 1 batch 15450 trainingloss 0.6931471805599453
iteration 1 batch 15460 trainingloss 0.6931471805599453
iteration 1 batch 15470 trainingloss 0.6931471805599453
iteration 1 batch 15480 trainingloss 0.6931471805599453
iteration 1 batch 15490 trainingloss 0.6916632528527511
iteration 1 batch 15500 trainingloss 0.6931471805599453
iteration 1 batch 15510 trainingloss 0.6931471805599453
iteration 1 batch 15520 trainingloss 0.6931471805599453
iteration 1 batch 15530 trainingloss 0.6931471805599453
iteration 1 batch 15540 trainingloss 0.6901793251455568
iteration 1 batch 15550 trainingloss 0.6931471805599453
iteration 1 batch 15560 trainingloss 0.6931471805599453
iteration 1 batch 15570 trainingloss 0.6931471805599453
iteration 1 batch 15580 trainingloss 0.6931471805599453
iteration 1 batch 15590 trainingloss 0.6931471805599453
iteration 1 batch 15600 trainingloss 0.6931471805599453
iteration 1 batch 15610 trainingloss 0.6931471805599453
iteration 1 batch 15620 trainingloss 0.6931471805599453
iteration 1 batch 15630 trainingloss 0.6931471805599453
iteration 1 batch 15640 trainingloss 0.6931471805599453
iteration 1 batch 15650 trainingloss 0.6931471805599453
iteration 1 batch 15660 trainingloss 0.6931471805599453
iteration 1 batch 15670 trainingloss 0.6916632528527511
iteration 1 batch 15680 trainingloss 0.6931471805599453
iteration 1 batch 15690 trainingloss 0.6931471805599453
iteration 1 batch 15700 trainingloss 0.6931471805599453
iteration 1 batch 15710 trainingloss 0.6931471805599453
iteration 1 batch 15720 trainingloss 0.6916632528527511
iteration 1 batch 15730 trainingloss 0.6931471805599453
iteration 1 batch 15740 trainingloss 0.6931471805599453
iteration 1 batch 15750 trainingloss 0.6931471805599453
iteration 1 batch 15760 trainingloss 0.6931471805599453
iteration 1 batch 15770 trainingloss 0.6931471805599453
iteration 1 batch 15780 trainingloss 0.6931471805599453
iteration 1 batch 15790 trainingloss 0.6931471805599453
iteration 1 batch 15800 trainingloss 0.6916632528527511
iteration 1 batch 15810 trainingloss 0.6931471805599453
iteration 1 batch 15820 trainingloss 0.6931471805599453
iteration 1 batch 15830 trainingloss 0.6931471805599453
iteration 1 batch 15840 trainingloss 0.6931471805599453
iteration 1 batch 15850 trainingloss 0.6931471805599453
iteration 1 batch 15860 trainingloss 0.6931471805599453
iteration 1 batch 15870 trainingloss 0.6931471805599453
iteration 1 batch 15880 trainingloss 0.6931471805599453
iteration 1 batch 15890 trainingloss 0.6931471805599453
iteration 1 batch 15900 trainingloss 0.6916632528527511
iteration 1 batch 15910 trainingloss 0.6931471805599453
iteration 1 batch 15920 trainingloss 0.6931471805599453
iteration 1 batch 15930 trainingloss 0.6916632528527511
iteration 1 batch 15940 trainingloss 0.6931471805599453
iteration 1 batch 15950 trainingloss 0.6931471805599453
iteration 1 batch 15960 trainingloss 0.6931471805599453
iteration 1 batch 15970 trainingloss 0.6931471805599453
iteration 1 batch 15980 trainingloss 0.6931471805599453
iteration 1 batch 15990 trainingloss 0.6931471805599453
iteration 1 batch 16000 trainingloss 0.6931471805599453
iteration 1 batch 16010 trainingloss 0.6931471805599453
iteration 1 batch 16020 trainingloss 0.6931471805599453
iteration 1 batch 16030 trainingloss 0.6931471805599453
iteration 1 batch 16040 trainingloss 0.6931471805599453
iteration 1 batch 16050 trainingloss 0.6931471805599453
iteration 1 batch 16060 trainingloss 0.6931471805599453
iteration 1 batch 16070 trainingloss 0.6916632528527511
iteration 1 batch 16080 trainingloss 0.6931471805599453
iteration 1 batch 16090 trainingloss 0.6931471805599453
iteration 1 batch 16100 trainingloss 0.6931471805599453
iteration 1 batch 16110 trainingloss 0.6901793251455568
iteration 1 batch 16120 trainingloss 0.6916632528527511
iteration 1 batch 16130 trainingloss 0.6916632528527511
iteration 1 batch 16140 trainingloss 0.6931471805599453
iteration 1 batch 16150 trainingloss 0.6931471805599453
iteration 1 batch 16160 trainingloss 0.6916632528527511
iteration 1 batch 16170 trainingloss 0.6931471805599453
iteration 1 batch 16180 trainingloss 0.6931471805599453
iteration 1 batch 16190 trainingloss 0.6931471805599453
iteration 1 batch 16200 trainingloss 0.6931471805599453
iteration 1 batch 16210 trainingloss 0.6916632528527511
iteration 1 batch 16220 trainingloss 0.6916632528527511
iteration 1 batch 16230 trainingloss 0.6931471805599453
iteration 1 batch 16240 trainingloss 0.6931471805599453
iteration 1 batch 16250 trainingloss 0.6931471805599453
iteration 1 batch 16260 trainingloss 0.6931471805599453
iteration 1 batch 16270 trainingloss 0.6931471805599453
iteration 1 batch 16280 trainingloss 0.6931471805599453
iteration 1 batch 16290 trainingloss 0.6931471805599453
iteration 1 batch 16300 trainingloss 0.6931471805599453
iteration 1 batch 16310 trainingloss 0.6931471805599453
iteration 1 batch 16320 trainingloss 0.6931471805599453
iteration 1 batch 16330 trainingloss 0.6931471805599453
iteration 1 batch 16340 trainingloss 0.6916632528527511
iteration 1 batch 16350 trainingloss 0.6931471805599453
iteration 1 batch 16360 trainingloss 0.6931471805599453
iteration 1 batch 16370 trainingloss 0.6931471805599453
iteration 1 batch 16380 trainingloss 0.6931471805599453
iteration 1 batch 16390 trainingloss 0.6916632528527511
iteration 1 batch 16400 trainingloss 0.6931471805599453
iteration 1 batch 16410 trainingloss 0.6931471805599453
iteration 1 batch 16420 trainingloss 0.6931471805599453
iteration 1 batch 16430 trainingloss 0.6931471805599453
iteration 1 batch 16440 trainingloss 0.6931471805599453
iteration 1 batch 16450 trainingloss 0.6931471805599453
iteration 1 batch 16460 trainingloss 0.6931471805599453
iteration 1 batch 16470 trainingloss 0.6916632528527511
iteration 1 batch 16480 trainingloss 0.6931471805599453
iteration 1 batch 16490 trainingloss 0.6931471805599453
iteration 1 batch 16500 trainingloss 0.6931471805599453
iteration 1 batch 16510 trainingloss 0.6931471805599453
iteration 1 batch 16520 trainingloss 0.6931471805599453
iteration 1 batch 16530 trainingloss 0.6931471805599453
iteration 1 batch 16540 trainingloss 0.6931471805599453
iteration 1 batch 16550 trainingloss 0.6931471805599453
iteration 1 batch 16560 trainingloss 0.6916632528527511
iteration 1 batch 16570 trainingloss 0.6931471805599453
iteration 1 batch 16580 trainingloss 0.6931471805599453
iteration 1 batch 16590 trainingloss 0.6931471805599453
iteration 1 batch 16600 trainingloss 0.6931471805599453
iteration 1 batch 16610 trainingloss 0.6931471805599453
iteration 1 batch 16620 trainingloss 0.6931471805599453
iteration 1 batch 16630 trainingloss 0.6931471805599453
iteration 1 batch 16640 trainingloss 0.6916632528527511
iteration 1 batch 16650 trainingloss 0.6931471805599453
iteration 1 batch 16660 trainingloss 0.6931471805599453
iteration 1 batch 16670 trainingloss 0.6931471805599453
iteration 1 batch 16680 trainingloss 0.6931471805599453
iteration 1 batch 16690 trainingloss 0.6931471805599453
iteration 1 batch 16700 trainingloss 0.6931471805599453
iteration 1 batch 16710 trainingloss 0.6931471805599453
iteration 1 batch 16720 trainingloss 0.6931471805599453
iteration 1 batch 16730 trainingloss 0.6931471805599453
iteration 1 batch 16740 trainingloss 0.6931471805599453
iteration 1 batch 16750 trainingloss 0.6931471805599453
iteration 1 batch 16760 trainingloss 0.6931471805599453
iteration 1 batch 16770 trainingloss 0.6931471805599453
iteration 1 batch 16780 trainingloss 0.6931471805599453
iteration 1 batch 16790 trainingloss 0.6931471805599453
iteration 1 batch 16800 trainingloss 0.6931471805599453
iteration 1 batch 16810 trainingloss 0.6931471805599453
iteration 1 batch 16820 trainingloss 0.6931471805599453
iteration 1 batch 16830 trainingloss 0.6931471805599453
iteration 1 batch 16840 trainingloss 0.6931471805599453
iteration 1 batch 16850 trainingloss 0.6931471805599453
iteration 1 batch 16860 trainingloss 0.6931471805599453
iteration 1 batch 16870 trainingloss 0.6931471805599453
iteration 1 batch 16880 trainingloss 0.6916632528527511
iteration 1 batch 16890 trainingloss 0.6931471805599453
iteration 1 batch 16900 trainingloss 0.6916632528527511
iteration 1 batch 16910 trainingloss 0.6931471805599453
iteration 1 batch 16920 trainingloss 0.6931471805599453
iteration 1 batch 16930 trainingloss 0.6931471805599453
iteration 1 batch 16940 trainingloss 0.6931471805599453
iteration 1 batch 16950 trainingloss 0.6931471805599453
iteration 1 batch 16960 trainingloss 0.6931471805599453
iteration 1 batch 16970 trainingloss 0.6931471805599453
iteration 1 batch 16980 trainingloss 0.6931471805599453
iteration 1 batch 16990 trainingloss 0.6916632528527511
iteration 1 batch 17000 trainingloss 0.6931471805599453
iteration 1 batch 17010 trainingloss 0.6931471805599453
iteration 1 batch 17020 trainingloss 0.6916632528527511
iteration 1 batch 17030 trainingloss 0.6931471805599453
iteration 1 batch 17040 trainingloss 0.6931471805599453
iteration 1 batch 17050 trainingloss 0.6916632528527511
iteration 1 batch 17060 trainingloss 0.6931471805599453
iteration 1 batch 17070 trainingloss 0.6931471805599453
iteration 1 batch 17080 trainingloss 0.6931471805599453
iteration 1 batch 17090 trainingloss 0.6916632528527511
iteration 1 batch 17100 trainingloss 0.6931471805599453
iteration 1 batch 17110 trainingloss 0.6916632528527511
iteration 1 batch 17120 trainingloss 0.6931471805599453
iteration 1 batch 17130 trainingloss 0.6916632528527511
iteration 1 batch 17140 trainingloss 0.6931471805599453
iteration 1 batch 17150 trainingloss 0.6931471805599453
iteration 1 batch 17160 trainingloss 0.6931471805599453
iteration 1 batch 17170 trainingloss 0.6931471805599453
iteration 1 batch 17180 trainingloss 0.6931471805599453
iteration 1 batch 17190 trainingloss 0.6931471805599453
iteration 1 batch 17200 trainingloss 0.6931471805599453
iteration 1 batch 17210 trainingloss 0.6931471805599453
iteration 1 batch 17220 trainingloss 0.6931471805599453
iteration 1 batch 17230 trainingloss 0.6931471805599453
iteration 1 batch 17240 trainingloss 0.6931471805599453
iteration 1 batch 17250 trainingloss 0.6916632528527511
iteration 1 batch 17260 trainingloss 0.6931471805599453
iteration 1 batch 17270 trainingloss 0.6931471805599453
iteration 1 batch 17280 trainingloss 0.6931471805599453
iteration 1 batch 17290 trainingloss 0.6931471805599453
iteration 1 batch 17300 trainingloss 0.6931471805599453
iteration 1 batch 17310 trainingloss 0.6931471805599453
iteration 1 batch 17320 trainingloss 0.6931471805599453
iteration 1 batch 17330 trainingloss 0.6916632528527511
iteration 1 batch 17340 trainingloss 0.6931471805599453
iteration 1 batch 17350 trainingloss 0.6931471805599453
iteration 1 batch 17360 trainingloss 0.6931471805599453
iteration 1 batch 17370 trainingloss 0.6931471805599453
iteration 1 batch 17380 trainingloss 0.6931471805599453
iteration 1 batch 17390 trainingloss 0.6931471805599453
iteration 1 batch 17400 trainingloss 0.6931471805599453
iteration 1 batch 17410 trainingloss 0.6931471805599453
iteration 1 batch 17420 trainingloss 0.6916632528527511
iteration 1 batch 17430 trainingloss 0.6931471805599453
iteration 1 batch 17440 trainingloss 0.6931471805599453
iteration 1 batch 17450 trainingloss 0.6916632528527511
iteration 1 batch 17460 trainingloss 0.6931471805599453
iteration 1 batch 17470 trainingloss 0.6931471805599453
iteration 1 batch 17480 trainingloss 0.6931471805599453
iteration 1 batch 17490 trainingloss 0.6931471805599453
iteration 1 batch 17500 trainingloss 0.6916632528527511
iteration 1 batch 17510 trainingloss 0.6916632528527511
iteration 1 batch 17520 trainingloss 0.6931471805599453
iteration 1 batch 17530 trainingloss 0.6931471805599453
iteration 1 batch 17540 trainingloss 0.6931471805599453
iteration 1 batch 17550 trainingloss 0.6931471805599453
iteration 1 batch 17560 trainingloss 0.6931471805599453
iteration 1 batch 17570 trainingloss 0.6931471805599453
iteration 1 batch 17580 trainingloss 0.6931471805599453
iteration 1 batch 17590 trainingloss 0.6931471805599453
iteration 1 batch 17600 trainingloss 0.6931471805599453
iteration 1 batch 17610 trainingloss 0.6931471805599453
iteration 1 batch 17620 trainingloss 0.6931471805599453
iteration 1 batch 17630 trainingloss 0.6931471805599453
iteration 1 batch 17640 trainingloss 0.6931471805599453
iteration 1 batch 17650 trainingloss 0.6931471805599453
iteration 1 batch 17660 trainingloss 0.6931471805599453
iteration 1 batch 17670 trainingloss 0.6931471805599453
iteration 1 batch 17680 trainingloss 0.6931471805599453
iteration 1 batch 17690 trainingloss 0.6931471805599453
iteration 1 batch 17700 trainingloss 0.6916632528527511
iteration 1 batch 17710 trainingloss 0.6931471805599453
iteration 1 batch 17720 trainingloss 0.6931471805599453
iteration 1 batch 17730 trainingloss 0.6916632528527511
iteration 1 batch 17740 trainingloss 0.6931471805599453
iteration 1 batch 17750 trainingloss 0.6931471805599453
iteration 1 batch 17760 trainingloss 0.6931471805599453
iteration 1 batch 17770 trainingloss 0.6931471805599453
iteration 1 batch 17780 trainingloss 0.6931471805599453
iteration 1 batch 17790 trainingloss 0.6931471805599453
iteration 1 batch 17800 trainingloss 0.6931471805599453
iteration 1 batch 17810 trainingloss 0.6931471805599453
iteration 1 batch 17820 trainingloss 0.6931471805599453
iteration 1 batch 17830 trainingloss 0.6916632528527511
iteration 1 batch 17840 trainingloss 0.6931471805599453
iteration 1 batch 17850 trainingloss 0.6931471805599453
iteration 1 batch 17860 trainingloss 0.6931471805599453
iteration 1 batch 17870 trainingloss 0.6931471805599453
iteration 1 batch 17880 trainingloss 0.6931471805599453
iteration 1 batch 17890 trainingloss 0.6931471805599453
iteration 1 batch 17900 trainingloss 0.6916632528527511
iteration 1 batch 17910 trainingloss 0.6931471805599453
iteration 1 batch 17920 trainingloss 0.6931471805599453
iteration 1 batch 17930 trainingloss 0.6931471805599453
iteration 1 batch 17940 trainingloss 0.6931471805599453
iteration 1 batch 17950 trainingloss 0.6931471805599453
iteration 1 batch 17960 trainingloss 0.6931471805599453
iteration 1 batch 17970 trainingloss 0.6931471805599453
iteration 1 batch 17980 trainingloss 0.6931471805599453
iteration 1 batch 17990 trainingloss 0.6916632528527511
iteration 1 batch 18000 trainingloss 0.6931471805599453
iteration 1 batch 18010 trainingloss 0.6931471805599453
iteration 1 batch 18020 trainingloss 0.6931471805599453
iteration 1 batch 18030 trainingloss 0.6931471805599453
iteration 1 batch 18040 trainingloss 0.6916632528527511
iteration 1 batch 18050 trainingloss 0.6931471805599453
iteration 1 batch 18060 trainingloss 0.6916632528527511
iteration 1 batch 18070 trainingloss 0.6931471805599453
iteration 1 batch 18080 trainingloss 0.6916632528527511
iteration 1 batch 18090 trainingloss 0.6931471805599453
iteration 1 batch 18100 trainingloss 0.6931471805599453
iteration 1 batch 18110 trainingloss 0.6931471805599453
iteration 1 batch 18120 trainingloss 0.6916632528527511
iteration 1 batch 18130 trainingloss 0.6931471805599453
iteration 1 batch 18140 trainingloss 0.6931471805599453
iteration 1 batch 18150 trainingloss 0.6916632528527511
iteration 1 batch 18160 trainingloss 0.6931471805599453
iteration 1 batch 18170 trainingloss 0.6931471805599453
iteration 1 batch 18180 trainingloss 0.6916632528527511
iteration 1 batch 18190 trainingloss 0.6931471805599453
iteration 1 batch 18200 trainingloss 0.6931471805599453
iteration 1 batch 18210 trainingloss 0.6931471805599453
iteration 1 batch 18220 trainingloss 0.6931471805599453
iteration 1 batch 18230 trainingloss 0.6931471805599453
iteration 1 batch 18240 trainingloss 0.6931471805599453
iteration 1 batch 18250 trainingloss 0.6931471805599453
iteration 1 batch 18260 trainingloss 0.6916632528527511
iteration 1 batch 18270 trainingloss 0.6931471805599453
iteration 1 batch 18280 trainingloss 0.6931471805599453
iteration 1 batch 18290 trainingloss 0.6931471805599453
iteration 1 batch 18300 trainingloss 0.6931471805599453
iteration 1 batch 18310 trainingloss 0.6931471805599453
iteration 1 batch 18320 trainingloss 0.6931471805599453
iteration 1 batch 18330 trainingloss 0.6931471805599453
iteration 1 batch 18340 trainingloss 0.6916632528527511
iteration 1 batch 18350 trainingloss 0.6931471805599453
iteration 1 batch 18360 trainingloss 0.6931471805599453
iteration 1 batch 18370 trainingloss 0.6916632528527511
iteration 1 batch 18380 trainingloss 0.6931471805599453
iteration 1 batch 18390 trainingloss 0.6901793251455568
iteration 1 batch 18400 trainingloss 0.6931471805599453
iteration 1 batch 18410 trainingloss 0.6916632528527511
iteration 1 batch 18420 trainingloss 0.6931471805599453
iteration 1 batch 18430 trainingloss 0.6931471805599453
iteration 1 batch 18440 trainingloss 0.6931471805599453
iteration 1 batch 18450 trainingloss 0.6931471805599453
iteration 1 batch 18460 trainingloss 0.6931471805599453
iteration 1 batch 18470 trainingloss 0.6931471805599453
iteration 1 batch 18480 trainingloss 0.6931471805599453
iteration 1 batch 18490 trainingloss 0.6931471805599453
iteration 1 batch 18500 trainingloss 0.6916632528527511
iteration 1 batch 18510 trainingloss 0.6931471805599453
iteration 1 batch 18520 trainingloss 0.6931471805599453
iteration 1 batch 18530 trainingloss 0.6931471805599453
iteration 1 batch 18540 trainingloss 0.6931471805599453
iteration 1 batch 18550 trainingloss 0.6916632528527511
iteration 1 batch 18560 trainingloss 0.6931471805599453
iteration 1 batch 18570 trainingloss 0.6931471805599453
iteration 1 batch 18580 trainingloss 0.6916632528527511
iteration 1 batch 18590 trainingloss 0.6916632528527511
iteration 1 batch 18600 trainingloss 0.6916632528527511
iteration 1 batch 18610 trainingloss 0.6931471805599453
iteration 2 batch 0 trainingloss 0.6931471805599453
iteration 2 batch 10 trainingloss 0.6931471805599453
iteration 2 batch 20 trainingloss 0.6916632528527511
iteration 2 batch 30 trainingloss 0.6916632528527511
iteration 2 batch 40 trainingloss 0.6931471805599453
iteration 2 batch 50 trainingloss 0.6916632528527511
iteration 2 batch 60 trainingloss 0.6931471805599453
iteration 2 batch 70 trainingloss 0.6931471805599453
iteration 2 batch 80 trainingloss 0.6931471805599453
iteration 2 batch 90 trainingloss 0.6931471805599453
iteration 2 batch 100 trainingloss 0.6916632528527511
iteration 2 batch 110 trainingloss 0.6931471805599453
iteration 2 batch 120 trainingloss 0.6931471805599453
iteration 2 batch 130 trainingloss 0.6931471805599453
iteration 2 batch 140 trainingloss 0.6931471805599453
iteration 2 batch 150 trainingloss 0.6931471805599453
iteration 2 batch 160 trainingloss 0.6931471805599453
iteration 2 batch 170 trainingloss 0.6931471805599453
iteration 2 batch 180 trainingloss 0.6931471805599453
iteration 2 batch 190 trainingloss 0.6931471805599453
iteration 2 batch 200 trainingloss 0.6931471805599453
iteration 2 batch 210 trainingloss 0.6931471805599453
iteration 2 batch 220 trainingloss 0.6931471805599453
iteration 2 batch 230 trainingloss 0.6931471805599453
iteration 2 batch 240 trainingloss 0.6931471805599453
iteration 2 batch 250 trainingloss 0.6931471805599453
iteration 2 batch 260 trainingloss 0.6931471805599453
iteration 2 batch 270 trainingloss 0.6931471805599453
iteration 2 batch 280 trainingloss 0.6931471805599453
iteration 2 batch 290 trainingloss 0.6931471805599453
iteration 2 batch 300 trainingloss 0.6931471805599453
iteration 2 batch 310 trainingloss 0.6931471805599453
iteration 2 batch 320 trainingloss 0.6931471805599453
iteration 2 batch 330 trainingloss 0.6931471805599453
iteration 2 batch 340 trainingloss 0.6931471805599453
iteration 2 batch 350 trainingloss 0.6931471805599453
iteration 2 batch 360 trainingloss 0.6931471805599453
iteration 2 batch 370 trainingloss 0.6931471805599453
iteration 2 batch 380 trainingloss 0.6931471805599453
iteration 2 batch 390 trainingloss 0.6931471805599453
iteration 2 batch 400 trainingloss 0.6916632528527511
iteration 2 batch 410 trainingloss 0.6931471805599453
iteration 2 batch 420 trainingloss 0.6931471805599453
iteration 2 batch 430 trainingloss 0.6916632528527511
iteration 2 batch 440 trainingloss 0.6931471805599453
iteration 2 batch 450 trainingloss 0.6916632528527511
iteration 2 batch 460 trainingloss 0.6931471805599453
iteration 2 batch 470 trainingloss 0.6931471805599453
iteration 2 batch 480 trainingloss 0.6931471805599453
iteration 2 batch 490 trainingloss 0.6916632528527511
iteration 2 batch 500 trainingloss 0.6931471805599453
iteration 2 batch 510 trainingloss 0.6931471805599453
iteration 2 batch 520 trainingloss 0.6931471805599453
iteration 2 batch 530 trainingloss 0.6931471805599453
iteration 2 batch 540 trainingloss 0.6931471805599453
iteration 2 batch 550 trainingloss 0.6931471805599453
iteration 2 batch 560 trainingloss 0.6931471805599453
iteration 2 batch 570 trainingloss 0.6931471805599453
iteration 2 batch 580 trainingloss 0.6931471805599453
iteration 2 batch 590 trainingloss 0.6931471805599453
iteration 2 batch 600 trainingloss 0.6931471805599453
iteration 2 batch 610 trainingloss 0.6931471805599453
iteration 2 batch 620 trainingloss 0.6931471805599453
iteration 2 batch 630 trainingloss 0.6931471805599453
iteration 2 batch 640 trainingloss 0.6931471805599453
iteration 2 batch 650 trainingloss 0.6931471805599453
iteration 2 batch 660 trainingloss 0.6931471805599453
iteration 2 batch 670 trainingloss 0.6916632528527511
iteration 2 batch 680 trainingloss 0.6931471805599453
iteration 2 batch 690 trainingloss 0.6901793251455568
iteration 2 batch 700 trainingloss 0.6931471805599453
iteration 2 batch 710 trainingloss 0.6931471805599453
iteration 2 batch 720 trainingloss 0.6931471805599453
iteration 2 batch 730 trainingloss 0.6931471805599453
iteration 2 batch 740 trainingloss 0.6931471805599453
iteration 2 batch 750 trainingloss 0.6931471805599453
iteration 2 batch 760 trainingloss 0.6931471805599453
iteration 2 batch 770 trainingloss 0.6916632528527511
iteration 2 batch 780 trainingloss 0.6916632528527511
iteration 2 batch 790 trainingloss 0.6931471805599453
iteration 2 batch 800 trainingloss 0.6931471805599453
iteration 2 batch 810 trainingloss 0.6931471805599453
iteration 2 batch 820 trainingloss 0.6916632528527511
iteration 2 batch 830 trainingloss 0.6916632528527511
iteration 2 batch 840 trainingloss 0.6931471805599453
iteration 2 batch 850 trainingloss 0.6931471805599453
iteration 2 batch 860 trainingloss 0.6916632528527511
iteration 2 batch 870 trainingloss 0.6931471805599453
iteration 2 batch 880 trainingloss 0.6916632528527511
iteration 2 batch 890 trainingloss 0.6931471805599453
iteration 2 batch 900 trainingloss 0.6916632528527511
iteration 2 batch 910 trainingloss 0.6931471805599453
iteration 2 batch 920 trainingloss 0.6931471805599453
iteration 2 batch 930 trainingloss 0.6931471805599453
iteration 2 batch 940 trainingloss 0.6931471805599453
iteration 2 batch 950 trainingloss 0.6931471805599453
iteration 2 batch 960 trainingloss 0.6931471805599453
iteration 2 batch 970 trainingloss 0.6931471805599453
iteration 2 batch 980 trainingloss 0.6916632528527511
iteration 2 batch 990 trainingloss 0.6931471805599453
iteration 2 batch 1000 trainingloss 0.6931471805599453
iteration 2 batch 1010 trainingloss 0.6931471805599453
iteration 2 batch 1020 trainingloss 0.6916632528527511
iteration 2 batch 1030 trainingloss 0.6931471805599453
iteration 2 batch 1040 trainingloss 0.6931471805599453
iteration 2 batch 1050 trainingloss 0.6931471805599453
iteration 2 batch 1060 trainingloss 0.6931471805599453
iteration 2 batch 1070 trainingloss 0.6916632528527511
iteration 2 batch 1080 trainingloss 0.6931471805599453
iteration 2 batch 1090 trainingloss 0.6931471805599453
iteration 2 batch 1100 trainingloss 0.6931471805599453
iteration 2 batch 1110 trainingloss 0.6931471805599453
iteration 2 batch 1120 trainingloss 0.6931471805599453
iteration 2 batch 1130 trainingloss 0.6931471805599453
iteration 2 batch 1140 trainingloss 0.6931471805599453
iteration 2 batch 1150 trainingloss 0.6931471805599453
iteration 2 batch 1160 trainingloss 0.6931471805599453
iteration 2 batch 1170 trainingloss 0.6931471805599453
iteration 2 batch 1180 trainingloss 0.6931471805599453
iteration 2 batch 1190 trainingloss 0.6931471805599453
iteration 2 batch 1200 trainingloss 0.6931471805599453
iteration 2 batch 1210 trainingloss 0.6931471805599453
iteration 2 batch 1220 trainingloss 0.6931471805599453
iteration 2 batch 1230 trainingloss 0.6916632528527511
iteration 2 batch 1240 trainingloss 0.6931471805599453
iteration 2 batch 1250 trainingloss 0.6931471805599453
iteration 2 batch 1260 trainingloss 0.6931471805599453
iteration 2 batch 1270 trainingloss 0.6931471805599453
iteration 2 batch 1280 trainingloss 0.6931471805599453
iteration 2 batch 1290 trainingloss 0.6931471805599453
iteration 2 batch 1300 trainingloss 0.6931471805599453
iteration 2 batch 1310 trainingloss 0.6931471805599453
iteration 2 batch 1320 trainingloss 0.6931471805599453
iteration 2 batch 1330 trainingloss 0.6931471805599453
iteration 2 batch 1340 trainingloss 0.6931471805599453
iteration 2 batch 1350 trainingloss 0.6931471805599453
iteration 2 batch 1360 trainingloss 0.6931471805599453
iteration 2 batch 1370 trainingloss 0.6931471805599453
iteration 2 batch 1380 trainingloss 0.6931471805599453
iteration 2 batch 1390 trainingloss 0.6931471805599453
iteration 2 batch 1400 trainingloss 0.6931471805599453
iteration 2 batch 1410 trainingloss 0.6931471805599453
iteration 2 batch 1420 trainingloss 0.6931471805599453
iteration 2 batch 1430 trainingloss 0.6931471805599453
iteration 2 batch 1440 trainingloss 0.6931471805599453
iteration 2 batch 1450 trainingloss 0.6901793251455568
iteration 2 batch 1460 trainingloss 0.6931471805599453
iteration 2 batch 1470 trainingloss 0.6931471805599453
iteration 2 batch 1480 trainingloss 0.6931471805599453
iteration 2 batch 1490 trainingloss 0.6916632528527511
iteration 2 batch 1500 trainingloss 0.6931471805599453
iteration 2 batch 1510 trainingloss 0.6931471805599453
iteration 2 batch 1520 trainingloss 0.6931471805599453
iteration 2 batch 1530 trainingloss 0.6931471805599453
iteration 2 batch 1540 trainingloss 0.6931471805599453
iteration 2 batch 1550 trainingloss 0.6931471805599453
iteration 2 batch 1560 trainingloss 0.6931471805599453
iteration 2 batch 1570 trainingloss 0.6931471805599453
iteration 2 batch 1580 trainingloss 0.6931471805599453
iteration 2 batch 1590 trainingloss 0.6931471805599453
iteration 2 batch 1600 trainingloss 0.6931471805599453
iteration 2 batch 1610 trainingloss 0.6931471805599453
iteration 2 batch 1620 trainingloss 0.6931471805599453
iteration 2 batch 1630 trainingloss 0.6916632528527511
iteration 2 batch 1640 trainingloss 0.6931471805599453
iteration 2 batch 1650 trainingloss 0.6931471805599453
iteration 2 batch 1660 trainingloss 0.6931471805599453
iteration 2 batch 1670 trainingloss 0.6916632528527511
iteration 2 batch 1680 trainingloss 0.6931471805599453
iteration 2 batch 1690 trainingloss 0.6931471805599453
iteration 2 batch 1700 trainingloss 0.6931471805599453
iteration 2 batch 1710 trainingloss 0.6931471805599453
iteration 2 batch 1720 trainingloss 0.6931471805599453
iteration 2 batch 1730 trainingloss 0.6931471805599453
iteration 2 batch 1740 trainingloss 0.6916632528527511
iteration 2 batch 1750 trainingloss 0.6931471805599453
iteration 2 batch 1760 trainingloss 0.6931471805599453
iteration 2 batch 1770 trainingloss 0.6931471805599453
iteration 2 batch 1780 trainingloss 0.6931471805599453
iteration 2 batch 1790 trainingloss 0.6931471805599453
iteration 2 batch 1800 trainingloss 0.6931471805599453
iteration 2 batch 1810 trainingloss 0.6931471805599453
iteration 2 batch 1820 trainingloss 0.6916632528527511
iteration 2 batch 1830 trainingloss 0.6931471805599453
iteration 2 batch 1840 trainingloss 0.6931471805599453
iteration 2 batch 1850 trainingloss 0.6886953974383626
iteration 2 batch 1860 trainingloss 0.6931471805599453
iteration 2 batch 1870 trainingloss 0.6931471805599453
iteration 2 batch 1880 trainingloss 0.6931471805599453
iteration 2 batch 1890 trainingloss 0.6931471805599453
iteration 2 batch 1900 trainingloss 0.6931471805599453
iteration 2 batch 1910 trainingloss 0.6931471805599453
iteration 2 batch 1920 trainingloss 0.6931471805599453
iteration 2 batch 1930 trainingloss 0.6931471805599453
iteration 2 batch 1940 trainingloss 0.6931471805599453
iteration 2 batch 1950 trainingloss 0.6931471805599453
iteration 2 batch 1960 trainingloss 0.6931471805599453
iteration 2 batch 1970 trainingloss 0.6931471805599453
iteration 2 batch 1980 trainingloss 0.6931471805599453
iteration 2 batch 1990 trainingloss 0.6931471805599453
iteration 2 batch 2000 trainingloss 0.6931471805599453
iteration 2 batch 2010 trainingloss 0.6931471805599453
iteration 2 batch 2020 trainingloss 0.6931471805599453
iteration 2 batch 2030 trainingloss 0.6931471805599453
iteration 2 batch 2040 trainingloss 0.6931471805599453
iteration 2 batch 2050 trainingloss 0.6931471805599453
iteration 2 batch 2060 trainingloss 0.6931471805599453
iteration 2 batch 2070 trainingloss 0.6931471805599453
iteration 2 batch 2080 trainingloss 0.6916632528527511
iteration 2 batch 2090 trainingloss 0.6916632528527511
iteration 2 batch 2100 trainingloss 0.6931471805599453
iteration 2 batch 2110 trainingloss 0.6931471805599453
iteration 2 batch 2120 trainingloss 0.6931471805599453
iteration 2 batch 2130 trainingloss 0.6931471805599453
iteration 2 batch 2140 trainingloss 0.6931471805599453
iteration 2 batch 2150 trainingloss 0.6931471805599453
iteration 2 batch 2160 trainingloss 0.6931471805599453
iteration 2 batch 2170 trainingloss 0.6931471805599453
iteration 2 batch 2180 trainingloss 0.6931471805599453
iteration 2 batch 2190 trainingloss 0.6931471805599453
iteration 2 batch 2200 trainingloss 0.6931471805599453
iteration 2 batch 2210 trainingloss 0.6931471805599453
iteration 2 batch 2220 trainingloss 0.6931471805599453
iteration 2 batch 2230 trainingloss 0.6931471805599453
iteration 2 batch 2240 trainingloss 0.6931471805599453
iteration 2 batch 2250 trainingloss 0.6931471805599453
iteration 2 batch 2260 trainingloss 0.6916632528527511
iteration 2 batch 2270 trainingloss 0.6931471805599453
iteration 2 batch 2280 trainingloss 0.6931471805599453
iteration 2 batch 2290 trainingloss 0.6931471805599453
iteration 2 batch 2300 trainingloss 0.6931471805599453
iteration 2 batch 2310 trainingloss 0.6931471805599453
iteration 2 batch 2320 trainingloss 0.6931471805599453
iteration 2 batch 2330 trainingloss 0.6931471805599453
iteration 2 batch 2340 trainingloss 0.6931471805599453
iteration 2 batch 2350 trainingloss 0.6931471805599453
iteration 2 batch 2360 trainingloss 0.6931471805599453
iteration 2 batch 2370 trainingloss 0.6931471805599453
iteration 2 batch 2380 trainingloss 0.6931471805599453
iteration 2 batch 2390 trainingloss 0.6931471805599453
iteration 2 batch 2400 trainingloss 0.6931471805599453
iteration 2 batch 2410 trainingloss 0.6931471805599453
iteration 2 batch 2420 trainingloss 0.6916632528527511
iteration 2 batch 2430 trainingloss 0.6931471805599453
iteration 2 batch 2440 trainingloss 0.6931471805599453
iteration 2 batch 2450 trainingloss 0.6931471805599453
iteration 2 batch 2460 trainingloss 0.6931471805599453
iteration 2 batch 2470 trainingloss 0.6931471805599453
iteration 2 batch 2480 trainingloss 0.6916632528527511
iteration 2 batch 2490 trainingloss 0.6931471805599453
iteration 2 batch 2500 trainingloss 0.6931471805599453
iteration 2 batch 2510 trainingloss 0.6931471805599453
iteration 2 batch 2520 trainingloss 0.6931471805599453
iteration 2 batch 2530 trainingloss 0.6931471805599453
iteration 2 batch 2540 trainingloss 0.6931471805599453
iteration 2 batch 2550 trainingloss 0.6931471805599453
iteration 2 batch 2560 trainingloss 0.6931471805599453
iteration 2 batch 2570 trainingloss 0.6931471805599453
iteration 2 batch 2580 trainingloss 0.6931471805599453
iteration 2 batch 2590 trainingloss 0.6931471805599453
iteration 2 batch 2600 trainingloss 0.6931471805599453
iteration 2 batch 2610 trainingloss 0.6931471805599453
iteration 2 batch 2620 trainingloss 0.6931471805599453
iteration 2 batch 2630 trainingloss 0.6931471805599453
iteration 2 batch 2640 trainingloss 0.6931471805599453
iteration 2 batch 2650 trainingloss 0.6931471805599453
iteration 2 batch 2660 trainingloss 0.6931471805599453
iteration 2 batch 2670 trainingloss 0.6931471805599453
iteration 2 batch 2680 trainingloss 0.6931471805599453
iteration 2 batch 2690 trainingloss 0.6931471805599453
iteration 2 batch 2700 trainingloss 0.6931471805599453
iteration 2 batch 2710 trainingloss 0.6931471805599453
iteration 2 batch 2720 trainingloss 0.6931471805599453
iteration 2 batch 2730 trainingloss 0.6916632528527511
iteration 2 batch 2740 trainingloss 0.6931471805599453
iteration 2 batch 2750 trainingloss 0.6931471805599453
iteration 2 batch 2760 trainingloss 0.6931471805599453
iteration 2 batch 2770 trainingloss 0.6931471805599453
iteration 2 batch 2780 trainingloss 0.6931471805599453
iteration 2 batch 2790 trainingloss 0.6916632528527511
iteration 2 batch 2800 trainingloss 0.6931471805599453
iteration 2 batch 2810 trainingloss 0.6931471805599453
iteration 2 batch 2820 trainingloss 0.6931471805599453
iteration 2 batch 2830 trainingloss 0.6931471805599453
iteration 2 batch 2840 trainingloss 0.6931471805599453
iteration 2 batch 2850 trainingloss 0.6931471805599453
iteration 2 batch 2860 trainingloss 0.6931471805599453
iteration 2 batch 2870 trainingloss 0.6931471805599453
iteration 2 batch 2880 trainingloss 0.6931471805599453
iteration 2 batch 2890 trainingloss 0.6916632528527511
iteration 2 batch 2900 trainingloss 0.6931471805599453
iteration 2 batch 2910 trainingloss 0.6931471805599453
iteration 2 batch 2920 trainingloss 0.6916632528527511
iteration 2 batch 2930 trainingloss 0.6931471805599453
iteration 2 batch 2940 trainingloss 0.6931471805599453
iteration 2 batch 2950 trainingloss 0.6931471805599453
iteration 2 batch 2960 trainingloss 0.6931471805599453
iteration 2 batch 2970 trainingloss 0.6931471805599453
iteration 2 batch 2980 trainingloss 0.6931471805599453
iteration 2 batch 2990 trainingloss 0.6931471805599453
iteration 2 batch 3000 trainingloss 0.6931471805599453
iteration 2 batch 3010 trainingloss 0.6931471805599453
iteration 2 batch 3020 trainingloss 0.6931471805599453
iteration 2 batch 3030 trainingloss 0.6931471805599453
iteration 2 batch 3040 trainingloss 0.6931471805599453
iteration 2 batch 3050 trainingloss 0.6931471805599453
iteration 2 batch 3060 trainingloss 0.6931471805599453
iteration 2 batch 3070 trainingloss 0.6931471805599453
iteration 2 batch 3080 trainingloss 0.6931471805599453
iteration 2 batch 3090 trainingloss 0.6931471805599453
iteration 2 batch 3100 trainingloss 0.6931471805599453
iteration 2 batch 3110 trainingloss 0.6931471805599453
iteration 2 batch 3120 trainingloss 0.6931471805599453
iteration 2 batch 3130 trainingloss 0.6931471805599453
iteration 2 batch 3140 trainingloss 0.6931471805599453
iteration 2 batch 3150 trainingloss 0.6931471805599453
iteration 2 batch 3160 trainingloss 0.6931471805599453
iteration 2 batch 3170 trainingloss 0.6931471805599453
iteration 2 batch 3180 trainingloss 0.6931471805599453
iteration 2 batch 3190 trainingloss 0.6931471805599453
iteration 2 batch 3200 trainingloss 0.6931471805599453
iteration 2 batch 3210 trainingloss 0.6931471805599453
iteration 2 batch 3220 trainingloss 0.6931471805599453
iteration 2 batch 3230 trainingloss 0.6931471805599453
iteration 2 batch 3240 trainingloss 0.6916632528527511
iteration 2 batch 3250 trainingloss 0.6931471805599453
iteration 2 batch 3260 trainingloss 0.6931471805599453
iteration 2 batch 3270 trainingloss 0.6931471805599453
iteration 2 batch 3280 trainingloss 0.6901793251455568
iteration 2 batch 3290 trainingloss 0.6931471805599453
iteration 2 batch 3300 trainingloss 0.6931471805599453
iteration 2 batch 3310 trainingloss 0.6931471805599453
iteration 2 batch 3320 trainingloss 0.6931471805599453
iteration 2 batch 3330 trainingloss 0.6931471805599453
iteration 2 batch 3340 trainingloss 0.6931471805599453
iteration 2 batch 3350 trainingloss 0.6931471805599453
iteration 2 batch 3360 trainingloss 0.6916632528527511
iteration 2 batch 3370 trainingloss 0.6931471805599453
iteration 2 batch 3380 trainingloss 0.6931471805599453
iteration 2 batch 3390 trainingloss 0.6931471805599453
iteration 2 batch 3400 trainingloss 0.6931471805599453
iteration 2 batch 3410 trainingloss 0.6931471805599453
iteration 2 batch 3420 trainingloss 0.6931471805599453
iteration 2 batch 3430 trainingloss 0.6931471805599453
iteration 2 batch 3440 trainingloss 0.6931471805599453
iteration 2 batch 3450 trainingloss 0.6931471805599453
iteration 2 batch 3460 trainingloss 0.6931471805599453
iteration 2 batch 3470 trainingloss 0.6931471805599453
iteration 2 batch 3480 trainingloss 0.6931471805599453
iteration 2 batch 3490 trainingloss 0.6931471805599453
iteration 2 batch 3500 trainingloss 0.6931471805599453
iteration 2 batch 3510 trainingloss 0.6931471805599453
iteration 2 batch 3520 trainingloss 0.6931471805599453
iteration 2 batch 3530 trainingloss 0.6931471805599453
iteration 2 batch 3540 trainingloss 0.6931471805599453
iteration 2 batch 3550 trainingloss 0.6931471805599453
iteration 2 batch 3560 trainingloss 0.6931471805599453
iteration 2 batch 3570 trainingloss 0.6931471805599453
iteration 2 batch 3580 trainingloss 0.6931471805599453
iteration 2 batch 3590 trainingloss 0.6916632528527511
iteration 2 batch 3600 trainingloss 0.6931471805599453
iteration 2 batch 3610 trainingloss 0.6931471805599453
iteration 2 batch 3620 trainingloss 0.6931471805599453
iteration 2 batch 3630 trainingloss 0.6916632528527511
iteration 2 batch 3640 trainingloss 0.6931471805599453
iteration 2 batch 3650 trainingloss 0.6931471805599453
iteration 2 batch 3660 trainingloss 0.6931471805599453
iteration 2 batch 3670 trainingloss 0.6931471805599453
iteration 2 batch 3680 trainingloss 0.6931471805599453
iteration 2 batch 3690 trainingloss 0.6931471805599453
iteration 2 batch 3700 trainingloss 0.6931471805599453
iteration 2 batch 3710 trainingloss 0.6916632528527511
iteration 2 batch 3720 trainingloss 0.6931471805599453
iteration 2 batch 3730 trainingloss 0.6931471805599453
iteration 2 batch 3740 trainingloss 0.6931471805599453
iteration 2 batch 3750 trainingloss 0.6931471805599453
iteration 2 batch 3760 trainingloss 0.6916632528527511
iteration 2 batch 3770 trainingloss 0.6931471805599453
iteration 2 batch 3780 trainingloss 0.6931471805599453
iteration 2 batch 3790 trainingloss 0.6916632528527511
iteration 2 batch 3800 trainingloss 0.6931471805599453
iteration 2 batch 3810 trainingloss 0.6931471805599453
iteration 2 batch 3820 trainingloss 0.6931471805599453
iteration 2 batch 3830 trainingloss 0.6931471805599453
iteration 2 batch 3840 trainingloss 0.6931471805599453
iteration 2 batch 3850 trainingloss 0.6931471805599453
iteration 2 batch 3860 trainingloss 0.6931471805599453
iteration 2 batch 3870 trainingloss 0.6931471805599453
iteration 2 batch 3880 trainingloss 0.6931471805599453
iteration 2 batch 3890 trainingloss 0.6931471805599453
iteration 2 batch 3900 trainingloss 0.6931471805599453
iteration 2 batch 3910 trainingloss 0.6931471805599453
iteration 2 batch 3920 trainingloss 0.6931471805599453
iteration 2 batch 3930 trainingloss 0.6931471805599453
iteration 2 batch 3940 trainingloss 0.6931471805599453
iteration 2 batch 3950 trainingloss 0.6931471805599453
iteration 2 batch 3960 trainingloss 0.6931471805599453
iteration 2 batch 3970 trainingloss 0.6931471805599453
iteration 2 batch 3980 trainingloss 0.6931471805599453
iteration 2 batch 3990 trainingloss 0.6931471805599453
iteration 2 batch 4000 trainingloss 0.6931471805599453
iteration 2 batch 4010 trainingloss 0.6931471805599453
iteration 2 batch 4020 trainingloss 0.6931471805599453
iteration 2 batch 4030 trainingloss 0.6931471805599453
iteration 2 batch 4040 trainingloss 0.6931471805599453
iteration 2 batch 4050 trainingloss 0.6931471805599453
iteration 2 batch 4060 trainingloss 0.6931471805599453
iteration 2 batch 4070 trainingloss 0.6931471805599453
iteration 2 batch 4080 trainingloss 0.6931471805599453
iteration 2 batch 4090 trainingloss 0.6931471805599453
iteration 2 batch 4100 trainingloss 0.6931471805599453
iteration 2 batch 4110 trainingloss 0.6931471805599453
iteration 2 batch 4120 trainingloss 0.6931471805599453
iteration 2 batch 4130 trainingloss 0.6931471805599453
iteration 2 batch 4140 trainingloss 0.6931471805599453
iteration 2 batch 4150 trainingloss 0.6916632528527511
iteration 2 batch 4160 trainingloss 0.6916632528527511
iteration 2 batch 4170 trainingloss 0.6931471805599453
iteration 2 batch 4180 trainingloss 0.6931471805599453
iteration 2 batch 4190 trainingloss 0.6931471805599453
iteration 2 batch 4200 trainingloss 0.6931471805599453
iteration 2 batch 4210 trainingloss 0.6931471805599453
iteration 2 batch 4220 trainingloss 0.6931471805599453
iteration 2 batch 4230 trainingloss 0.6931471805599453
iteration 2 batch 4240 trainingloss 0.6931471805599453
iteration 2 batch 4250 trainingloss 0.6931471805599453
iteration 2 batch 4260 trainingloss 0.6931471805599453
iteration 2 batch 4270 trainingloss 0.6931471805599453
iteration 2 batch 4280 trainingloss 0.6931471805599453
iteration 2 batch 4290 trainingloss 0.6916632528527511
iteration 2 batch 4300 trainingloss 0.6931471805599453
iteration 2 batch 4310 trainingloss 0.6931471805599453
iteration 2 batch 4320 trainingloss 0.6931471805599453
iteration 2 batch 4330 trainingloss 0.6931471805599453
iteration 2 batch 4340 trainingloss 0.6931471805599453
iteration 2 batch 4350 trainingloss 0.6931471805599453
iteration 2 batch 4360 trainingloss 0.6931471805599453
iteration 2 batch 4370 trainingloss 0.6931471805599453
iteration 2 batch 4380 trainingloss 0.6931471805599453
iteration 2 batch 4390 trainingloss 0.6931471805599453
iteration 2 batch 4400 trainingloss 0.6931471805599453
iteration 2 batch 4410 trainingloss 0.6931471805599453
iteration 2 batch 4420 trainingloss 0.6931471805599453
iteration 2 batch 4430 trainingloss 0.6931471805599453
iteration 2 batch 4440 trainingloss 0.6931471805599453
iteration 2 batch 4450 trainingloss 0.6916632528527511
iteration 2 batch 4460 trainingloss 0.6931471805599453
iteration 2 batch 4470 trainingloss 0.6931471805599453
iteration 2 batch 4480 trainingloss 0.6916632528527511
iteration 2 batch 4490 trainingloss 0.6931471805599453
iteration 2 batch 4500 trainingloss 0.6931471805599453
iteration 2 batch 4510 trainingloss 0.6931471805599453
iteration 2 batch 4520 trainingloss 0.6931471805599453
iteration 2 batch 4530 trainingloss 0.6931471805599453
iteration 2 batch 4540 trainingloss 0.6931471805599453
iteration 2 batch 4550 trainingloss 0.6916632528527511
iteration 2 batch 4560 trainingloss 0.6931471805599453
iteration 2 batch 4570 trainingloss 0.6931471805599453
iteration 2 batch 4580 trainingloss 0.6931471805599453
iteration 2 batch 4590 trainingloss 0.6931471805599453
iteration 2 batch 4600 trainingloss 0.6931471805599453
iteration 2 batch 4610 trainingloss 0.6916632528527511
iteration 2 batch 4620 trainingloss 0.6931471805599453
iteration 2 batch 4630 trainingloss 0.6931471805599453
iteration 2 batch 4640 trainingloss 0.6931471805599453
iteration 2 batch 4650 trainingloss 0.6931471805599453
iteration 2 batch 4660 trainingloss 0.6916632528527511
iteration 2 batch 4670 trainingloss 0.6931471805599453
iteration 2 batch 4680 trainingloss 0.6931471805599453
iteration 2 batch 4690 trainingloss 0.6931471805599453
iteration 2 batch 4700 trainingloss 0.6931471805599453
iteration 2 batch 4710 trainingloss 0.6931471805599453
iteration 2 batch 4720 trainingloss 0.6931471805599453
iteration 2 batch 4730 trainingloss 0.6931471805599453
iteration 2 batch 4740 trainingloss 0.6931471805599453
iteration 2 batch 4750 trainingloss 0.6931471805599453
iteration 2 batch 4760 trainingloss 0.6931471805599453
iteration 2 batch 4770 trainingloss 0.6931471805599453
iteration 2 batch 4780 trainingloss 0.6931471805599453
iteration 2 batch 4790 trainingloss 0.6916632528527511
iteration 2 batch 4800 trainingloss 0.6916632528527511
iteration 2 batch 4810 trainingloss 0.6931471805599453
iteration 2 batch 4820 trainingloss 0.6916632528527511
iteration 2 batch 4830 trainingloss 0.6931471805599453
iteration 2 batch 4840 trainingloss 0.6931471805599453
iteration 2 batch 4850 trainingloss 0.6931471805599453
iteration 2 batch 4860 trainingloss 0.6931471805599453
iteration 2 batch 4870 trainingloss 0.6931471805599453
iteration 2 batch 4880 trainingloss 0.6931471805599453
iteration 2 batch 4890 trainingloss 0.6916632528527511
iteration 2 batch 4900 trainingloss 0.6916632528527511
iteration 2 batch 4910 trainingloss 0.6931471805599453
iteration 2 batch 4920 trainingloss 0.6931471805599453
iteration 2 batch 4930 trainingloss 0.6901793251455568
iteration 2 batch 4940 trainingloss 0.6931471805599453
iteration 2 batch 4950 trainingloss 0.6931471805599453
iteration 2 batch 4960 trainingloss 0.6916632528527511
iteration 2 batch 4970 trainingloss 0.6931471805599453
iteration 2 batch 4980 trainingloss 0.6931471805599453
iteration 2 batch 4990 trainingloss 0.6916632528527511
iteration 2 batch 5000 trainingloss 0.6916632528527511
iteration 2 batch 5010 trainingloss 0.6931471805599453
iteration 2 batch 5020 trainingloss 0.6931471805599453
iteration 2 batch 5030 trainingloss 0.6931471805599453
iteration 2 batch 5040 trainingloss 0.6931471805599453
iteration 2 batch 5050 trainingloss 0.6916632528527511
iteration 2 batch 5060 trainingloss 0.6931471805599453
iteration 2 batch 5070 trainingloss 0.6931471805599453
iteration 2 batch 5080 trainingloss 0.6916632528527511
iteration 2 batch 5090 trainingloss 0.6916632528527511
iteration 2 batch 5100 trainingloss 0.6931471805599453
iteration 2 batch 5110 trainingloss 0.6931471805599453
iteration 2 batch 5120 trainingloss 0.6931471805599453
iteration 2 batch 5130 trainingloss 0.6931471805599453
iteration 2 batch 5140 trainingloss 0.6931471805599453
iteration 2 batch 5150 trainingloss 0.6931471805599453
iteration 2 batch 5160 trainingloss 0.6931471805599453
iteration 2 batch 5170 trainingloss 0.6931471805599453
iteration 2 batch 5180 trainingloss 0.6931471805599453
iteration 2 batch 5190 trainingloss 0.6931471805599453
iteration 2 batch 5200 trainingloss 0.6931471805599453
iteration 2 batch 5210 trainingloss 0.6916632528527511
iteration 2 batch 5220 trainingloss 0.6931471805599453
iteration 2 batch 5230 trainingloss 0.6916632528527511
iteration 2 batch 5240 trainingloss 0.6931471805599453
iteration 2 batch 5250 trainingloss 0.6916632528527511
iteration 2 batch 5260 trainingloss 0.6916632528527511
iteration 2 batch 5270 trainingloss 0.6931471805599453
iteration 2 batch 5280 trainingloss 0.6931471805599453
iteration 2 batch 5290 trainingloss 0.6931471805599453
iteration 2 batch 5300 trainingloss 0.6931471805599453
iteration 2 batch 5310 trainingloss 0.6931471805599453
iteration 2 batch 5320 trainingloss 0.6931471805599453
iteration 2 batch 5330 trainingloss 0.6931471805599453
iteration 2 batch 5340 trainingloss 0.6916632528527511
iteration 2 batch 5350 trainingloss 0.6931471805599453
iteration 2 batch 5360 trainingloss 0.6916632528527511
iteration 2 batch 5370 trainingloss 0.6931471805599453
iteration 2 batch 5380 trainingloss 0.6931471805599453
iteration 2 batch 5390 trainingloss 0.6931471805599453
iteration 2 batch 5400 trainingloss 0.6931471805599453
iteration 2 batch 5410 trainingloss 0.6931471805599453
iteration 2 batch 5420 trainingloss 0.6931471805599453
iteration 2 batch 5430 trainingloss 0.6931471805599453
iteration 2 batch 5440 trainingloss 0.6931471805599453
iteration 2 batch 5450 trainingloss 0.6931471805599453
iteration 2 batch 5460 trainingloss 0.6931471805599453
iteration 2 batch 5470 trainingloss 0.6931471805599453
iteration 2 batch 5480 trainingloss 0.6931471805599453
iteration 2 batch 5490 trainingloss 0.6916632528527511
iteration 2 batch 5500 trainingloss 0.6931471805599453
iteration 2 batch 5510 trainingloss 0.6931471805599453
iteration 2 batch 5520 trainingloss 0.6931471805599453
iteration 2 batch 5530 trainingloss 0.6931471805599453
iteration 2 batch 5540 trainingloss 0.6931471805599453
iteration 2 batch 5550 trainingloss 0.6931471805599453
iteration 2 batch 5560 trainingloss 0.6916632528527511
iteration 2 batch 5570 trainingloss 0.6931471805599453
iteration 2 batch 5580 trainingloss 0.6916632528527511
iteration 2 batch 5590 trainingloss 0.6931471805599453
iteration 2 batch 5600 trainingloss 0.6931471805599453
iteration 2 batch 5610 trainingloss 0.6931471805599453
iteration 2 batch 5620 trainingloss 0.6931471805599453
iteration 2 batch 5630 trainingloss 0.6931471805599453
iteration 2 batch 5640 trainingloss 0.6931471805599453
iteration 2 batch 5650 trainingloss 0.6916632528527511
iteration 2 batch 5660 trainingloss 0.6931471805599453
iteration 2 batch 5670 trainingloss 0.6931471805599453
iteration 2 batch 5680 trainingloss 0.6931471805599453
iteration 2 batch 5690 trainingloss 0.6931471805599453
iteration 2 batch 5700 trainingloss 0.6931471805599453
iteration 2 batch 5710 trainingloss 0.6931471805599453
iteration 2 batch 5720 trainingloss 0.6931471805599453
iteration 2 batch 5730 trainingloss 0.6931471805599453
iteration 2 batch 5740 trainingloss 0.6931471805599453
iteration 2 batch 5750 trainingloss 0.6931471805599453
iteration 2 batch 5760 trainingloss 0.6931471805599453
iteration 2 batch 5770 trainingloss 0.6931471805599453
iteration 2 batch 5780 trainingloss 0.6916632528527511
iteration 2 batch 5790 trainingloss 0.6931471805599453
iteration 2 batch 5800 trainingloss 0.6931471805599453
iteration 2 batch 5810 trainingloss 0.6931471805599453
iteration 2 batch 5820 trainingloss 0.6931471805599453
iteration 2 batch 5830 trainingloss 0.6931471805599453
iteration 2 batch 5840 trainingloss 0.6931471805599453
iteration 2 batch 5850 trainingloss 0.6931471805599453
iteration 2 batch 5860 trainingloss 0.6931471805599453
iteration 2 batch 5870 trainingloss 0.6931471805599453
iteration 2 batch 5880 trainingloss 0.6931471805599453
iteration 2 batch 5890 trainingloss 0.6931471805599453
iteration 2 batch 5900 trainingloss 0.6931471805599453
iteration 2 batch 5910 trainingloss 0.6931471805599453
iteration 2 batch 5920 trainingloss 0.6931471805599453
iteration 2 batch 5930 trainingloss 0.6931471805599453
iteration 2 batch 5940 trainingloss 0.6931471805599453
iteration 2 batch 5950 trainingloss 0.6931471805599453
iteration 2 batch 5960 trainingloss 0.6916632528527511
iteration 2 batch 5970 trainingloss 0.6931471805599453
iteration 2 batch 5980 trainingloss 0.6931471805599453
iteration 2 batch 5990 trainingloss 0.6931471805599453
iteration 2 batch 6000 trainingloss 0.6931471805599453
iteration 2 batch 6010 trainingloss 0.6931471805599453
iteration 2 batch 6020 trainingloss 0.6931471805599453
iteration 2 batch 6030 trainingloss 0.6931471805599453
iteration 2 batch 6040 trainingloss 0.6931471805599453
iteration 2 batch 6050 trainingloss 0.6931471805599453
iteration 2 batch 6060 trainingloss 0.6931471805599453
iteration 2 batch 6070 trainingloss 0.6931471805599453
iteration 2 batch 6080 trainingloss 0.6931471805599453
iteration 2 batch 6090 trainingloss 0.6916632528527511
iteration 2 batch 6100 trainingloss 0.6916632528527511
iteration 2 batch 6110 trainingloss 0.6931471805599453
iteration 2 batch 6120 trainingloss 0.6931471805599453
iteration 2 batch 6130 trainingloss 0.6931471805599453
iteration 2 batch 6140 trainingloss 0.6931471805599453
iteration 2 batch 6150 trainingloss 0.6931471805599453
iteration 2 batch 6160 trainingloss 0.6931471805599453
iteration 2 batch 6170 trainingloss 0.6916632528527511
iteration 2 batch 6180 trainingloss 0.6916632528527511
iteration 2 batch 6190 trainingloss 0.6931471805599453
iteration 2 batch 6200 trainingloss 0.6931471805599453
iteration 2 batch 6210 trainingloss 0.6931471805599453
iteration 2 batch 6220 trainingloss 0.6931471805599453
iteration 2 batch 6230 trainingloss 0.6916632528527511
iteration 2 batch 6240 trainingloss 0.6886953974383626
iteration 2 batch 6250 trainingloss 0.6931471805599453
iteration 2 batch 6260 trainingloss 0.6931471805599453
iteration 2 batch 6270 trainingloss 0.6931471805599453
iteration 2 batch 6280 trainingloss 0.6931471805599453
iteration 2 batch 6290 trainingloss 0.6901793251455568
iteration 2 batch 6300 trainingloss 0.6931471805599453
iteration 2 batch 6310 trainingloss 0.6931471805599453
iteration 2 batch 6320 trainingloss 0.6931471805599453
iteration 2 batch 6330 trainingloss 0.6931471805599453
iteration 2 batch 6340 trainingloss 0.6931471805599453
iteration 2 batch 6350 trainingloss 0.6931471805599453
iteration 2 batch 6360 trainingloss 0.6931471805599453
iteration 2 batch 6370 trainingloss 0.6916632528527511
iteration 2 batch 6380 trainingloss 0.6931471805599453
iteration 2 batch 6390 trainingloss 0.6931471805599453
iteration 2 batch 6400 trainingloss 0.6931471805599453
iteration 2 batch 6410 trainingloss 0.6916632528527511
iteration 2 batch 6420 trainingloss 0.6931471805599453
iteration 2 batch 6430 trainingloss 0.6931471805599453
iteration 2 batch 6440 trainingloss 0.6931471805599453
iteration 2 batch 6450 trainingloss 0.6931471805599453
iteration 2 batch 6460 trainingloss 0.6931471805599453
iteration 2 batch 6470 trainingloss 0.6931471805599453
iteration 2 batch 6480 trainingloss 0.6931471805599453
iteration 2 batch 6490 trainingloss 0.6931471805599453
iteration 2 batch 6500 trainingloss 0.6916632528527511
iteration 2 batch 6510 trainingloss 0.6931471805599453
iteration 2 batch 6520 trainingloss 0.6931471805599453
iteration 2 batch 6530 trainingloss 0.6931471805599453
iteration 2 batch 6540 trainingloss 0.6931471805599453
iteration 2 batch 6550 trainingloss 0.6931471805599453
iteration 2 batch 6560 trainingloss 0.6931471805599453
iteration 2 batch 6570 trainingloss 0.6931471805599453
iteration 2 batch 6580 trainingloss 0.6931471805599453
iteration 2 batch 6590 trainingloss 0.6931471805599453
iteration 2 batch 6600 trainingloss 0.6931471805599453
iteration 2 batch 6610 trainingloss 0.6931471805599453
iteration 2 batch 6620 trainingloss 0.6931471805599453
iteration 2 batch 6630 trainingloss 0.6931471805599453
iteration 2 batch 6640 trainingloss 0.6931471805599453
iteration 2 batch 6650 trainingloss 0.6931471805599453
iteration 2 batch 6660 trainingloss 0.6931471805599453
iteration 2 batch 6670 trainingloss 0.6931471805599453
iteration 2 batch 6680 trainingloss 0.6931471805599453
iteration 2 batch 6690 trainingloss 0.6931471805599453
iteration 2 batch 6700 trainingloss 0.6931471805599453
iteration 2 batch 6710 trainingloss 0.6931471805599453
iteration 2 batch 6720 trainingloss 0.6916632528527511
iteration 2 batch 6730 trainingloss 0.6931471805599453
iteration 2 batch 6740 trainingloss 0.6931471805599453
iteration 2 batch 6750 trainingloss 0.6931471805599453
iteration 2 batch 6760 trainingloss 0.6931471805599453
iteration 2 batch 6770 trainingloss 0.6916632528527511
iteration 2 batch 6780 trainingloss 0.6931471805599453
iteration 2 batch 6790 trainingloss 0.6931471805599453
iteration 2 batch 6800 trainingloss 0.6931471805599453
iteration 2 batch 6810 trainingloss 0.6916632528527511
iteration 2 batch 6820 trainingloss 0.6931471805599453
iteration 2 batch 6830 trainingloss 0.6931471805599453
iteration 2 batch 6840 trainingloss 0.6931471805599453
iteration 2 batch 6850 trainingloss 0.6931471805599453
iteration 2 batch 6860 trainingloss 0.6931471805599453
iteration 2 batch 6870 trainingloss 0.6931471805599453
iteration 2 batch 6880 trainingloss 0.6916632528527511
iteration 2 batch 6890 trainingloss 0.6931471805599453
iteration 2 batch 6900 trainingloss 0.6931471805599453
iteration 2 batch 6910 trainingloss 0.6931471805599453
iteration 2 batch 6920 trainingloss 0.6931471805599453
iteration 2 batch 6930 trainingloss 0.6931471805599453
iteration 2 batch 6940 trainingloss 0.6931471805599453
iteration 2 batch 6950 trainingloss 0.6931471805599453
iteration 2 batch 6960 trainingloss 0.6931471805599453
iteration 2 batch 6970 trainingloss 0.6931471805599453
iteration 2 batch 6980 trainingloss 0.6931471805599453
iteration 2 batch 6990 trainingloss 0.6931471805599453
iteration 2 batch 7000 trainingloss 0.6931471805599453
iteration 2 batch 7010 trainingloss 0.6931471805599453
iteration 2 batch 7020 trainingloss 0.6931471805599453
iteration 2 batch 7030 trainingloss 0.6916632528527511
iteration 2 batch 7040 trainingloss 0.6931471805599453
iteration 2 batch 7050 trainingloss 0.6931471805599453
iteration 2 batch 7060 trainingloss 0.6931471805599453
iteration 2 batch 7070 trainingloss 0.6931471805599453
iteration 2 batch 7080 trainingloss 0.6931471805599453
iteration 2 batch 7090 trainingloss 0.6931471805599453
iteration 2 batch 7100 trainingloss 0.6931471805599453
iteration 2 batch 7110 trainingloss 0.6931471805599453
iteration 2 batch 7120 trainingloss 0.6931471805599453
iteration 2 batch 7130 trainingloss 0.6931471805599453
iteration 2 batch 7140 trainingloss 0.6931471805599453
iteration 2 batch 7150 trainingloss 0.6931471805599453
iteration 2 batch 7160 trainingloss 0.6931471805599453
iteration 2 batch 7170 trainingloss 0.6931471805599453
iteration 2 batch 7180 trainingloss 0.6931471805599453
iteration 2 batch 7190 trainingloss 0.6931471805599453
iteration 2 batch 7200 trainingloss 0.6931471805599453
iteration 2 batch 7210 trainingloss 0.6931471805599453
iteration 2 batch 7220 trainingloss 0.6931471805599453
iteration 2 batch 7230 trainingloss 0.6931471805599453
iteration 2 batch 7240 trainingloss 0.6931471805599453
iteration 2 batch 7250 trainingloss 0.6901793251455568
iteration 2 batch 7260 trainingloss 0.6931471805599453
iteration 2 batch 7270 trainingloss 0.6931471805599453
iteration 2 batch 7280 trainingloss 0.6931471805599453
iteration 2 batch 7290 trainingloss 0.6931471805599453
iteration 2 batch 7300 trainingloss 0.6931471805599453
iteration 2 batch 7310 trainingloss 0.6931471805599453
iteration 2 batch 7320 trainingloss 0.6931471805599453
iteration 2 batch 7330 trainingloss 0.6931471805599453
iteration 2 batch 7340 trainingloss 0.6931471805599453
iteration 2 batch 7350 trainingloss 0.6931471805599453
iteration 2 batch 7360 trainingloss 0.6931471805599453
iteration 2 batch 7370 trainingloss 0.6901793251455568
iteration 2 batch 7380 trainingloss 0.6931471805599453
iteration 2 batch 7390 trainingloss 0.6931471805599453
iteration 2 batch 7400 trainingloss 0.6931471805599453
iteration 2 batch 7410 trainingloss 0.6931471805599453
iteration 2 batch 7420 trainingloss 0.6931471805599453
iteration 2 batch 7430 trainingloss 0.6931471805599453
iteration 2 batch 7440 trainingloss 0.6931471805599453
iteration 2 batch 7450 trainingloss 0.6931471805599453
iteration 2 batch 7460 trainingloss 0.6916632528527511
iteration 2 batch 7470 trainingloss 0.6931471805599453
iteration 2 batch 7480 trainingloss 0.6931471805599453
iteration 2 batch 7490 trainingloss 0.6931471805599453
iteration 2 batch 7500 trainingloss 0.6931471805599453
iteration 2 batch 7510 trainingloss 0.6931471805599453
iteration 2 batch 7520 trainingloss 0.6931471805599453
iteration 2 batch 7530 trainingloss 0.6931471805599453
iteration 2 batch 7540 trainingloss 0.6931471805599453
iteration 2 batch 7550 trainingloss 0.6931471805599453
iteration 2 batch 7560 trainingloss 0.6931471805599453
iteration 2 batch 7570 trainingloss 0.6931471805599453
iteration 2 batch 7580 trainingloss 0.6931471805599453
iteration 2 batch 7590 trainingloss 0.6931471805599453
iteration 2 batch 7600 trainingloss 0.6931471805599453
iteration 2 batch 7610 trainingloss 0.6931471805599453
iteration 2 batch 7620 trainingloss 0.6931471805599453
iteration 2 batch 7630 trainingloss 0.6931471805599453
iteration 2 batch 7640 trainingloss 0.6931471805599453
iteration 2 batch 7650 trainingloss 0.6931471805599453
iteration 2 batch 7660 trainingloss 0.6931471805599453
iteration 2 batch 7670 trainingloss 0.6931471805599453
iteration 2 batch 7680 trainingloss 0.6931471805599453
iteration 2 batch 7690 trainingloss 0.6916632528527511
iteration 2 batch 7700 trainingloss 0.6931471805599453
iteration 2 batch 7710 trainingloss 0.6931471805599453
iteration 2 batch 7720 trainingloss 0.6916632528527511
iteration 2 batch 7730 trainingloss 0.6931471805599453
iteration 2 batch 7740 trainingloss 0.6931471805599453
iteration 2 batch 7750 trainingloss 0.6931471805599453
iteration 2 batch 7760 trainingloss 0.6916632528527511
iteration 2 batch 7770 trainingloss 0.6931471805599453
iteration 2 batch 7780 trainingloss 0.6931471805599453
iteration 2 batch 7790 trainingloss 0.6931471805599453
iteration 2 batch 7800 trainingloss 0.6916632528527511
iteration 2 batch 7810 trainingloss 0.6931471805599453
iteration 2 batch 7820 trainingloss 0.6931471805599453
iteration 2 batch 7830 trainingloss 0.6916632528527511
iteration 2 batch 7840 trainingloss 0.6931471805599453
iteration 2 batch 7850 trainingloss 0.6931471805599453
iteration 2 batch 7860 trainingloss 0.6931471805599453
iteration 2 batch 7870 trainingloss 0.6931471805599453
iteration 2 batch 7880 trainingloss 0.6931471805599453
iteration 2 batch 7890 trainingloss 0.6931471805599453
iteration 2 batch 7900 trainingloss 0.6931471805599453
iteration 2 batch 7910 trainingloss 0.6931471805599453
iteration 2 batch 7920 trainingloss 0.6931471805599453
iteration 2 batch 7930 trainingloss 0.6931471805599453
iteration 2 batch 7940 trainingloss 0.6931471805599453
iteration 2 batch 7950 trainingloss 0.6931471805599453
iteration 2 batch 7960 trainingloss 0.6931471805599453
iteration 2 batch 7970 trainingloss 0.6931471805599453
iteration 2 batch 7980 trainingloss 0.6931471805599453
iteration 2 batch 7990 trainingloss 0.6931471805599453
iteration 2 batch 8000 trainingloss 0.6931471805599453
iteration 2 batch 8010 trainingloss 0.6931471805599453
iteration 2 batch 8020 trainingloss 0.6931471805599453
iteration 2 batch 8030 trainingloss 0.6931471805599453
iteration 2 batch 8040 trainingloss 0.6931471805599453
iteration 2 batch 8050 trainingloss 0.6931471805599453
iteration 2 batch 8060 trainingloss 0.6931471805599453
iteration 2 batch 8070 trainingloss 0.6931471805599453
iteration 2 batch 8080 trainingloss 0.6916632528527511
iteration 2 batch 8090 trainingloss 0.6931471805599453
iteration 2 batch 8100 trainingloss 0.6931471805599453
iteration 2 batch 8110 trainingloss 0.6931471805599453
iteration 2 batch 8120 trainingloss 0.6931471805599453
iteration 2 batch 8130 trainingloss 0.6931471805599453
iteration 2 batch 8140 trainingloss 0.6931471805599453
iteration 2 batch 8150 trainingloss 0.6931471805599453
iteration 2 batch 8160 trainingloss 0.6931471805599453
iteration 2 batch 8170 trainingloss 0.6931471805599453
iteration 2 batch 8180 trainingloss 0.6931471805599453
iteration 2 batch 8190 trainingloss 0.6931471805599453
iteration 2 batch 8200 trainingloss 0.6931471805599453
iteration 2 batch 8210 trainingloss 0.6931471805599453
iteration 2 batch 8220 trainingloss 0.6931471805599453
iteration 2 batch 8230 trainingloss 0.6931471805599453
iteration 2 batch 8240 trainingloss 0.6931471805599453
iteration 2 batch 8250 trainingloss 0.6931471805599453
iteration 2 batch 8260 trainingloss 0.6931471805599453
iteration 2 batch 8270 trainingloss 0.6931471805599453
iteration 2 batch 8280 trainingloss 0.6931471805599453
iteration 2 batch 8290 trainingloss 0.6916632528527511
iteration 2 batch 8300 trainingloss 0.6931471805599453
iteration 2 batch 8310 trainingloss 0.6931471805599453
iteration 2 batch 8320 trainingloss 0.6931471805599453
iteration 2 batch 8330 trainingloss 0.6931471805599453
iteration 2 batch 8340 trainingloss 0.6931471805599453
iteration 2 batch 8350 trainingloss 0.6931471805599453
iteration 2 batch 8360 trainingloss 0.6931471805599453
iteration 2 batch 8370 trainingloss 0.6931471805599453
iteration 2 batch 8380 trainingloss 0.6931471805599453
iteration 2 batch 8390 trainingloss 0.6931471805599453
iteration 2 batch 8400 trainingloss 0.6931471805599453
iteration 2 batch 8410 trainingloss 0.6931471805599453
iteration 2 batch 8420 trainingloss 0.6931471805599453
iteration 2 batch 8430 trainingloss 0.6931471805599453
iteration 2 batch 8440 trainingloss 0.6931471805599453
iteration 2 batch 8450 trainingloss 0.6931471805599453
iteration 2 batch 8460 trainingloss 0.6931471805599453
iteration 2 batch 8470 trainingloss 0.6931471805599453
iteration 2 batch 8480 trainingloss 0.6931471805599453
iteration 2 batch 8490 trainingloss 0.6931471805599453
iteration 2 batch 8500 trainingloss 0.6931471805599453
iteration 2 batch 8510 trainingloss 0.6931471805599453
iteration 2 batch 8520 trainingloss 0.6916632528527511
iteration 2 batch 8530 trainingloss 0.6931471805599453
iteration 2 batch 8540 trainingloss 0.6931471805599453
iteration 2 batch 8550 trainingloss 0.6931471805599453
iteration 2 batch 8560 trainingloss 0.6931471805599453
iteration 2 batch 8570 trainingloss 0.6931471805599453
iteration 2 batch 8580 trainingloss 0.6931471805599453
iteration 2 batch 8590 trainingloss 0.6931471805599453
iteration 2 batch 8600 trainingloss 0.6931471805599453
iteration 2 batch 8610 trainingloss 0.6916632528527511
iteration 2 batch 8620 trainingloss 0.6931471805599453
iteration 2 batch 8630 trainingloss 0.6931471805599453
iteration 2 batch 8640 trainingloss 0.6931471805599453
iteration 2 batch 8650 trainingloss 0.6931471805599453
iteration 2 batch 8660 trainingloss 0.6931471805599453
iteration 2 batch 8670 trainingloss 0.6931471805599453
iteration 2 batch 8680 trainingloss 0.6916632528527511
iteration 2 batch 8690 trainingloss 0.6931471805599453
iteration 2 batch 8700 trainingloss 0.6916632528527511
iteration 2 batch 8710 trainingloss 0.6931471805599453
iteration 2 batch 8720 trainingloss 0.6931471805599453
iteration 2 batch 8730 trainingloss 0.6931471805599453
iteration 2 batch 8740 trainingloss 0.6931471805599453
iteration 2 batch 8750 trainingloss 0.6931471805599453
iteration 2 batch 8760 trainingloss 0.6931471805599453
iteration 2 batch 8770 trainingloss 0.6931471805599453
iteration 2 batch 8780 trainingloss 0.6931471805599453
iteration 2 batch 8790 trainingloss 0.6931471805599453
iteration 2 batch 8800 trainingloss 0.6931471805599453
iteration 2 batch 8810 trainingloss 0.6931471805599453
iteration 2 batch 8820 trainingloss 0.6931471805599453
iteration 2 batch 8830 trainingloss 0.6931471805599453
iteration 2 batch 8840 trainingloss 0.6931471805599453
iteration 2 batch 8850 trainingloss 0.6931471805599453
iteration 2 batch 8860 trainingloss 0.6916632528527511
iteration 2 batch 8870 trainingloss 0.6931471805599453
iteration 2 batch 8880 trainingloss 0.6931471805599453
iteration 2 batch 8890 trainingloss 0.6931471805599453
iteration 2 batch 8900 trainingloss 0.6931471805599453
iteration 2 batch 8910 trainingloss 0.6931471805599453
iteration 2 batch 8920 trainingloss 0.6931471805599453
iteration 2 batch 8930 trainingloss 0.6931471805599453
iteration 2 batch 8940 trainingloss 0.6931471805599453
iteration 2 batch 8950 trainingloss 0.6931471805599453
iteration 2 batch 8960 trainingloss 0.6931471805599453
iteration 2 batch 8970 trainingloss 0.6931471805599453
iteration 2 batch 8980 trainingloss 0.6931471805599453
iteration 2 batch 8990 trainingloss 0.6931471805599453
iteration 2 batch 9000 trainingloss 0.6931471805599453
iteration 2 batch 9010 trainingloss 0.6931471805599453
iteration 2 batch 9020 trainingloss 0.6931471805599453
iteration 2 batch 9030 trainingloss 0.6931471805599453
iteration 2 batch 9040 trainingloss 0.6931471805599453
iteration 2 batch 9050 trainingloss 0.6931471805599453
iteration 2 batch 9060 trainingloss 0.6931471805599453
iteration 2 batch 9070 trainingloss 0.6931471805599453
iteration 2 batch 9080 trainingloss 0.6931471805599453
iteration 2 batch 9090 trainingloss 0.6931471805599453
iteration 2 batch 9100 trainingloss 0.6931471805599453
iteration 2 batch 9110 trainingloss 0.6931471805599453
iteration 2 batch 9120 trainingloss 0.6931471805599453
iteration 2 batch 9130 trainingloss 0.6931471805599453
iteration 2 batch 9140 trainingloss 0.6931471805599453
iteration 2 batch 9150 trainingloss 0.6931471805599453
iteration 2 batch 9160 trainingloss 0.6931471805599453
iteration 2 batch 9170 trainingloss 0.6931471805599453
iteration 2 batch 9180 trainingloss 0.6931471805599453
iteration 2 batch 9190 trainingloss 0.6916632528527511
iteration 2 batch 9200 trainingloss 0.6931471805599453
iteration 2 batch 9210 trainingloss 0.6931471805599453
iteration 2 batch 9220 trainingloss 0.6901793251455568
iteration 2 batch 9230 trainingloss 0.6916632528527511
iteration 2 batch 9240 trainingloss 0.6931471805599453
iteration 2 batch 9250 trainingloss 0.6931471805599453
iteration 2 batch 9260 trainingloss 0.6931471805599453
iteration 2 batch 9270 trainingloss 0.6931471805599453
iteration 2 batch 9280 trainingloss 0.6931471805599453
iteration 2 batch 9290 trainingloss 0.6931471805599453
iteration 2 batch 9300 trainingloss 0.6931471805599453
iteration 2 batch 9310 trainingloss 0.6931471805599453
iteration 2 batch 9320 trainingloss 0.6931471805599453
iteration 2 batch 9330 trainingloss 0.6931471805599453
iteration 2 batch 9340 trainingloss 0.6931471805599453
iteration 2 batch 9350 trainingloss 0.6931471805599453
iteration 2 batch 9360 trainingloss 0.6931471805599453
iteration 2 batch 9370 trainingloss 0.6901793251455568
iteration 2 batch 9380 trainingloss 0.6916632528527511
iteration 2 batch 9390 trainingloss 0.6931471805599453
iteration 2 batch 9400 trainingloss 0.6931471805599453
iteration 2 batch 9410 trainingloss 0.6931471805599453
iteration 2 batch 9420 trainingloss 0.6931471805599453
iteration 2 batch 9430 trainingloss 0.6931471805599453
iteration 2 batch 9440 trainingloss 0.6901793251455568
iteration 2 batch 9450 trainingloss 0.6931471805599453
iteration 2 batch 9460 trainingloss 0.6931471805599453
iteration 2 batch 9470 trainingloss 0.6931471805599453
iteration 2 batch 9480 trainingloss 0.6916632528527511
iteration 2 batch 9490 trainingloss 0.6931471805599453
iteration 2 batch 9500 trainingloss 0.6931471805599453
iteration 2 batch 9510 trainingloss 0.6931471805599453
iteration 2 batch 9520 trainingloss 0.6931471805599453
iteration 2 batch 9530 trainingloss 0.6931471805599453
iteration 2 batch 9540 trainingloss 0.6931471805599453
iteration 2 batch 9550 trainingloss 0.6916632528527511
iteration 2 batch 9560 trainingloss 0.6931471805599453
iteration 2 batch 9570 trainingloss 0.6916632528527511
iteration 2 batch 9580 trainingloss 0.6931471805599453
iteration 2 batch 9590 trainingloss 0.6931471805599453
iteration 2 batch 9600 trainingloss 0.6931471805599453
iteration 2 batch 9610 trainingloss 0.6931471805599453
iteration 2 batch 9620 trainingloss 0.6931471805599453
iteration 2 batch 9630 trainingloss 0.6931471805599453
iteration 2 batch 9640 trainingloss 0.6931471805599453
iteration 2 batch 9650 trainingloss 0.6931471805599453
iteration 2 batch 9660 trainingloss 0.6931471805599453
iteration 2 batch 9670 trainingloss 0.6931471805599453
iteration 2 batch 9680 trainingloss 0.6931471805599453
iteration 2 batch 9690 trainingloss 0.6916632528527511
iteration 2 batch 9700 trainingloss 0.6931471805599453
iteration 2 batch 9710 trainingloss 0.6931471805599453
iteration 2 batch 9720 trainingloss 0.6931471805599453
iteration 2 batch 9730 trainingloss 0.6931471805599453
iteration 2 batch 9740 trainingloss 0.6931471805599453
iteration 2 batch 9750 trainingloss 0.6931471805599453
iteration 2 batch 9760 trainingloss 0.6931471805599453
iteration 2 batch 9770 trainingloss 0.6931471805599453
iteration 2 batch 9780 trainingloss 0.6931471805599453
iteration 2 batch 9790 trainingloss 0.6931471805599453
iteration 2 batch 9800 trainingloss 0.6931471805599453
iteration 2 batch 9810 trainingloss 0.6931471805599453
iteration 2 batch 9820 trainingloss 0.6931471805599453
iteration 2 batch 9830 trainingloss 0.6931471805599453
iteration 2 batch 9840 trainingloss 0.6931471805599453
iteration 2 batch 9850 trainingloss 0.6931471805599453
iteration 2 batch 9860 trainingloss 0.6931471805599453
iteration 2 batch 9870 trainingloss 0.6931471805599453
iteration 2 batch 9880 trainingloss 0.6931471805599453
iteration 2 batch 9890 trainingloss 0.6931471805599453
iteration 2 batch 9900 trainingloss 0.6931471805599453
iteration 2 batch 9910 trainingloss 0.6931471805599453
iteration 2 batch 9920 trainingloss 0.6931471805599453
iteration 2 batch 9930 trainingloss 0.6931471805599453
iteration 2 batch 9940 trainingloss 0.6931471805599453
iteration 2 batch 9950 trainingloss 0.6931471805599453
iteration 2 batch 9960 trainingloss 0.6931471805599453
iteration 2 batch 9970 trainingloss 0.6916632528527511
iteration 2 batch 9980 trainingloss 0.6931471805599453
iteration 2 batch 9990 trainingloss 0.6931471805599453
iteration 2 batch 10000 trainingloss 0.6931471805599453
iteration 2 batch 10010 trainingloss 0.6931471805599453
iteration 2 batch 10020 trainingloss 0.6931471805599453
iteration 2 batch 10030 trainingloss 0.6931471805599453
iteration 2 batch 10040 trainingloss 0.6931471805599453
iteration 2 batch 10050 trainingloss 0.6931471805599453
iteration 2 batch 10060 trainingloss 0.6931471805599453
iteration 2 batch 10070 trainingloss 0.6931471805599453
iteration 2 batch 10080 trainingloss 0.6931471805599453
iteration 2 batch 10090 trainingloss 0.6931471805599453
iteration 2 batch 10100 trainingloss 0.6931471805599453
iteration 2 batch 10110 trainingloss 0.6931471805599453
iteration 2 batch 10120 trainingloss 0.6931471805599453
iteration 2 batch 10130 trainingloss 0.6931471805599453
iteration 2 batch 10140 trainingloss 0.6931471805599453
iteration 2 batch 10150 trainingloss 0.6931471805599453
iteration 2 batch 10160 trainingloss 0.6931471805599453
iteration 2 batch 10170 trainingloss 0.6931471805599453
iteration 2 batch 10180 trainingloss 0.6931471805599453
iteration 2 batch 10190 trainingloss 0.6931471805599453
iteration 2 batch 10200 trainingloss 0.6931471805599453
iteration 2 batch 10210 trainingloss 0.6916632528527511
iteration 2 batch 10220 trainingloss 0.6916632528527511
iteration 2 batch 10230 trainingloss 0.6916632528527511
iteration 2 batch 10240 trainingloss 0.6931471805599453
iteration 2 batch 10250 trainingloss 0.6931471805599453
iteration 2 batch 10260 trainingloss 0.6931471805599453
iteration 2 batch 10270 trainingloss 0.6931471805599453
iteration 2 batch 10280 trainingloss 0.6931471805599453
iteration 2 batch 10290 trainingloss 0.6916632528527511
iteration 2 batch 10300 trainingloss 0.6931471805599453
iteration 2 batch 10310 trainingloss 0.6931471805599453
iteration 2 batch 10320 trainingloss 0.6931471805599453
iteration 2 batch 10330 trainingloss 0.6931471805599453
iteration 2 batch 10340 trainingloss 0.6931471805599453
iteration 2 batch 10350 trainingloss 0.6916632528527511
iteration 2 batch 10360 trainingloss 0.6931471805599453
iteration 2 batch 10370 trainingloss 0.6916632528527511
iteration 2 batch 10380 trainingloss 0.6931471805599453
iteration 2 batch 10390 trainingloss 0.6931471805599453
iteration 2 batch 10400 trainingloss 0.6931471805599453
iteration 2 batch 10410 trainingloss 0.6931471805599453
iteration 2 batch 10420 trainingloss 0.6931471805599453
iteration 2 batch 10430 trainingloss 0.6931471805599453
iteration 2 batch 10440 trainingloss 0.6931471805599453
iteration 2 batch 10450 trainingloss 0.6931471805599453
iteration 2 batch 10460 trainingloss 0.6931471805599453
iteration 2 batch 10470 trainingloss 0.6931471805599453
iteration 2 batch 10480 trainingloss 0.6931471805599453
iteration 2 batch 10490 trainingloss 0.6931471805599453
iteration 2 batch 10500 trainingloss 0.6931471805599453
iteration 2 batch 10510 trainingloss 0.6931471805599453
iteration 2 batch 10520 trainingloss 0.6931471805599453
iteration 2 batch 10530 trainingloss 0.6931471805599453
iteration 2 batch 10540 trainingloss 0.6931471805599453
iteration 2 batch 10550 trainingloss 0.6931471805599453
iteration 2 batch 10560 trainingloss 0.6916632528527511
iteration 2 batch 10570 trainingloss 0.6931471805599453
iteration 2 batch 10580 trainingloss 0.6916632528527511
iteration 2 batch 10590 trainingloss 0.6931471805599453
iteration 2 batch 10600 trainingloss 0.6931471805599453
iteration 2 batch 10610 trainingloss 0.6931471805599453
iteration 2 batch 10620 trainingloss 0.6931471805599453
iteration 2 batch 10630 trainingloss 0.6931471805599453
iteration 2 batch 10640 trainingloss 0.6931471805599453
iteration 2 batch 10650 trainingloss 0.6931471805599453
iteration 2 batch 10660 trainingloss 0.6916632528527511
iteration 2 batch 10670 trainingloss 0.6931471805599453
iteration 2 batch 10680 trainingloss 0.6931471805599453
iteration 2 batch 10690 trainingloss 0.6931471805599453
iteration 2 batch 10700 trainingloss 0.6931471805599453
iteration 2 batch 10710 trainingloss 0.6931471805599453
iteration 2 batch 10720 trainingloss 0.6916632528527511
iteration 2 batch 10730 trainingloss 0.6931471805599453
iteration 2 batch 10740 trainingloss 0.6931471805599453
iteration 2 batch 10750 trainingloss 0.6931471805599453
iteration 2 batch 10760 trainingloss 0.6931471805599453
iteration 2 batch 10770 trainingloss 0.6931471805599453
iteration 2 batch 10780 trainingloss 0.6931471805599453
iteration 2 batch 10790 trainingloss 0.6931471805599453
iteration 2 batch 10800 trainingloss 0.6931471805599453
iteration 2 batch 10810 trainingloss 0.6931471805599453
iteration 2 batch 10820 trainingloss 0.6931471805599453
iteration 2 batch 10830 trainingloss 0.6931471805599453
iteration 2 batch 10840 trainingloss 0.6931471805599453
iteration 2 batch 10850 trainingloss 0.6916632528527511
iteration 2 batch 10860 trainingloss 0.6901793251455568
iteration 2 batch 10870 trainingloss 0.6931471805599453
iteration 2 batch 10880 trainingloss 0.6916632528527511
iteration 2 batch 10890 trainingloss 0.6931471805599453
iteration 2 batch 10900 trainingloss 0.6901793251455568
iteration 2 batch 10910 trainingloss 0.6916632528527511
iteration 2 batch 10920 trainingloss 0.6931471805599453
iteration 2 batch 10930 trainingloss 0.6931471805599453
iteration 2 batch 10940 trainingloss 0.6931471805599453
iteration 2 batch 10950 trainingloss 0.6931471805599453
iteration 2 batch 10960 trainingloss 0.6931471805599453
iteration 2 batch 10970 trainingloss 0.6931471805599453
iteration 2 batch 10980 trainingloss 0.6916632528527511
iteration 2 batch 10990 trainingloss 0.6931471805599453
iteration 2 batch 11000 trainingloss 0.6931471805599453
iteration 2 batch 11010 trainingloss 0.6931471805599453
iteration 2 batch 11020 trainingloss 0.6931471805599453
iteration 2 batch 11030 trainingloss 0.6931471805599453
iteration 2 batch 11040 trainingloss 0.6931471805599453
iteration 2 batch 11050 trainingloss 0.6931471805599453
iteration 2 batch 11060 trainingloss 0.6931471805599453
iteration 2 batch 11070 trainingloss 0.6931471805599453
iteration 2 batch 11080 trainingloss 0.6916632528527511
iteration 2 batch 11090 trainingloss 0.6931471805599453
iteration 2 batch 11100 trainingloss 0.6931471805599453
iteration 2 batch 11110 trainingloss 0.6931471805599453
iteration 2 batch 11120 trainingloss 0.6931471805599453
iteration 2 batch 11130 trainingloss 0.6931471805599453
iteration 2 batch 11140 trainingloss 0.6931471805599453
iteration 2 batch 11150 trainingloss 0.6931471805599453
iteration 2 batch 11160 trainingloss 0.6931471805599453
iteration 2 batch 11170 trainingloss 0.6931471805599453
iteration 2 batch 11180 trainingloss 0.6916632528527511
iteration 2 batch 11190 trainingloss 0.6931471805599453
iteration 2 batch 11200 trainingloss 0.6931471805599453
iteration 2 batch 11210 trainingloss 0.6931471805599453
iteration 2 batch 11220 trainingloss 0.6931471805599453
iteration 2 batch 11230 trainingloss 0.6931471805599453
iteration 2 batch 11240 trainingloss 0.6931471805599453
iteration 2 batch 11250 trainingloss 0.6931471805599453
iteration 2 batch 11260 trainingloss 0.6931471805599453
iteration 2 batch 11270 trainingloss 0.6931471805599453
iteration 2 batch 11280 trainingloss 0.6931471805599453
iteration 2 batch 11290 trainingloss 0.6931471805599453
iteration 2 batch 11300 trainingloss 0.6916632528527511
iteration 2 batch 11310 trainingloss 0.6931471805599453
iteration 2 batch 11320 trainingloss 0.6916632528527511
iteration 2 batch 11330 trainingloss 0.6931471805599453
iteration 2 batch 11340 trainingloss 0.6931471805599453
iteration 2 batch 11350 trainingloss 0.6931471805599453
iteration 2 batch 11360 trainingloss 0.6931471805599453
iteration 2 batch 11370 trainingloss 0.6931471805599453
iteration 2 batch 11380 trainingloss 0.6931471805599453
iteration 2 batch 11390 trainingloss 0.6916632528527511
iteration 2 batch 11400 trainingloss 0.6931471805599453
iteration 2 batch 11410 trainingloss 0.6901793251455568
iteration 2 batch 11420 trainingloss 0.6931471805599453
iteration 2 batch 11430 trainingloss 0.6931471805599453
iteration 2 batch 11440 trainingloss 0.6931471805599453
iteration 2 batch 11450 trainingloss 0.6931471805599453
iteration 2 batch 11460 trainingloss 0.6931471805599453
iteration 2 batch 11470 trainingloss 0.6931471805599453
iteration 2 batch 11480 trainingloss 0.6931471805599453
iteration 2 batch 11490 trainingloss 0.6931471805599453
iteration 2 batch 11500 trainingloss 0.6931471805599453
iteration 2 batch 11510 trainingloss 0.6931471805599453
iteration 2 batch 11520 trainingloss 0.6931471805599453
iteration 2 batch 11530 trainingloss 0.6931471805599453
iteration 2 batch 11540 trainingloss 0.6931471805599453
iteration 2 batch 11550 trainingloss 0.6931471805599453
iteration 2 batch 11560 trainingloss 0.6931471805599453
iteration 2 batch 11570 trainingloss 0.6931471805599453
iteration 2 batch 11580 trainingloss 0.6931471805599453
iteration 2 batch 11590 trainingloss 0.6931471805599453
iteration 2 batch 11600 trainingloss 0.6916632528527511
iteration 2 batch 11610 trainingloss 0.6931471805599453
iteration 2 batch 11620 trainingloss 0.6931471805599453
iteration 2 batch 11630 trainingloss 0.6931471805599453
iteration 2 batch 11640 trainingloss 0.6931471805599453
iteration 2 batch 11650 trainingloss 0.6916632528527511
iteration 2 batch 11660 trainingloss 0.6931471805599453
iteration 2 batch 11670 trainingloss 0.6931471805599453
iteration 2 batch 11680 trainingloss 0.6931471805599453
iteration 2 batch 11690 trainingloss 0.6931471805599453
iteration 2 batch 11700 trainingloss 0.6931471805599453
iteration 2 batch 11710 trainingloss 0.6931471805599453
iteration 2 batch 11720 trainingloss 0.6931471805599453
iteration 2 batch 11730 trainingloss 0.6916632528527511
iteration 2 batch 11740 trainingloss 0.6931471805599453
iteration 2 batch 11750 trainingloss 0.6916632528527511
iteration 2 batch 11760 trainingloss 0.6931471805599453
iteration 2 batch 11770 trainingloss 0.6931471805599453
iteration 2 batch 11780 trainingloss 0.6916632528527511
iteration 2 batch 11790 trainingloss 0.6931471805599453
iteration 2 batch 11800 trainingloss 0.6931471805599453
iteration 2 batch 11810 trainingloss 0.6931471805599453
iteration 2 batch 11820 trainingloss 0.6931471805599453
iteration 2 batch 11830 trainingloss 0.6931471805599453
iteration 2 batch 11840 trainingloss 0.6931471805599453
iteration 2 batch 11850 trainingloss 0.6931471805599453
iteration 2 batch 11860 trainingloss 0.6931471805599453
iteration 2 batch 11870 trainingloss 0.6931471805599453
iteration 2 batch 11880 trainingloss 0.6931471805599453
iteration 2 batch 11890 trainingloss 0.6931471805599453
iteration 2 batch 11900 trainingloss 0.6931471805599453
iteration 2 batch 11910 trainingloss 0.6931471805599453
iteration 2 batch 11920 trainingloss 0.6931471805599453
iteration 2 batch 11930 trainingloss 0.6931471805599453
iteration 2 batch 11940 trainingloss 0.6931471805599453
iteration 2 batch 11950 trainingloss 0.6931471805599453
iteration 2 batch 11960 trainingloss 0.6931471805599453
iteration 2 batch 11970 trainingloss 0.6916632528527511
iteration 2 batch 11980 trainingloss 0.6931471805599453
iteration 2 batch 11990 trainingloss 0.6931471805599453
iteration 2 batch 12000 trainingloss 0.6931471805599453
iteration 2 batch 12010 trainingloss 0.6916632528527511
iteration 2 batch 12020 trainingloss 0.6931471805599453
iteration 2 batch 12030 trainingloss 0.6931471805599453
iteration 2 batch 12040 trainingloss 0.6931471805599453
iteration 2 batch 12050 trainingloss 0.6931471805599453
iteration 2 batch 12060 trainingloss 0.6931471805599453
iteration 2 batch 12070 trainingloss 0.6931471805599453
iteration 2 batch 12080 trainingloss 0.6931471805599453
iteration 2 batch 12090 trainingloss 0.6931471805599453
iteration 2 batch 12100 trainingloss 0.6931471805599453
iteration 2 batch 12110 trainingloss 0.6931471805599453
iteration 2 batch 12120 trainingloss 0.6931471805599453
iteration 2 batch 12130 trainingloss 0.6931471805599453
iteration 2 batch 12140 trainingloss 0.6931471805599453
iteration 2 batch 12150 trainingloss 0.6931471805599453
iteration 2 batch 12160 trainingloss 0.6931471805599453
iteration 2 batch 12170 trainingloss 0.6931471805599453
iteration 2 batch 12180 trainingloss 0.6931471805599453
iteration 2 batch 12190 trainingloss 0.6931471805599453
iteration 2 batch 12200 trainingloss 0.6931471805599453
iteration 2 batch 12210 trainingloss 0.6931471805599453
iteration 2 batch 12220 trainingloss 0.6916632528527511
iteration 2 batch 12230 trainingloss 0.6931471805599453
iteration 2 batch 12240 trainingloss 0.6931471805599453
iteration 2 batch 12250 trainingloss 0.6931471805599453
iteration 2 batch 12260 trainingloss 0.6931471805599453
iteration 2 batch 12270 trainingloss 0.6931471805599453
iteration 2 batch 12280 trainingloss 0.6931471805599453
iteration 2 batch 12290 trainingloss 0.6931471805599453
iteration 2 batch 12300 trainingloss 0.6931471805599453
iteration 2 batch 12310 trainingloss 0.6931471805599453
iteration 2 batch 12320 trainingloss 0.6931471805599453
iteration 2 batch 12330 trainingloss 0.6931471805599453
iteration 2 batch 12340 trainingloss 0.6931471805599453
iteration 2 batch 12350 trainingloss 0.6931471805599453
iteration 2 batch 12360 trainingloss 0.6931471805599453
iteration 2 batch 12370 trainingloss 0.6931471805599453
iteration 2 batch 12380 trainingloss 0.6931471805599453
iteration 2 batch 12390 trainingloss 0.6931471805599453
iteration 2 batch 12400 trainingloss 0.6931471805599453
iteration 2 batch 12410 trainingloss 0.6916632528527511
iteration 2 batch 12420 trainingloss 0.6931471805599453
iteration 2 batch 12430 trainingloss 0.6931471805599453
iteration 2 batch 12440 trainingloss 0.6931471805599453
iteration 2 batch 12450 trainingloss 0.6931471805599453
iteration 2 batch 12460 trainingloss 0.6931471805599453
iteration 2 batch 12470 trainingloss 0.6931471805599453
iteration 2 batch 12480 trainingloss 0.6931471805599453
iteration 2 batch 12490 trainingloss 0.6916632528527511
iteration 2 batch 12500 trainingloss 0.6931471805599453
iteration 2 batch 12510 trainingloss 0.6931471805599453
iteration 2 batch 12520 trainingloss 0.6931471805599453
iteration 2 batch 12530 trainingloss 0.6931471805599453
iteration 2 batch 12540 trainingloss 0.6931471805599453
iteration 2 batch 12550 trainingloss 0.6931471805599453
iteration 2 batch 12560 trainingloss 0.6931471805599453
iteration 2 batch 12570 trainingloss 0.6931471805599453
iteration 2 batch 12580 trainingloss 0.6931471805599453
iteration 2 batch 12590 trainingloss 0.6931471805599453
iteration 2 batch 12600 trainingloss 0.6931471805599453
iteration 2 batch 12610 trainingloss 0.6931471805599453
iteration 2 batch 12620 trainingloss 0.6931471805599453
iteration 2 batch 12630 trainingloss 0.6931471805599453
iteration 2 batch 12640 trainingloss 0.6931471805599453
iteration 2 batch 12650 trainingloss 0.6931471805599453
iteration 2 batch 12660 trainingloss 0.6931471805599453
iteration 2 batch 12670 trainingloss 0.6931471805599453
iteration 2 batch 12680 trainingloss 0.6916632528527511
iteration 2 batch 12690 trainingloss 0.6931471805599453
iteration 2 batch 12700 trainingloss 0.6931471805599453
iteration 2 batch 12710 trainingloss 0.6931471805599453
iteration 2 batch 12720 trainingloss 0.6931471805599453
iteration 2 batch 12730 trainingloss 0.6931471805599453
iteration 2 batch 12740 trainingloss 0.6931471805599453
iteration 2 batch 12750 trainingloss 0.6931471805599453
iteration 2 batch 12760 trainingloss 0.6931471805599453
iteration 2 batch 12770 trainingloss 0.6931471805599453
iteration 2 batch 12780 trainingloss 0.6931471805599453
iteration 2 batch 12790 trainingloss 0.6931471805599453
iteration 2 batch 12800 trainingloss 0.6916632528527511
iteration 2 batch 12810 trainingloss 0.6931471805599453
iteration 2 batch 12820 trainingloss 0.6931471805599453
iteration 2 batch 12830 trainingloss 0.6931471805599453
iteration 2 batch 12840 trainingloss 0.6931471805599453
iteration 2 batch 12850 trainingloss 0.6931471805599453
iteration 2 batch 12860 trainingloss 0.6931471805599453
iteration 2 batch 12870 trainingloss 0.6916632528527511
iteration 2 batch 12880 trainingloss 0.6931471805599453
iteration 2 batch 12890 trainingloss 0.6931471805599453
iteration 2 batch 12900 trainingloss 0.6931471805599453
iteration 2 batch 12910 trainingloss 0.6931471805599453
iteration 2 batch 12920 trainingloss 0.6931471805599453
iteration 2 batch 12930 trainingloss 0.6931471805599453
iteration 2 batch 12940 trainingloss 0.6916632528527511
iteration 2 batch 12950 trainingloss 0.6916632528527511
iteration 2 batch 12960 trainingloss 0.6931471805599453
iteration 2 batch 12970 trainingloss 0.6931471805599453
iteration 2 batch 12980 trainingloss 0.6931471805599453
iteration 2 batch 12990 trainingloss 0.6931471805599453
iteration 2 batch 13000 trainingloss 0.6931471805599453
iteration 2 batch 13010 trainingloss 0.6931471805599453
iteration 2 batch 13020 trainingloss 0.6931471805599453
iteration 2 batch 13030 trainingloss 0.6931471805599453
iteration 2 batch 13040 trainingloss 0.6931471805599453
iteration 2 batch 13050 trainingloss 0.6931471805599453
iteration 2 batch 13060 trainingloss 0.6931471805599453
iteration 2 batch 13070 trainingloss 0.6931471805599453
iteration 2 batch 13080 trainingloss 0.6931471805599453
iteration 2 batch 13090 trainingloss 0.6931471805599453
iteration 2 batch 13100 trainingloss 0.6931471805599453
iteration 2 batch 13110 trainingloss 0.6931471805599453
iteration 2 batch 13120 trainingloss 0.6931471805599453
iteration 2 batch 13130 trainingloss 0.6931471805599453
iteration 2 batch 13140 trainingloss 0.6931471805599453
iteration 2 batch 13150 trainingloss 0.6931471805599453
iteration 2 batch 13160 trainingloss 0.6931471805599453
iteration 2 batch 13170 trainingloss 0.6931471805599453
iteration 2 batch 13180 trainingloss 0.6931471805599453
iteration 2 batch 13190 trainingloss 0.6931471805599453
iteration 2 batch 13200 trainingloss 0.6931471805599453
iteration 2 batch 13210 trainingloss 0.6901793251455568
iteration 2 batch 13220 trainingloss 0.6931471805599453
iteration 2 batch 13230 trainingloss 0.6931471805599453
iteration 2 batch 13240 trainingloss 0.6931471805599453
iteration 2 batch 13250 trainingloss 0.6931471805599453
iteration 2 batch 13260 trainingloss 0.6931471805599453
iteration 2 batch 13270 trainingloss 0.6931471805599453
iteration 2 batch 13280 trainingloss 0.6931471805599453
iteration 2 batch 13290 trainingloss 0.6931471805599453
iteration 2 batch 13300 trainingloss 0.6931471805599453
iteration 2 batch 13310 trainingloss 0.6931471805599453
iteration 2 batch 13320 trainingloss 0.6916632528527511
iteration 2 batch 13330 trainingloss 0.6931471805599453
iteration 2 batch 13340 trainingloss 0.6931471805599453
iteration 2 batch 13350 trainingloss 0.6931471805599453
iteration 2 batch 13360 trainingloss 0.6931471805599453
iteration 2 batch 13370 trainingloss 0.6931471805599453
iteration 2 batch 13380 trainingloss 0.6931471805599453
iteration 2 batch 13390 trainingloss 0.6931471805599453
iteration 2 batch 13400 trainingloss 0.6901793251455568
iteration 2 batch 13410 trainingloss 0.6931471805599453
iteration 2 batch 13420 trainingloss 0.6931471805599453
iteration 2 batch 13430 trainingloss 0.6931471805599453
iteration 2 batch 13440 trainingloss 0.6916632528527511
iteration 2 batch 13450 trainingloss 0.6931471805599453
iteration 2 batch 13460 trainingloss 0.6931471805599453
iteration 2 batch 13470 trainingloss 0.6931471805599453
iteration 2 batch 13480 trainingloss 0.6931471805599453
iteration 2 batch 13490 trainingloss 0.6931471805599453
iteration 2 batch 13500 trainingloss 0.6931471805599453
iteration 2 batch 13510 trainingloss 0.6931471805599453
iteration 2 batch 13520 trainingloss 0.6931471805599453
iteration 2 batch 13530 trainingloss 0.6931471805599453
iteration 2 batch 13540 trainingloss 0.6931471805599453
iteration 2 batch 13550 trainingloss 0.6931471805599453
iteration 2 batch 13560 trainingloss 0.6931471805599453
iteration 2 batch 13570 trainingloss 0.6931471805599453
iteration 2 batch 13580 trainingloss 0.6931471805599453
iteration 2 batch 13590 trainingloss 0.6931471805599453
iteration 2 batch 13600 trainingloss 0.6931471805599453
iteration 2 batch 13610 trainingloss 0.6916632528527511
iteration 2 batch 13620 trainingloss 0.6916632528527511
iteration 2 batch 13630 trainingloss 0.6931471805599453
iteration 2 batch 13640 trainingloss 0.6931471805599453
iteration 2 batch 13650 trainingloss 0.6931471805599453
iteration 2 batch 13660 trainingloss 0.6931471805599453
iteration 2 batch 13670 trainingloss 0.6931471805599453
iteration 2 batch 13680 trainingloss 0.6931471805599453
iteration 2 batch 13690 trainingloss 0.6931471805599453
iteration 2 batch 13700 trainingloss 0.6931471805599453
iteration 2 batch 13710 trainingloss 0.6931471805599453
iteration 2 batch 13720 trainingloss 0.6931471805599453
iteration 2 batch 13730 trainingloss 0.6931471805599453
iteration 2 batch 13740 trainingloss 0.6931471805599453
iteration 2 batch 13750 trainingloss 0.6916632528527511
iteration 2 batch 13760 trainingloss 0.6931471805599453
iteration 2 batch 13770 trainingloss 0.6931471805599453
iteration 2 batch 13780 trainingloss 0.6931471805599453
iteration 2 batch 13790 trainingloss 0.6931471805599453
iteration 2 batch 13800 trainingloss 0.6931471805599453
iteration 2 batch 13810 trainingloss 0.6931471805599453
iteration 2 batch 13820 trainingloss 0.6931471805599453
iteration 2 batch 13830 trainingloss 0.6916632528527511
iteration 2 batch 13840 trainingloss 0.6931471805599453
iteration 2 batch 13850 trainingloss 0.6916632528527511
iteration 2 batch 13860 trainingloss 0.6931471805599453
iteration 2 batch 13870 trainingloss 0.6916632528527511
iteration 2 batch 13880 trainingloss 0.6931471805599453
iteration 2 batch 13890 trainingloss 0.6931471805599453
iteration 2 batch 13900 trainingloss 0.6916632528527511
iteration 2 batch 13910 trainingloss 0.6931471805599453
iteration 2 batch 13920 trainingloss 0.6931471805599453
iteration 2 batch 13930 trainingloss 0.6916632528527511
iteration 2 batch 13940 trainingloss 0.6931471805599453
iteration 2 batch 13950 trainingloss 0.6931471805599453
iteration 2 batch 13960 trainingloss 0.6931471805599453
iteration 2 batch 13970 trainingloss 0.6931471805599453
iteration 2 batch 13980 trainingloss 0.6931471805599453
iteration 2 batch 13990 trainingloss 0.6931471805599453
iteration 2 batch 14000 trainingloss 0.6931471805599453
iteration 2 batch 14010 trainingloss 0.6931471805599453
iteration 2 batch 14020 trainingloss 0.6931471805599453
iteration 2 batch 14030 trainingloss 0.6931471805599453
iteration 2 batch 14040 trainingloss 0.6931471805599453
iteration 2 batch 14050 trainingloss 0.6916632528527511
iteration 2 batch 14060 trainingloss 0.6916632528527511
iteration 2 batch 14070 trainingloss 0.6931471805599453
iteration 2 batch 14080 trainingloss 0.6931471805599453
iteration 2 batch 14090 trainingloss 0.6931471805599453
iteration 2 batch 14100 trainingloss 0.6931471805599453
iteration 2 batch 14110 trainingloss 0.6931471805599453
iteration 2 batch 14120 trainingloss 0.6931471805599453
iteration 2 batch 14130 trainingloss 0.6931471805599453
iteration 2 batch 14140 trainingloss 0.6931471805599453
iteration 2 batch 14150 trainingloss 0.6916632528527511
iteration 2 batch 14160 trainingloss 0.6931471805599453
iteration 2 batch 14170 trainingloss 0.6916632528527511
iteration 2 batch 14180 trainingloss 0.6931471805599453
iteration 2 batch 14190 trainingloss 0.6931471805599453
iteration 2 batch 14200 trainingloss 0.6931471805599453
iteration 2 batch 14210 trainingloss 0.6931471805599453
iteration 2 batch 14220 trainingloss 0.6931471805599453
iteration 2 batch 14230 trainingloss 0.6931471805599453
iteration 2 batch 14240 trainingloss 0.6931471805599453
iteration 2 batch 14250 trainingloss 0.6931471805599453
iteration 2 batch 14260 trainingloss 0.6931471805599453
iteration 2 batch 14270 trainingloss 0.6931471805599453
iteration 2 batch 14280 trainingloss 0.6931471805599453
iteration 2 batch 14290 trainingloss 0.6931471805599453
iteration 2 batch 14300 trainingloss 0.6931471805599453
iteration 2 batch 14310 trainingloss 0.6931471805599453
iteration 2 batch 14320 trainingloss 0.6916632528527511
iteration 2 batch 14330 trainingloss 0.6931471805599453
iteration 2 batch 14340 trainingloss 0.6931471805599453
iteration 2 batch 14350 trainingloss 0.6916632528527511
iteration 2 batch 14360 trainingloss 0.6916632528527511
iteration 2 batch 14370 trainingloss 0.6916632528527511
iteration 2 batch 14380 trainingloss 0.6931471805599453
iteration 2 batch 14390 trainingloss 0.6931471805599453
iteration 2 batch 14400 trainingloss 0.6931471805599453
iteration 2 batch 14410 trainingloss 0.6931471805599453
iteration 2 batch 14420 trainingloss 0.6931471805599453
iteration 2 batch 14430 trainingloss 0.6931471805599453
iteration 2 batch 14440 trainingloss 0.6931471805599453
iteration 2 batch 14450 trainingloss 0.6931471805599453
iteration 2 batch 14460 trainingloss 0.6931471805599453
iteration 2 batch 14470 trainingloss 0.6931471805599453
iteration 2 batch 14480 trainingloss 0.6931471805599453
iteration 2 batch 14490 trainingloss 0.6931471805599453
iteration 2 batch 14500 trainingloss 0.6931471805599453
iteration 2 batch 14510 trainingloss 0.6931471805599453
iteration 2 batch 14520 trainingloss 0.6931471805599453
iteration 2 batch 14530 trainingloss 0.6931471805599453
iteration 2 batch 14540 trainingloss 0.6931471805599453
iteration 2 batch 14550 trainingloss 0.6931471805599453
iteration 2 batch 14560 trainingloss 0.6931471805599453
iteration 2 batch 14570 trainingloss 0.6931471805599453
iteration 2 batch 14580 trainingloss 0.6916632528527511
iteration 2 batch 14590 trainingloss 0.6931471805599453
iteration 2 batch 14600 trainingloss 0.6916632528527511
iteration 2 batch 14610 trainingloss 0.6931471805599453
iteration 2 batch 14620 trainingloss 0.6931471805599453
iteration 2 batch 14630 trainingloss 0.6916632528527511
iteration 2 batch 14640 trainingloss 0.6931471805599453
iteration 2 batch 14650 trainingloss 0.6931471805599453
iteration 2 batch 14660 trainingloss 0.6931471805599453
iteration 2 batch 14670 trainingloss 0.6931471805599453
iteration 2 batch 14680 trainingloss 0.6931471805599453
iteration 2 batch 14690 trainingloss 0.6916632528527511
iteration 2 batch 14700 trainingloss 0.6931471805599453
iteration 2 batch 14710 trainingloss 0.6931471805599453
iteration 2 batch 14720 trainingloss 0.6931471805599453
iteration 2 batch 14730 trainingloss 0.6931471805599453
iteration 2 batch 14740 trainingloss 0.6931471805599453
iteration 2 batch 14750 trainingloss 0.6931471805599453
iteration 2 batch 14760 trainingloss 0.6931471805599453
iteration 2 batch 14770 trainingloss 0.6931471805599453
iteration 2 batch 14780 trainingloss 0.6931471805599453
iteration 2 batch 14790 trainingloss 0.6931471805599453
iteration 2 batch 14800 trainingloss 0.6931471805599453
iteration 2 batch 14810 trainingloss 0.6931471805599453
iteration 2 batch 14820 trainingloss 0.6931471805599453
iteration 2 batch 14830 trainingloss 0.6931471805599453
iteration 2 batch 14840 trainingloss 0.6931471805599453
iteration 2 batch 14850 trainingloss 0.6931471805599453
iteration 2 batch 14860 trainingloss 0.6931471805599453
iteration 2 batch 14870 trainingloss 0.6931471805599453
iteration 2 batch 14880 trainingloss 0.6931471805599453
iteration 2 batch 14890 trainingloss 0.6931471805599453
iteration 2 batch 14900 trainingloss 0.6931471805599453
iteration 2 batch 14910 trainingloss 0.6931471805599453
iteration 2 batch 14920 trainingloss 0.6931471805599453
iteration 2 batch 14930 trainingloss 0.6916632528527511
iteration 2 batch 14940 trainingloss 0.6931471805599453
iteration 2 batch 14950 trainingloss 0.6931471805599453
iteration 2 batch 14960 trainingloss 0.6931471805599453
iteration 2 batch 14970 trainingloss 0.6931471805599453
iteration 2 batch 14980 trainingloss 0.6916632528527511
iteration 2 batch 14990 trainingloss 0.6931471805599453
iteration 2 batch 15000 trainingloss 0.6931471805599453
iteration 2 batch 15010 trainingloss 0.6931471805599453
iteration 2 batch 15020 trainingloss 0.6931471805599453
iteration 2 batch 15030 trainingloss 0.6931471805599453
iteration 2 batch 15040 trainingloss 0.6931471805599453
iteration 2 batch 15050 trainingloss 0.6931471805599453
iteration 2 batch 15060 trainingloss 0.6931471805599453
iteration 2 batch 15070 trainingloss 0.6931471805599453
iteration 2 batch 15080 trainingloss 0.6931471805599453
iteration 2 batch 15090 trainingloss 0.6931471805599453
iteration 2 batch 15100 trainingloss 0.6916632528527511
iteration 2 batch 15110 trainingloss 0.6931471805599453
iteration 2 batch 15120 trainingloss 0.6931471805599453
iteration 2 batch 15130 trainingloss 0.6931471805599453
iteration 2 batch 15140 trainingloss 0.6931471805599453
iteration 2 batch 15150 trainingloss 0.6916632528527511
iteration 2 batch 15160 trainingloss 0.6931471805599453
iteration 2 batch 15170 trainingloss 0.6931471805599453
iteration 2 batch 15180 trainingloss 0.6931471805599453
iteration 2 batch 15190 trainingloss 0.6931471805599453
iteration 2 batch 15200 trainingloss 0.6931471805599453
iteration 2 batch 15210 trainingloss 0.6931471805599453
iteration 2 batch 15220 trainingloss 0.6931471805599453
iteration 2 batch 15230 trainingloss 0.6931471805599453
iteration 2 batch 15240 trainingloss 0.6931471805599453
iteration 2 batch 15250 trainingloss 0.6931471805599453
iteration 2 batch 15260 trainingloss 0.6931471805599453
iteration 2 batch 15270 trainingloss 0.6931471805599453
iteration 2 batch 15280 trainingloss 0.6931471805599453
iteration 2 batch 15290 trainingloss 0.6931471805599453
iteration 2 batch 15300 trainingloss 0.6931471805599453
iteration 2 batch 15310 trainingloss 0.6931471805599453
iteration 2 batch 15320 trainingloss 0.6931471805599453
iteration 2 batch 15330 trainingloss 0.6931471805599453
iteration 2 batch 15340 trainingloss 0.6931471805599453
iteration 2 batch 15350 trainingloss 0.6931471805599453
iteration 2 batch 15360 trainingloss 0.6931471805599453
iteration 2 batch 15370 trainingloss 0.6916632528527511
iteration 2 batch 15380 trainingloss 0.6931471805599453
iteration 2 batch 15390 trainingloss 0.6916632528527511
iteration 2 batch 15400 trainingloss 0.6931471805599453
iteration 2 batch 15410 trainingloss 0.6931471805599453
iteration 2 batch 15420 trainingloss 0.6931471805599453
iteration 2 batch 15430 trainingloss 0.6931471805599453
iteration 2 batch 15440 trainingloss 0.6931471805599453
iteration 2 batch 15450 trainingloss 0.6931471805599453
iteration 2 batch 15460 trainingloss 0.6931471805599453
iteration 2 batch 15470 trainingloss 0.6931471805599453
iteration 2 batch 15480 trainingloss 0.6931471805599453
iteration 2 batch 15490 trainingloss 0.6916632528527511
iteration 2 batch 15500 trainingloss 0.6931471805599453
iteration 2 batch 15510 trainingloss 0.6931471805599453
iteration 2 batch 15520 trainingloss 0.6931471805599453
iteration 2 batch 15530 trainingloss 0.6931471805599453
iteration 2 batch 15540 trainingloss 0.6901793251455568
iteration 2 batch 15550 trainingloss 0.6931471805599453
iteration 2 batch 15560 trainingloss 0.6931471805599453
iteration 2 batch 15570 trainingloss 0.6931471805599453
iteration 2 batch 15580 trainingloss 0.6931471805599453
iteration 2 batch 15590 trainingloss 0.6931471805599453
iteration 2 batch 15600 trainingloss 0.6931471805599453
iteration 2 batch 15610 trainingloss 0.6931471805599453
iteration 2 batch 15620 trainingloss 0.6931471805599453
iteration 2 batch 15630 trainingloss 0.6931471805599453
iteration 2 batch 15640 trainingloss 0.6931471805599453
iteration 2 batch 15650 trainingloss 0.6931471805599453
iteration 2 batch 15660 trainingloss 0.6931471805599453
iteration 2 batch 15670 trainingloss 0.6916632528527511
iteration 2 batch 15680 trainingloss 0.6931471805599453
iteration 2 batch 15690 trainingloss 0.6931471805599453
iteration 2 batch 15700 trainingloss 0.6931471805599453
iteration 2 batch 15710 trainingloss 0.6931471805599453
iteration 2 batch 15720 trainingloss 0.6916632528527511
iteration 2 batch 15730 trainingloss 0.6931471805599453
iteration 2 batch 15740 trainingloss 0.6931471805599453
iteration 2 batch 15750 trainingloss 0.6931471805599453
iteration 2 batch 15760 trainingloss 0.6931471805599453
iteration 2 batch 15770 trainingloss 0.6931471805599453
iteration 2 batch 15780 trainingloss 0.6931471805599453
iteration 2 batch 15790 trainingloss 0.6931471805599453
iteration 2 batch 15800 trainingloss 0.6931471805599453
iteration 2 batch 15810 trainingloss 0.6931471805599453
iteration 2 batch 15820 trainingloss 0.6931471805599453
iteration 2 batch 15830 trainingloss 0.6931471805599453
iteration 2 batch 15840 trainingloss 0.6931471805599453
iteration 2 batch 15850 trainingloss 0.6931471805599453
iteration 2 batch 15860 trainingloss 0.6931471805599453
iteration 2 batch 15870 trainingloss 0.6931471805599453
iteration 2 batch 15880 trainingloss 0.6931471805599453
iteration 2 batch 15890 trainingloss 0.6931471805599453
iteration 2 batch 15900 trainingloss 0.6931471805599453
iteration 2 batch 15910 trainingloss 0.6931471805599453
iteration 2 batch 15920 trainingloss 0.6931471805599453
iteration 2 batch 15930 trainingloss 0.6931471805599453
iteration 2 batch 15940 trainingloss 0.6931471805599453
iteration 2 batch 15950 trainingloss 0.6931471805599453
iteration 2 batch 15960 trainingloss 0.6931471805599453
iteration 2 batch 15970 trainingloss 0.6931471805599453
iteration 2 batch 15980 trainingloss 0.6931471805599453
iteration 2 batch 15990 trainingloss 0.6931471805599453
iteration 2 batch 16000 trainingloss 0.6931471805599453
iteration 2 batch 16010 trainingloss 0.6931471805599453
iteration 2 batch 16020 trainingloss 0.6931471805599453
iteration 2 batch 16030 trainingloss 0.6931471805599453
iteration 2 batch 16040 trainingloss 0.6931471805599453
iteration 2 batch 16050 trainingloss 0.6931471805599453
iteration 2 batch 16060 trainingloss 0.6931471805599453
iteration 2 batch 16070 trainingloss 0.6916632528527511
iteration 2 batch 16080 trainingloss 0.6931471805599453
iteration 2 batch 16090 trainingloss 0.6931471805599453
iteration 2 batch 16100 trainingloss 0.6931471805599453
iteration 2 batch 16110 trainingloss 0.6916632528527511
iteration 2 batch 16120 trainingloss 0.6916632528527511
iteration 2 batch 16130 trainingloss 0.6931471805599453
iteration 2 batch 16140 trainingloss 0.6931471805599453
iteration 2 batch 16150 trainingloss 0.6931471805599453
iteration 2 batch 16160 trainingloss 0.6931471805599453
iteration 2 batch 16170 trainingloss 0.6931471805599453
iteration 2 batch 16180 trainingloss 0.6931471805599453
iteration 2 batch 16190 trainingloss 0.6931471805599453
iteration 2 batch 16200 trainingloss 0.6931471805599453
iteration 2 batch 16210 trainingloss 0.6916632528527511
iteration 2 batch 16220 trainingloss 0.6931471805599453
iteration 2 batch 16230 trainingloss 0.6931471805599453
iteration 2 batch 16240 trainingloss 0.6931471805599453
iteration 2 batch 16250 trainingloss 0.6931471805599453
iteration 2 batch 16260 trainingloss 0.6931471805599453
iteration 2 batch 16270 trainingloss 0.6931471805599453
iteration 2 batch 16280 trainingloss 0.6931471805599453
iteration 2 batch 16290 trainingloss 0.6931471805599453
iteration 2 batch 16300 trainingloss 0.6931471805599453
iteration 2 batch 16310 trainingloss 0.6931471805599453
iteration 2 batch 16320 trainingloss 0.6931471805599453
iteration 2 batch 16330 trainingloss 0.6931471805599453
iteration 2 batch 16340 trainingloss 0.6916632528527511
iteration 2 batch 16350 trainingloss 0.6931471805599453
iteration 2 batch 16360 trainingloss 0.6931471805599453
iteration 2 batch 16370 trainingloss 0.6931471805599453
iteration 2 batch 16380 trainingloss 0.6931471805599453
iteration 2 batch 16390 trainingloss 0.6916632528527511
iteration 2 batch 16400 trainingloss 0.6931471805599453
iteration 2 batch 16410 trainingloss 0.6931471805599453
iteration 2 batch 16420 trainingloss 0.6931471805599453
iteration 2 batch 16430 trainingloss 0.6931471805599453
iteration 2 batch 16440 trainingloss 0.6931471805599453
iteration 2 batch 16450 trainingloss 0.6931471805599453
iteration 2 batch 16460 trainingloss 0.6931471805599453
iteration 2 batch 16470 trainingloss 0.6931471805599453
iteration 2 batch 16480 trainingloss 0.6931471805599453
iteration 2 batch 16490 trainingloss 0.6931471805599453
iteration 2 batch 16500 trainingloss 0.6931471805599453
iteration 2 batch 16510 trainingloss 0.6931471805599453
iteration 2 batch 16520 trainingloss 0.6931471805599453
iteration 2 batch 16530 trainingloss 0.6931471805599453
iteration 2 batch 16540 trainingloss 0.6931471805599453
iteration 2 batch 16550 trainingloss 0.6931471805599453
iteration 2 batch 16560 trainingloss 0.6916632528527511
iteration 2 batch 16570 trainingloss 0.6931471805599453
iteration 2 batch 16580 trainingloss 0.6931471805599453
iteration 2 batch 16590 trainingloss 0.6931471805599453
iteration 2 batch 16600 trainingloss 0.6931471805599453
iteration 2 batch 16610 trainingloss 0.6931471805599453
iteration 2 batch 16620 trainingloss 0.6931471805599453
iteration 2 batch 16630 trainingloss 0.6931471805599453
iteration 2 batch 16640 trainingloss 0.6916632528527511
iteration 2 batch 16650 trainingloss 0.6931471805599453
iteration 2 batch 16660 trainingloss 0.6931471805599453
iteration 2 batch 16670 trainingloss 0.6931471805599453
iteration 2 batch 16680 trainingloss 0.6931471805599453
iteration 2 batch 16690 trainingloss 0.6931471805599453
iteration 2 batch 16700 trainingloss 0.6931471805599453
iteration 2 batch 16710 trainingloss 0.6931471805599453
iteration 2 batch 16720 trainingloss 0.6931471805599453
iteration 2 batch 16730 trainingloss 0.6931471805599453
iteration 2 batch 16740 trainingloss 0.6931471805599453
iteration 2 batch 16750 trainingloss 0.6931471805599453
iteration 2 batch 16760 trainingloss 0.6931471805599453
iteration 2 batch 16770 trainingloss 0.6931471805599453
iteration 2 batch 16780 trainingloss 0.6931471805599453
iteration 2 batch 16790 trainingloss 0.6931471805599453
iteration 2 batch 16800 trainingloss 0.6931471805599453
iteration 2 batch 16810 trainingloss 0.6931471805599453
iteration 2 batch 16820 trainingloss 0.6931471805599453
iteration 2 batch 16830 trainingloss 0.6931471805599453
iteration 2 batch 16840 trainingloss 0.6931471805599453
iteration 2 batch 16850 trainingloss 0.6931471805599453
iteration 2 batch 16860 trainingloss 0.6931471805599453
iteration 2 batch 16870 trainingloss 0.6931471805599453
iteration 2 batch 16880 trainingloss 0.6931471805599453
iteration 2 batch 16890 trainingloss 0.6931471805599453
iteration 2 batch 16900 trainingloss 0.6916632528527511
iteration 2 batch 16910 trainingloss 0.6931471805599453
iteration 2 batch 16920 trainingloss 0.6931471805599453
iteration 2 batch 16930 trainingloss 0.6931471805599453
iteration 2 batch 16940 trainingloss 0.6931471805599453
iteration 2 batch 16950 trainingloss 0.6931471805599453
iteration 2 batch 16960 trainingloss 0.6931471805599453
iteration 2 batch 16970 trainingloss 0.6931471805599453
iteration 2 batch 16980 trainingloss 0.6931471805599453
iteration 2 batch 16990 trainingloss 0.6916632528527511
iteration 2 batch 17000 trainingloss 0.6931471805599453
iteration 2 batch 17010 trainingloss 0.6931471805599453
iteration 2 batch 17020 trainingloss 0.6931471805599453
iteration 2 batch 17030 trainingloss 0.6931471805599453
iteration 2 batch 17040 trainingloss 0.6931471805599453
iteration 2 batch 17050 trainingloss 0.6916632528527511
iteration 2 batch 17060 trainingloss 0.6931471805599453
iteration 2 batch 17070 trainingloss 0.6931471805599453
iteration 2 batch 17080 trainingloss 0.6931471805599453
iteration 2 batch 17090 trainingloss 0.6931471805599453
iteration 2 batch 17100 trainingloss 0.6931471805599453
iteration 2 batch 17110 trainingloss 0.6916632528527511
iteration 2 batch 17120 trainingloss 0.6931471805599453
iteration 2 batch 17130 trainingloss 0.6931471805599453
iteration 2 batch 17140 trainingloss 0.6931471805599453
iteration 2 batch 17150 trainingloss 0.6931471805599453
iteration 2 batch 17160 trainingloss 0.6931471805599453
iteration 2 batch 17170 trainingloss 0.6931471805599453
iteration 2 batch 17180 trainingloss 0.6931471805599453
iteration 2 batch 17190 trainingloss 0.6931471805599453
iteration 2 batch 17200 trainingloss 0.6931471805599453
iteration 2 batch 17210 trainingloss 0.6931471805599453
iteration 2 batch 17220 trainingloss 0.6931471805599453
iteration 2 batch 17230 trainingloss 0.6931471805599453
iteration 2 batch 17240 trainingloss 0.6931471805599453
iteration 2 batch 17250 trainingloss 0.6916632528527511
iteration 2 batch 17260 trainingloss 0.6931471805599453
iteration 2 batch 17270 trainingloss 0.6931471805599453
iteration 2 batch 17280 trainingloss 0.6931471805599453
iteration 2 batch 17290 trainingloss 0.6931471805599453
iteration 2 batch 17300 trainingloss 0.6931471805599453
iteration 2 batch 17310 trainingloss 0.6931471805599453
iteration 2 batch 17320 trainingloss 0.6931471805599453
iteration 2 batch 17330 trainingloss 0.6916632528527511
iteration 2 batch 17340 trainingloss 0.6931471805599453
iteration 2 batch 17350 trainingloss 0.6931471805599453
iteration 2 batch 17360 trainingloss 0.6931471805599453
iteration 2 batch 17370 trainingloss 0.6931471805599453
iteration 2 batch 17380 trainingloss 0.6931471805599453
iteration 2 batch 17390 trainingloss 0.6931471805599453
iteration 2 batch 17400 trainingloss 0.6931471805599453
iteration 2 batch 17410 trainingloss 0.6931471805599453
iteration 2 batch 17420 trainingloss 0.6931471805599453
iteration 2 batch 17430 trainingloss 0.6931471805599453
iteration 2 batch 17440 trainingloss 0.6931471805599453
iteration 2 batch 17450 trainingloss 0.6916632528527511
iteration 2 batch 17460 trainingloss 0.6931471805599453
iteration 2 batch 17470 trainingloss 0.6931471805599453
iteration 2 batch 17480 trainingloss 0.6931471805599453
iteration 2 batch 17490 trainingloss 0.6931471805599453
iteration 2 batch 17500 trainingloss 0.6916632528527511
iteration 2 batch 17510 trainingloss 0.6916632528527511
iteration 2 batch 17520 trainingloss 0.6931471805599453
iteration 2 batch 17530 trainingloss 0.6931471805599453
iteration 2 batch 17540 trainingloss 0.6931471805599453
iteration 2 batch 17550 trainingloss 0.6931471805599453
iteration 2 batch 17560 trainingloss 0.6931471805599453
iteration 2 batch 17570 trainingloss 0.6931471805599453
iteration 2 batch 17580 trainingloss 0.6931471805599453
iteration 2 batch 17590 trainingloss 0.6931471805599453
iteration 2 batch 17600 trainingloss 0.6931471805599453
iteration 2 batch 17610 trainingloss 0.6931471805599453
iteration 2 batch 17620 trainingloss 0.6931471805599453
iteration 2 batch 17630 trainingloss 0.6931471805599453
iteration 2 batch 17640 trainingloss 0.6931471805599453
iteration 2 batch 17650 trainingloss 0.6931471805599453
iteration 2 batch 17660 trainingloss 0.6931471805599453
iteration 2 batch 17670 trainingloss 0.6931471805599453
iteration 2 batch 17680 trainingloss 0.6931471805599453
iteration 2 batch 17690 trainingloss 0.6931471805599453
iteration 2 batch 17700 trainingloss 0.6916632528527511
iteration 2 batch 17710 trainingloss 0.6931471805599453
iteration 2 batch 17720 trainingloss 0.6931471805599453
iteration 2 batch 17730 trainingloss 0.6916632528527511
iteration 2 batch 17740 trainingloss 0.6931471805599453
iteration 2 batch 17750 trainingloss 0.6931471805599453
iteration 2 batch 17760 trainingloss 0.6931471805599453
iteration 2 batch 17770 trainingloss 0.6931471805599453
iteration 2 batch 17780 trainingloss 0.6931471805599453
iteration 2 batch 17790 trainingloss 0.6931471805599453
iteration 2 batch 17800 trainingloss 0.6931471805599453
iteration 2 batch 17810 trainingloss 0.6931471805599453
iteration 2 batch 17820 trainingloss 0.6931471805599453
iteration 2 batch 17830 trainingloss 0.6916632528527511
iteration 2 batch 17840 trainingloss 0.6931471805599453
iteration 2 batch 17850 trainingloss 0.6931471805599453
iteration 2 batch 17860 trainingloss 0.6931471805599453
iteration 2 batch 17870 trainingloss 0.6931471805599453
iteration 2 batch 17880 trainingloss 0.6931471805599453
iteration 2 batch 17890 trainingloss 0.6931471805599453
iteration 2 batch 17900 trainingloss 0.6916632528527511
iteration 2 batch 17910 trainingloss 0.6931471805599453
iteration 2 batch 17920 trainingloss 0.6931471805599453
iteration 2 batch 17930 trainingloss 0.6931471805599453
iteration 2 batch 17940 trainingloss 0.6931471805599453
iteration 2 batch 17950 trainingloss 0.6931471805599453
iteration 2 batch 17960 trainingloss 0.6931471805599453
iteration 2 batch 17970 trainingloss 0.6931471805599453
iteration 2 batch 17980 trainingloss 0.6931471805599453
iteration 2 batch 17990 trainingloss 0.6916632528527511
iteration 2 batch 18000 trainingloss 0.6931471805599453
iteration 2 batch 18010 trainingloss 0.6931471805599453
iteration 2 batch 18020 trainingloss 0.6931471805599453
iteration 2 batch 18030 trainingloss 0.6931471805599453
iteration 2 batch 18040 trainingloss 0.6916632528527511
iteration 2 batch 18050 trainingloss 0.6931471805599453
iteration 2 batch 18060 trainingloss 0.6916632528527511
iteration 2 batch 18070 trainingloss 0.6931471805599453
iteration 2 batch 18080 trainingloss 0.6931471805599453
iteration 2 batch 18090 trainingloss 0.6931471805599453
iteration 2 batch 18100 trainingloss 0.6931471805599453
iteration 2 batch 18110 trainingloss 0.6931471805599453
iteration 2 batch 18120 trainingloss 0.6931471805599453
iteration 2 batch 18130 trainingloss 0.6931471805599453
iteration 2 batch 18140 trainingloss 0.6931471805599453
iteration 2 batch 18150 trainingloss 0.6916632528527511
iteration 2 batch 18160 trainingloss 0.6931471805599453
iteration 2 batch 18170 trainingloss 0.6931471805599453
iteration 2 batch 18180 trainingloss 0.6916632528527511
iteration 2 batch 18190 trainingloss 0.6931471805599453
iteration 2 batch 18200 trainingloss 0.6931471805599453
iteration 2 batch 18210 trainingloss 0.6931471805599453
iteration 2 batch 18220 trainingloss 0.6931471805599453
iteration 2 batch 18230 trainingloss 0.6931471805599453
iteration 2 batch 18240 trainingloss 0.6931471805599453
iteration 2 batch 18250 trainingloss 0.6931471805599453
iteration 2 batch 18260 trainingloss 0.6916632528527511
iteration 2 batch 18270 trainingloss 0.6931471805599453
iteration 2 batch 18280 trainingloss 0.6931471805599453
iteration 2 batch 18290 trainingloss 0.6931471805599453
iteration 2 batch 18300 trainingloss 0.6931471805599453
iteration 2 batch 18310 trainingloss 0.6931471805599453
iteration 2 batch 18320 trainingloss 0.6931471805599453
iteration 2 batch 18330 trainingloss 0.6931471805599453
iteration 2 batch 18340 trainingloss 0.6916632528527511
iteration 2 batch 18350 trainingloss 0.6931471805599453
iteration 2 batch 18360 trainingloss 0.6931471805599453
iteration 2 batch 18370 trainingloss 0.6931471805599453
iteration 2 batch 18380 trainingloss 0.6931471805599453
iteration 2 batch 18390 trainingloss 0.6901793251455568
iteration 2 batch 18400 trainingloss 0.6931471805599453
iteration 2 batch 18410 trainingloss 0.6931471805599453
iteration 2 batch 18420 trainingloss 0.6931471805599453
iteration 2 batch 18430 trainingloss 0.6931471805599453
iteration 2 batch 18440 trainingloss 0.6931471805599453
iteration 2 batch 18450 trainingloss 0.6931471805599453
iteration 2 batch 18460 trainingloss 0.6931471805599453
iteration 2 batch 18470 trainingloss 0.6931471805599453
iteration 2 batch 18480 trainingloss 0.6931471805599453
iteration 2 batch 18490 trainingloss 0.6931471805599453
iteration 2 batch 18500 trainingloss 0.6931471805599453
iteration 2 batch 18510 trainingloss 0.6931471805599453
iteration 2 batch 18520 trainingloss 0.6931471805599453
iteration 2 batch 18530 trainingloss 0.6931471805599453
iteration 2 batch 18540 trainingloss 0.6931471805599453
iteration 2 batch 18550 trainingloss 0.6916632528527511
iteration 2 batch 18560 trainingloss 0.6931471805599453
iteration 2 batch 18570 trainingloss 0.6931471805599453
iteration 2 batch 18580 trainingloss 0.6931471805599453
iteration 2 batch 18590 trainingloss 0.6916632528527511
iteration 2 batch 18600 trainingloss 0.6916632528527511
iteration 2 batch 18610 trainingloss 0.6931471805599453
iteration 3 batch 0 trainingloss 0.6931471805599453
iteration 3 batch 10 trainingloss 0.6931471805599453
iteration 3 batch 20 trainingloss 0.6931471805599453
iteration 3 batch 30 trainingloss 0.6931471805599453
iteration 3 batch 40 trainingloss 0.6931471805599453
iteration 3 batch 50 trainingloss 0.6916632528527511
iteration 3 batch 60 trainingloss 0.6931471805599453
iteration 3 batch 70 trainingloss 0.6931471805599453
iteration 3 batch 80 trainingloss 0.6931471805599453
iteration 3 batch 90 trainingloss 0.6931471805599453
iteration 3 batch 100 trainingloss 0.6931471805599453
iteration 3 batch 110 trainingloss 0.6931471805599453
iteration 3 batch 120 trainingloss 0.6931471805599453
iteration 3 batch 130 trainingloss 0.6931471805599453
iteration 3 batch 140 trainingloss 0.6931471805599453
iteration 3 batch 150 trainingloss 0.6931471805599453
iteration 3 batch 160 trainingloss 0.6931471805599453
iteration 3 batch 170 trainingloss 0.6931471805599453
iteration 3 batch 180 trainingloss 0.6931471805599453
iteration 3 batch 190 trainingloss 0.6931471805599453
iteration 3 batch 200 trainingloss 0.6931471805599453
iteration 3 batch 210 trainingloss 0.6931471805599453
iteration 3 batch 220 trainingloss 0.6931471805599453
iteration 3 batch 230 trainingloss 0.6931471805599453
iteration 3 batch 240 trainingloss 0.6931471805599453
iteration 3 batch 250 trainingloss 0.6931471805599453
iteration 3 batch 260 trainingloss 0.6931471805599453
iteration 3 batch 270 trainingloss 0.6931471805599453
iteration 3 batch 280 trainingloss 0.6931471805599453
iteration 3 batch 290 trainingloss 0.6931471805599453
iteration 3 batch 300 trainingloss 0.6931471805599453
iteration 3 batch 310 trainingloss 0.6931471805599453
iteration 3 batch 320 trainingloss 0.6931471805599453
iteration 3 batch 330 trainingloss 0.6931471805599453
iteration 3 batch 340 trainingloss 0.6931471805599453
iteration 3 batch 350 trainingloss 0.6931471805599453
iteration 3 batch 360 trainingloss 0.6931471805599453
iteration 3 batch 370 trainingloss 0.6931471805599453
iteration 3 batch 380 trainingloss 0.6931471805599453
iteration 3 batch 390 trainingloss 0.6931471805599453
iteration 3 batch 400 trainingloss 0.6916632528527511
iteration 3 batch 410 trainingloss 0.6931471805599453
iteration 3 batch 420 trainingloss 0.6931471805599453
iteration 3 batch 430 trainingloss 0.6916632528527511
iteration 3 batch 440 trainingloss 0.6931471805599453
iteration 3 batch 450 trainingloss 0.6916632528527511
iteration 3 batch 460 trainingloss 0.6931471805599453
iteration 3 batch 470 trainingloss 0.6931471805599453
iteration 3 batch 480 trainingloss 0.6931471805599453
iteration 3 batch 490 trainingloss 0.6931471805599453
iteration 3 batch 500 trainingloss 0.6931471805599453
iteration 3 batch 510 trainingloss 0.6931471805599453
iteration 3 batch 520 trainingloss 0.6931471805599453
iteration 3 batch 530 trainingloss 0.6931471805599453
iteration 3 batch 540 trainingloss 0.6931471805599453
iteration 3 batch 550 trainingloss 0.6931471805599453
iteration 3 batch 560 trainingloss 0.6931471805599453
iteration 3 batch 570 trainingloss 0.6931471805599453
iteration 3 batch 580 trainingloss 0.6931471805599453
iteration 3 batch 590 trainingloss 0.6931471805599453
iteration 3 batch 600 trainingloss 0.6931471805599453
iteration 3 batch 610 trainingloss 0.6931471805599453
iteration 3 batch 620 trainingloss 0.6931471805599453
iteration 3 batch 630 trainingloss 0.6931471805599453
iteration 3 batch 640 trainingloss 0.6931471805599453
iteration 3 batch 650 trainingloss 0.6931471805599453
iteration 3 batch 660 trainingloss 0.6931471805599453
iteration 3 batch 670 trainingloss 0.6916632528527511
iteration 3 batch 680 trainingloss 0.6931471805599453
iteration 3 batch 690 trainingloss 0.6916632528527511
iteration 3 batch 700 trainingloss 0.6931471805599453
iteration 3 batch 710 trainingloss 0.6931471805599453
iteration 3 batch 720 trainingloss 0.6931471805599453
iteration 3 batch 730 trainingloss 0.6931471805599453
iteration 3 batch 740 trainingloss 0.6931471805599453
iteration 3 batch 750 trainingloss 0.6931471805599453
iteration 3 batch 760 trainingloss 0.6931471805599453
iteration 3 batch 770 trainingloss 0.6931471805599453
iteration 3 batch 780 trainingloss 0.6916632528527511
iteration 3 batch 790 trainingloss 0.6931471805599453
iteration 3 batch 800 trainingloss 0.6931471805599453
iteration 3 batch 810 trainingloss 0.6931471805599453
iteration 3 batch 820 trainingloss 0.6916632528527511
iteration 3 batch 830 trainingloss 0.6916632528527511
iteration 3 batch 840 trainingloss 0.6931471805599453
iteration 3 batch 850 trainingloss 0.6931471805599453
iteration 3 batch 860 trainingloss 0.6916632528527511
iteration 3 batch 870 trainingloss 0.6931471805599453
iteration 3 batch 880 trainingloss 0.6916632528527511
iteration 3 batch 890 trainingloss 0.6931471805599453
iteration 3 batch 900 trainingloss 0.6931471805599453
iteration 3 batch 910 trainingloss 0.6931471805599453
iteration 3 batch 920 trainingloss 0.6931471805599453
iteration 3 batch 930 trainingloss 0.6931471805599453
iteration 3 batch 940 trainingloss 0.6931471805599453
iteration 3 batch 950 trainingloss 0.6931471805599453
iteration 3 batch 960 trainingloss 0.6931471805599453
iteration 3 batch 970 trainingloss 0.6931471805599453
iteration 3 batch 980 trainingloss 0.6916632528527511
iteration 3 batch 990 trainingloss 0.6931471805599453
iteration 3 batch 1000 trainingloss 0.6931471805599453
iteration 3 batch 1010 trainingloss 0.6931471805599453
iteration 3 batch 1020 trainingloss 0.6931471805599453
iteration 3 batch 1030 trainingloss 0.6931471805599453
iteration 3 batch 1040 trainingloss 0.6931471805599453
iteration 3 batch 1050 trainingloss 0.6931471805599453
iteration 3 batch 1060 trainingloss 0.6931471805599453
iteration 3 batch 1070 trainingloss 0.6916632528527511
iteration 3 batch 1080 trainingloss 0.6931471805599453
iteration 3 batch 1090 trainingloss 0.6931471805599453
iteration 3 batch 1100 trainingloss 0.6931471805599453
iteration 3 batch 1110 trainingloss 0.6931471805599453
iteration 3 batch 1120 trainingloss 0.6931471805599453
iteration 3 batch 1130 trainingloss 0.6931471805599453
iteration 3 batch 1140 trainingloss 0.6931471805599453
iteration 3 batch 1150 trainingloss 0.6931471805599453
iteration 3 batch 1160 trainingloss 0.6931471805599453
iteration 3 batch 1170 trainingloss 0.6931471805599453
iteration 3 batch 1180 trainingloss 0.6931471805599453
iteration 3 batch 1190 trainingloss 0.6931471805599453
iteration 3 batch 1200 trainingloss 0.6931471805599453
iteration 3 batch 1210 trainingloss 0.6931471805599453
iteration 3 batch 1220 trainingloss 0.6931471805599453
iteration 3 batch 1230 trainingloss 0.6916632528527511
iteration 3 batch 1240 trainingloss 0.6931471805599453
iteration 3 batch 1250 trainingloss 0.6931471805599453
iteration 3 batch 1260 trainingloss 0.6931471805599453
iteration 3 batch 1270 trainingloss 0.6931471805599453
iteration 3 batch 1280 trainingloss 0.6931471805599453
iteration 3 batch 1290 trainingloss 0.6931471805599453
iteration 3 batch 1300 trainingloss 0.6931471805599453
iteration 3 batch 1310 trainingloss 0.6931471805599453
iteration 3 batch 1320 trainingloss 0.6931471805599453
iteration 3 batch 1330 trainingloss 0.6931471805599453
iteration 3 batch 1340 trainingloss 0.6931471805599453
iteration 3 batch 1350 trainingloss 0.6931471805599453
iteration 3 batch 1360 trainingloss 0.6931471805599453
iteration 3 batch 1370 trainingloss 0.6931471805599453
iteration 3 batch 1380 trainingloss 0.6931471805599453
iteration 3 batch 1390 trainingloss 0.6931471805599453
iteration 3 batch 1400 trainingloss 0.6931471805599453
iteration 3 batch 1410 trainingloss 0.6931471805599453
iteration 3 batch 1420 trainingloss 0.6931471805599453
iteration 3 batch 1430 trainingloss 0.6931471805599453
iteration 3 batch 1440 trainingloss 0.6931471805599453
iteration 3 batch 1450 trainingloss 0.6901793251455568
iteration 3 batch 1460 trainingloss 0.6931471805599453
iteration 3 batch 1470 trainingloss 0.6931471805599453
iteration 3 batch 1480 trainingloss 0.6931471805599453
iteration 3 batch 1490 trainingloss 0.6916632528527511
iteration 3 batch 1500 trainingloss 0.6931471805599453
iteration 3 batch 1510 trainingloss 0.6931471805599453
iteration 3 batch 1520 trainingloss 0.6931471805599453
iteration 3 batch 1530 trainingloss 0.6931471805599453
iteration 3 batch 1540 trainingloss 0.6931471805599453
iteration 3 batch 1550 trainingloss 0.6931471805599453
iteration 3 batch 1560 trainingloss 0.6931471805599453
iteration 3 batch 1570 trainingloss 0.6931471805599453
iteration 3 batch 1580 trainingloss 0.6931471805599453
iteration 3 batch 1590 trainingloss 0.6931471805599453
iteration 3 batch 1600 trainingloss 0.6931471805599453
iteration 3 batch 1610 trainingloss 0.6931471805599453
iteration 3 batch 1620 trainingloss 0.6931471805599453
iteration 3 batch 1630 trainingloss 0.6916632528527511
iteration 3 batch 1640 trainingloss 0.6931471805599453
iteration 3 batch 1650 trainingloss 0.6931471805599453
iteration 3 batch 1660 trainingloss 0.6931471805599453
iteration 3 batch 1670 trainingloss 0.6916632528527511
iteration 3 batch 1680 trainingloss 0.6931471805599453
iteration 3 batch 1690 trainingloss 0.6931471805599453
iteration 3 batch 1700 trainingloss 0.6931471805599453
iteration 3 batch 1710 trainingloss 0.6931471805599453
iteration 3 batch 1720 trainingloss 0.6931471805599453
iteration 3 batch 1730 trainingloss 0.6931471805599453
iteration 3 batch 1740 trainingloss 0.6916632528527511
iteration 3 batch 1750 trainingloss 0.6931471805599453
iteration 3 batch 1760 trainingloss 0.6931471805599453
iteration 3 batch 1770 trainingloss 0.6931471805599453
iteration 3 batch 1780 trainingloss 0.6931471805599453
iteration 3 batch 1790 trainingloss 0.6931471805599453
iteration 3 batch 1800 trainingloss 0.6931471805599453
iteration 3 batch 1810 trainingloss 0.6931471805599453
iteration 3 batch 1820 trainingloss 0.6916632528527511
iteration 3 batch 1830 trainingloss 0.6931471805599453
iteration 3 batch 1840 trainingloss 0.6931471805599453
iteration 3 batch 1850 trainingloss 0.6886953974383626
iteration 3 batch 1860 trainingloss 0.6931471805599453
iteration 3 batch 1870 trainingloss 0.6931471805599453
iteration 3 batch 1880 trainingloss 0.6931471805599453
iteration 3 batch 1890 trainingloss 0.6931471805599453
iteration 3 batch 1900 trainingloss 0.6931471805599453
iteration 3 batch 1910 trainingloss 0.6931471805599453
iteration 3 batch 1920 trainingloss 0.6931471805599453
iteration 3 batch 1930 trainingloss 0.6931471805599453
iteration 3 batch 1940 trainingloss 0.6931471805599453
iteration 3 batch 1950 trainingloss 0.6931471805599453
iteration 3 batch 1960 trainingloss 0.6931471805599453
iteration 3 batch 1970 trainingloss 0.6931471805599453
iteration 3 batch 1980 trainingloss 0.6931471805599453
iteration 3 batch 1990 trainingloss 0.6931471805599453
iteration 3 batch 2000 trainingloss 0.6931471805599453
iteration 3 batch 2010 trainingloss 0.6931471805599453
iteration 3 batch 2020 trainingloss 0.6931471805599453
iteration 3 batch 2030 trainingloss 0.6931471805599453
iteration 3 batch 2040 trainingloss 0.6931471805599453
iteration 3 batch 2050 trainingloss 0.6931471805599453
iteration 3 batch 2060 trainingloss 0.6931471805599453
iteration 3 batch 2070 trainingloss 0.6931471805599453
iteration 3 batch 2080 trainingloss 0.6931471805599453
iteration 3 batch 2090 trainingloss 0.6931471805599453
iteration 3 batch 2100 trainingloss 0.6931471805599453
iteration 3 batch 2110 trainingloss 0.6931471805599453
iteration 3 batch 2120 trainingloss 0.6931471805599453
iteration 3 batch 2130 trainingloss 0.6931471805599453
iteration 3 batch 2140 trainingloss 0.6931471805599453
iteration 3 batch 2150 trainingloss 0.6931471805599453
iteration 3 batch 2160 trainingloss 0.6931471805599453
iteration 3 batch 2170 trainingloss 0.6931471805599453
iteration 3 batch 2180 trainingloss 0.6931471805599453
iteration 3 batch 2190 trainingloss 0.6931471805599453
iteration 3 batch 2200 trainingloss 0.6931471805599453
iteration 3 batch 2210 trainingloss 0.6931471805599453
iteration 3 batch 2220 trainingloss 0.6931471805599453
iteration 3 batch 2230 trainingloss 0.6931471805599453
iteration 3 batch 2240 trainingloss 0.6931471805599453
iteration 3 batch 2250 trainingloss 0.6931471805599453
iteration 3 batch 2260 trainingloss 0.6931471805599453
iteration 3 batch 2270 trainingloss 0.6931471805599453
iteration 3 batch 2280 trainingloss 0.6931471805599453
iteration 3 batch 2290 trainingloss 0.6931471805599453
iteration 3 batch 2300 trainingloss 0.6931471805599453
iteration 3 batch 2310 trainingloss 0.6931471805599453
iteration 3 batch 2320 trainingloss 0.6931471805599453
iteration 3 batch 2330 trainingloss 0.6931471805599453
iteration 3 batch 2340 trainingloss 0.6931471805599453
iteration 3 batch 2350 trainingloss 0.6931471805599453
iteration 3 batch 2360 trainingloss 0.6931471805599453
iteration 3 batch 2370 trainingloss 0.6931471805599453
iteration 3 batch 2380 trainingloss 0.6931471805599453
iteration 3 batch 2390 trainingloss 0.6931471805599453
iteration 3 batch 2400 trainingloss 0.6931471805599453
iteration 3 batch 2410 trainingloss 0.6931471805599453
iteration 3 batch 2420 trainingloss 0.6931471805599453
iteration 3 batch 2430 trainingloss 0.6931471805599453
iteration 3 batch 2440 trainingloss 0.6931471805599453
iteration 3 batch 2450 trainingloss 0.6931471805599453
iteration 3 batch 2460 trainingloss 0.6931471805599453
iteration 3 batch 2470 trainingloss 0.6931471805599453
iteration 3 batch 2480 trainingloss 0.6931471805599453
iteration 3 batch 2490 trainingloss 0.6931471805599453
iteration 3 batch 2500 trainingloss 0.6931471805599453
iteration 3 batch 2510 trainingloss 0.6931471805599453
iteration 3 batch 2520 trainingloss 0.6931471805599453
iteration 3 batch 2530 trainingloss 0.6931471805599453
iteration 3 batch 2540 trainingloss 0.6931471805599453
iteration 3 batch 2550 trainingloss 0.6931471805599453
iteration 3 batch 2560 trainingloss 0.6931471805599453
iteration 3 batch 2570 trainingloss 0.6931471805599453
iteration 3 batch 2580 trainingloss 0.6931471805599453
iteration 3 batch 2590 trainingloss 0.6931471805599453
iteration 3 batch 2600 trainingloss 0.6931471805599453
iteration 3 batch 2610 trainingloss 0.6931471805599453
iteration 3 batch 2620 trainingloss 0.6931471805599453
iteration 3 batch 2630 trainingloss 0.6931471805599453
iteration 3 batch 2640 trainingloss 0.6931471805599453
iteration 3 batch 2650 trainingloss 0.6931471805599453
iteration 3 batch 2660 trainingloss 0.6931471805599453
iteration 3 batch 2670 trainingloss 0.6931471805599453
iteration 3 batch 2680 trainingloss 0.6931471805599453
iteration 3 batch 2690 trainingloss 0.6931471805599453
iteration 3 batch 2700 trainingloss 0.6931471805599453
iteration 3 batch 2710 trainingloss 0.6931471805599453
iteration 3 batch 2720 trainingloss 0.6931471805599453
iteration 3 batch 2730 trainingloss 0.6916632528527511
iteration 3 batch 2740 trainingloss 0.6931471805599453
iteration 3 batch 2750 trainingloss 0.6931471805599453
iteration 3 batch 2760 trainingloss 0.6931471805599453
iteration 3 batch 2770 trainingloss 0.6931471805599453
iteration 3 batch 2780 trainingloss 0.6931471805599453
iteration 3 batch 2790 trainingloss 0.6916632528527511
iteration 3 batch 2800 trainingloss 0.6931471805599453
iteration 3 batch 2810 trainingloss 0.6931471805599453
iteration 3 batch 2820 trainingloss 0.6931471805599453
iteration 3 batch 2830 trainingloss 0.6931471805599453
iteration 3 batch 2840 trainingloss 0.6931471805599453
iteration 3 batch 2850 trainingloss 0.6931471805599453
iteration 3 batch 2860 trainingloss 0.6931471805599453
iteration 3 batch 2870 trainingloss 0.6931471805599453
iteration 3 batch 2880 trainingloss 0.6931471805599453
iteration 3 batch 2890 trainingloss 0.6931471805599453
iteration 3 batch 2900 trainingloss 0.6931471805599453
iteration 3 batch 2910 trainingloss 0.6931471805599453
iteration 3 batch 2920 trainingloss 0.6916632528527511
iteration 3 batch 2930 trainingloss 0.6931471805599453
iteration 3 batch 2940 trainingloss 0.6931471805599453
iteration 3 batch 2950 trainingloss 0.6931471805599453
iteration 3 batch 2960 trainingloss 0.6931471805599453
iteration 3 batch 2970 trainingloss 0.6931471805599453
iteration 3 batch 2980 trainingloss 0.6931471805599453
iteration 3 batch 2990 trainingloss 0.6931471805599453
iteration 3 batch 3000 trainingloss 0.6931471805599453
iteration 3 batch 3010 trainingloss 0.6931471805599453
iteration 3 batch 3020 trainingloss 0.6931471805599453
iteration 3 batch 3030 trainingloss 0.6931471805599453
iteration 3 batch 3040 trainingloss 0.6931471805599453
iteration 3 batch 3050 trainingloss 0.6931471805599453
iteration 3 batch 3060 trainingloss 0.6931471805599453
iteration 3 batch 3070 trainingloss 0.6931471805599453
iteration 3 batch 3080 trainingloss 0.6931471805599453
iteration 3 batch 3090 trainingloss 0.6931471805599453
iteration 3 batch 3100 trainingloss 0.6931471805599453
iteration 3 batch 3110 trainingloss 0.6931471805599453
iteration 3 batch 3120 trainingloss 0.6931471805599453
iteration 3 batch 3130 trainingloss 0.6931471805599453
iteration 3 batch 3140 trainingloss 0.6931471805599453
iteration 3 batch 3150 trainingloss 0.6931471805599453
iteration 3 batch 3160 trainingloss 0.6931471805599453
iteration 3 batch 3170 trainingloss 0.6931471805599453
iteration 3 batch 3180 trainingloss 0.6931471805599453
iteration 3 batch 3190 trainingloss 0.6931471805599453
iteration 3 batch 3200 trainingloss 0.6931471805599453
iteration 3 batch 3210 trainingloss 0.6931471805599453
iteration 3 batch 3220 trainingloss 0.6931471805599453
iteration 3 batch 3230 trainingloss 0.6931471805599453
iteration 3 batch 3240 trainingloss 0.6916632528527511
iteration 3 batch 3250 trainingloss 0.6931471805599453
iteration 3 batch 3260 trainingloss 0.6931471805599453
iteration 3 batch 3270 trainingloss 0.6931471805599453
iteration 3 batch 3280 trainingloss 0.6931471805599453
iteration 3 batch 3290 trainingloss 0.6931471805599453
iteration 3 batch 3300 trainingloss 0.6931471805599453
iteration 3 batch 3310 trainingloss 0.6931471805599453
iteration 3 batch 3320 trainingloss 0.6931471805599453
iteration 3 batch 3330 trainingloss 0.6931471805599453
iteration 3 batch 3340 trainingloss 0.6931471805599453
iteration 3 batch 3350 trainingloss 0.6931471805599453
iteration 3 batch 3360 trainingloss 0.6916632528527511
iteration 3 batch 3370 trainingloss 0.6931471805599453
iteration 3 batch 3380 trainingloss 0.6931471805599453
iteration 3 batch 3390 trainingloss 0.6931471805599453
iteration 3 batch 3400 trainingloss 0.6931471805599453
iteration 3 batch 3410 trainingloss 0.6931471805599453
iteration 3 batch 3420 trainingloss 0.6931471805599453
iteration 3 batch 3430 trainingloss 0.6931471805599453
iteration 3 batch 3440 trainingloss 0.6931471805599453
iteration 3 batch 3450 trainingloss 0.6931471805599453
iteration 3 batch 3460 trainingloss 0.6931471805599453
iteration 3 batch 3470 trainingloss 0.6931471805599453
iteration 3 batch 3480 trainingloss 0.6931471805599453
iteration 3 batch 3490 trainingloss 0.6931471805599453
iteration 3 batch 3500 trainingloss 0.6931471805599453
iteration 3 batch 3510 trainingloss 0.6931471805599453
iteration 3 batch 3520 trainingloss 0.6931471805599453
iteration 3 batch 3530 trainingloss 0.6931471805599453
iteration 3 batch 3540 trainingloss 0.6931471805599453
iteration 3 batch 3550 trainingloss 0.6931471805599453
iteration 3 batch 3560 trainingloss 0.6931471805599453
iteration 3 batch 3570 trainingloss 0.6931471805599453
iteration 3 batch 3580 trainingloss 0.6931471805599453
iteration 3 batch 3590 trainingloss 0.6916632528527511
iteration 3 batch 3600 trainingloss 0.6931471805599453
iteration 3 batch 3610 trainingloss 0.6931471805599453
iteration 3 batch 3620 trainingloss 0.6931471805599453
iteration 3 batch 3630 trainingloss 0.6916632528527511
iteration 3 batch 3640 trainingloss 0.6931471805599453
iteration 3 batch 3650 trainingloss 0.6931471805599453
iteration 3 batch 3660 trainingloss 0.6931471805599453
iteration 3 batch 3670 trainingloss 0.6931471805599453
iteration 3 batch 3680 trainingloss 0.6931471805599453
iteration 3 batch 3690 trainingloss 0.6931471805599453
iteration 3 batch 3700 trainingloss 0.6931471805599453
iteration 3 batch 3710 trainingloss 0.6916632528527511
iteration 3 batch 3720 trainingloss 0.6931471805599453
iteration 3 batch 3730 trainingloss 0.6931471805599453
iteration 3 batch 3740 trainingloss 0.6931471805599453
iteration 3 batch 3750 trainingloss 0.6931471805599453
iteration 3 batch 3760 trainingloss 0.6916632528527511
iteration 3 batch 3770 trainingloss 0.6931471805599453
iteration 3 batch 3780 trainingloss 0.6931471805599453
iteration 3 batch 3790 trainingloss 0.6916632528527511
iteration 3 batch 3800 trainingloss 0.6931471805599453
iteration 3 batch 3810 trainingloss 0.6931471805599453
iteration 3 batch 3820 trainingloss 0.6931471805599453
iteration 3 batch 3830 trainingloss 0.6931471805599453
iteration 3 batch 3840 trainingloss 0.6931471805599453
iteration 3 batch 3850 trainingloss 0.6931471805599453
iteration 3 batch 3860 trainingloss 0.6931471805599453
iteration 3 batch 3870 trainingloss 0.6931471805599453
iteration 3 batch 3880 trainingloss 0.6931471805599453
iteration 3 batch 3890 trainingloss 0.6931471805599453
iteration 3 batch 3900 trainingloss 0.6931471805599453
iteration 3 batch 3910 trainingloss 0.6931471805599453
iteration 3 batch 3920 trainingloss 0.6931471805599453
iteration 3 batch 3930 trainingloss 0.6931471805599453
iteration 3 batch 3940 trainingloss 0.6931471805599453
iteration 3 batch 3950 trainingloss 0.6931471805599453
iteration 3 batch 3960 trainingloss 0.6931471805599453
iteration 3 batch 3970 trainingloss 0.6931471805599453
iteration 3 batch 3980 trainingloss 0.6931471805599453
iteration 3 batch 3990 trainingloss 0.6931471805599453
iteration 3 batch 4000 trainingloss 0.6931471805599453
iteration 3 batch 4010 trainingloss 0.6931471805599453
iteration 3 batch 4020 trainingloss 0.6931471805599453
iteration 3 batch 4030 trainingloss 0.6931471805599453
iteration 3 batch 4040 trainingloss 0.6931471805599453
iteration 3 batch 4050 trainingloss 0.6931471805599453
iteration 3 batch 4060 trainingloss 0.6931471805599453
iteration 3 batch 4070 trainingloss 0.6931471805599453
iteration 3 batch 4080 trainingloss 0.6931471805599453
iteration 3 batch 4090 trainingloss 0.6931471805599453
iteration 3 batch 4100 trainingloss 0.6931471805599453
iteration 3 batch 4110 trainingloss 0.6931471805599453
iteration 3 batch 4120 trainingloss 0.6931471805599453
iteration 3 batch 4130 trainingloss 0.6931471805599453
iteration 3 batch 4140 trainingloss 0.6931471805599453
iteration 3 batch 4150 trainingloss 0.6916632528527511
iteration 3 batch 4160 trainingloss 0.6916632528527511
iteration 3 batch 4170 trainingloss 0.6931471805599453
iteration 3 batch 4180 trainingloss 0.6931471805599453
iteration 3 batch 4190 trainingloss 0.6931471805599453
iteration 3 batch 4200 trainingloss 0.6931471805599453
iteration 3 batch 4210 trainingloss 0.6931471805599453
iteration 3 batch 4220 trainingloss 0.6931471805599453
iteration 3 batch 4230 trainingloss 0.6931471805599453
iteration 3 batch 4240 trainingloss 0.6931471805599453
iteration 3 batch 4250 trainingloss 0.6931471805599453
iteration 3 batch 4260 trainingloss 0.6931471805599453
iteration 3 batch 4270 trainingloss 0.6931471805599453
iteration 3 batch 4280 trainingloss 0.6931471805599453
iteration 3 batch 4290 trainingloss 0.6916632528527511
iteration 3 batch 4300 trainingloss 0.6931471805599453
iteration 3 batch 4310 trainingloss 0.6931471805599453
iteration 3 batch 4320 trainingloss 0.6931471805599453
iteration 3 batch 4330 trainingloss 0.6931471805599453
iteration 3 batch 4340 trainingloss 0.6931471805599453
iteration 3 batch 4350 trainingloss 0.6931471805599453
iteration 3 batch 4360 trainingloss 0.6931471805599453
iteration 3 batch 4370 trainingloss 0.6931471805599453
iteration 3 batch 4380 trainingloss 0.6931471805599453
iteration 3 batch 4390 trainingloss 0.6931471805599453
iteration 3 batch 4400 trainingloss 0.6931471805599453
iteration 3 batch 4410 trainingloss 0.6931471805599453
iteration 3 batch 4420 trainingloss 0.6931471805599453
iteration 3 batch 4430 trainingloss 0.6931471805599453
iteration 3 batch 4440 trainingloss 0.6931471805599453
iteration 3 batch 4450 trainingloss 0.6931471805599453
iteration 3 batch 4460 trainingloss 0.6931471805599453
iteration 3 batch 4470 trainingloss 0.6931471805599453
iteration 3 batch 4480 trainingloss 0.6931471805599453
iteration 3 batch 4490 trainingloss 0.6931471805599453
iteration 3 batch 4500 trainingloss 0.6931471805599453
iteration 3 batch 4510 trainingloss 0.6931471805599453
iteration 3 batch 4520 trainingloss 0.6931471805599453
iteration 3 batch 4530 trainingloss 0.6931471805599453
iteration 3 batch 4540 trainingloss 0.6931471805599453
iteration 3 batch 4550 trainingloss 0.6916632528527511
iteration 3 batch 4560 trainingloss 0.6931471805599453
iteration 3 batch 4570 trainingloss 0.6931471805599453
iteration 3 batch 4580 trainingloss 0.6931471805599453
iteration 3 batch 4590 trainingloss 0.6931471805599453
iteration 3 batch 4600 trainingloss 0.6931471805599453
iteration 3 batch 4610 trainingloss 0.6916632528527511
iteration 3 batch 4620 trainingloss 0.6931471805599453
iteration 3 batch 4630 trainingloss 0.6931471805599453
iteration 3 batch 4640 trainingloss 0.6931471805599453
iteration 3 batch 4650 trainingloss 0.6931471805599453
iteration 3 batch 4660 trainingloss 0.6931471805599453
iteration 3 batch 4670 trainingloss 0.6931471805599453
iteration 3 batch 4680 trainingloss 0.6931471805599453
iteration 3 batch 4690 trainingloss 0.6931471805599453
iteration 3 batch 4700 trainingloss 0.6931471805599453
iteration 3 batch 4710 trainingloss 0.6931471805599453
iteration 3 batch 4720 trainingloss 0.6931471805599453
iteration 3 batch 4730 trainingloss 0.6931471805599453
iteration 3 batch 4740 trainingloss 0.6931471805599453
iteration 3 batch 4750 trainingloss 0.6931471805599453
iteration 3 batch 4760 trainingloss 0.6931471805599453
iteration 3 batch 4770 trainingloss 0.6931471805599453
iteration 3 batch 4780 trainingloss 0.6931471805599453
iteration 3 batch 4790 trainingloss 0.6916632528527511
iteration 3 batch 4800 trainingloss 0.6931471805599453
iteration 3 batch 4810 trainingloss 0.6931471805599453
iteration 3 batch 4820 trainingloss 0.6916632528527511
iteration 3 batch 4830 trainingloss 0.6931471805599453
iteration 3 batch 4840 trainingloss 0.6931471805599453
iteration 3 batch 4850 trainingloss 0.6931471805599453
iteration 3 batch 4860 trainingloss 0.6931471805599453
iteration 3 batch 4870 trainingloss 0.6931471805599453
iteration 3 batch 4880 trainingloss 0.6931471805599453
iteration 3 batch 4890 trainingloss 0.6931471805599453
iteration 3 batch 4900 trainingloss 0.6931471805599453
iteration 3 batch 4910 trainingloss 0.6931471805599453
iteration 3 batch 4920 trainingloss 0.6931471805599453
iteration 3 batch 4930 trainingloss 0.6916632528527511
iteration 3 batch 4940 trainingloss 0.6931471805599453
iteration 3 batch 4950 trainingloss 0.6931471805599453
iteration 3 batch 4960 trainingloss 0.6916632528527511
iteration 3 batch 4970 trainingloss 0.6931471805599453
iteration 3 batch 4980 trainingloss 0.6931471805599453
iteration 3 batch 4990 trainingloss 0.6916632528527511
iteration 3 batch 5000 trainingloss 0.6931471805599453
iteration 3 batch 5010 trainingloss 0.6931471805599453
iteration 3 batch 5020 trainingloss 0.6931471805599453
iteration 3 batch 5030 trainingloss 0.6931471805599453
iteration 3 batch 5040 trainingloss 0.6931471805599453
iteration 3 batch 5050 trainingloss 0.6916632528527511
iteration 3 batch 5060 trainingloss 0.6931471805599453
iteration 3 batch 5070 trainingloss 0.6931471805599453
iteration 3 batch 5080 trainingloss 0.6931471805599453
iteration 3 batch 5090 trainingloss 0.6916632528527511
iteration 3 batch 5100 trainingloss 0.6931471805599453
iteration 3 batch 5110 trainingloss 0.6931471805599453
iteration 3 batch 5120 trainingloss 0.6931471805599453
iteration 3 batch 5130 trainingloss 0.6931471805599453
iteration 3 batch 5140 trainingloss 0.6931471805599453
iteration 3 batch 5150 trainingloss 0.6931471805599453
iteration 3 batch 5160 trainingloss 0.6931471805599453
iteration 3 batch 5170 trainingloss 0.6931471805599453
iteration 3 batch 5180 trainingloss 0.6931471805599453
iteration 3 batch 5190 trainingloss 0.6931471805599453
iteration 3 batch 5200 trainingloss 0.6931471805599453
iteration 3 batch 5210 trainingloss 0.6916632528527511
iteration 3 batch 5220 trainingloss 0.6931471805599453
iteration 3 batch 5230 trainingloss 0.6931471805599453
iteration 3 batch 5240 trainingloss 0.6931471805599453
iteration 3 batch 5250 trainingloss 0.6916632528527511
iteration 3 batch 5260 trainingloss 0.6916632528527511
iteration 3 batch 5270 trainingloss 0.6931471805599453
iteration 3 batch 5280 trainingloss 0.6931471805599453
iteration 3 batch 5290 trainingloss 0.6931471805599453
iteration 3 batch 5300 trainingloss 0.6931471805599453
iteration 3 batch 5310 trainingloss 0.6931471805599453
iteration 3 batch 5320 trainingloss 0.6931471805599453
iteration 3 batch 5330 trainingloss 0.6931471805599453
iteration 3 batch 5340 trainingloss 0.6916632528527511
iteration 3 batch 5350 trainingloss 0.6931471805599453
iteration 3 batch 5360 trainingloss 0.6931471805599453
iteration 3 batch 5370 trainingloss 0.6931471805599453
iteration 3 batch 5380 trainingloss 0.6931471805599453
iteration 3 batch 5390 trainingloss 0.6931471805599453
iteration 3 batch 5400 trainingloss 0.6931471805599453
iteration 3 batch 5410 trainingloss 0.6931471805599453
iteration 3 batch 5420 trainingloss 0.6931471805599453
iteration 3 batch 5430 trainingloss 0.6931471805599453
iteration 3 batch 5440 trainingloss 0.6931471805599453
iteration 3 batch 5450 trainingloss 0.6931471805599453
iteration 3 batch 5460 trainingloss 0.6931471805599453
iteration 3 batch 5470 trainingloss 0.6931471805599453
iteration 3 batch 5480 trainingloss 0.6931471805599453
iteration 3 batch 5490 trainingloss 0.6916632528527511
iteration 3 batch 5500 trainingloss 0.6931471805599453
iteration 3 batch 5510 trainingloss 0.6931471805599453
iteration 3 batch 5520 trainingloss 0.6931471805599453
iteration 3 batch 5530 trainingloss 0.6931471805599453
iteration 3 batch 5540 trainingloss 0.6931471805599453
iteration 3 batch 5550 trainingloss 0.6931471805599453
iteration 3 batch 5560 trainingloss 0.6916632528527511
iteration 3 batch 5570 trainingloss 0.6931471805599453
iteration 3 batch 5580 trainingloss 0.6916632528527511
iteration 3 batch 5590 trainingloss 0.6931471805599453
iteration 3 batch 5600 trainingloss 0.6931471805599453
iteration 3 batch 5610 trainingloss 0.6931471805599453
iteration 3 batch 5620 trainingloss 0.6931471805599453
iteration 3 batch 5630 trainingloss 0.6931471805599453
iteration 3 batch 5640 trainingloss 0.6931471805599453
iteration 3 batch 5650 trainingloss 0.6916632528527511
iteration 3 batch 5660 trainingloss 0.6931471805599453
iteration 3 batch 5670 trainingloss 0.6931471805599453
iteration 3 batch 5680 trainingloss 0.6931471805599453
iteration 3 batch 5690 trainingloss 0.6931471805599453
iteration 3 batch 5700 trainingloss 0.6931471805599453
iteration 3 batch 5710 trainingloss 0.6931471805599453
iteration 3 batch 5720 trainingloss 0.6931471805599453
iteration 3 batch 5730 trainingloss 0.6931471805599453
iteration 3 batch 5740 trainingloss 0.6931471805599453
iteration 3 batch 5750 trainingloss 0.6931471805599453
iteration 3 batch 5760 trainingloss 0.6931471805599453
iteration 3 batch 5770 trainingloss 0.6931471805599453
iteration 3 batch 5780 trainingloss 0.6916632528527511
iteration 3 batch 5790 trainingloss 0.6931471805599453
iteration 3 batch 5800 trainingloss 0.6931471805599453
iteration 3 batch 5810 trainingloss 0.6931471805599453
iteration 3 batch 5820 trainingloss 0.6931471805599453
iteration 3 batch 5830 trainingloss 0.6931471805599453
iteration 3 batch 5840 trainingloss 0.6931471805599453
iteration 3 batch 5850 trainingloss 0.6931471805599453
iteration 3 batch 5860 trainingloss 0.6931471805599453
iteration 3 batch 5870 trainingloss 0.6931471805599453
iteration 3 batch 5880 trainingloss 0.6931471805599453
iteration 3 batch 5890 trainingloss 0.6931471805599453
iteration 3 batch 5900 trainingloss 0.6931471805599453
iteration 3 batch 5910 trainingloss 0.6931471805599453
iteration 3 batch 5920 trainingloss 0.6931471805599453
iteration 3 batch 5930 trainingloss 0.6931471805599453
iteration 3 batch 5940 trainingloss 0.6931471805599453
iteration 3 batch 5950 trainingloss 0.6931471805599453
iteration 3 batch 5960 trainingloss 0.6931471805599453
iteration 3 batch 5970 trainingloss 0.6931471805599453
iteration 3 batch 5980 trainingloss 0.6931471805599453
iteration 3 batch 5990 trainingloss 0.6931471805599453
iteration 3 batch 6000 trainingloss 0.6931471805599453
iteration 3 batch 6010 trainingloss 0.6931471805599453
iteration 3 batch 6020 trainingloss 0.6931471805599453
iteration 3 batch 6030 trainingloss 0.6931471805599453
iteration 3 batch 6040 trainingloss 0.6931471805599453
iteration 3 batch 6050 trainingloss 0.6931471805599453
iteration 3 batch 6060 trainingloss 0.6931471805599453
iteration 3 batch 6070 trainingloss 0.6931471805599453
iteration 3 batch 6080 trainingloss 0.6931471805599453
iteration 3 batch 6090 trainingloss 0.6916632528527511
iteration 3 batch 6100 trainingloss 0.6931471805599453
iteration 3 batch 6110 trainingloss 0.6931471805599453
iteration 3 batch 6120 trainingloss 0.6931471805599453
iteration 3 batch 6130 trainingloss 0.6931471805599453
iteration 3 batch 6140 trainingloss 0.6931471805599453
iteration 3 batch 6150 trainingloss 0.6931471805599453
iteration 3 batch 6160 trainingloss 0.6931471805599453
iteration 3 batch 6170 trainingloss 0.6916632528527511
iteration 3 batch 6180 trainingloss 0.6916632528527511
iteration 3 batch 6190 trainingloss 0.6931471805599453
iteration 3 batch 6200 trainingloss 0.6931471805599453
iteration 3 batch 6210 trainingloss 0.6931471805599453
iteration 3 batch 6220 trainingloss 0.6931471805599453
iteration 3 batch 6230 trainingloss 0.6916632528527511
iteration 3 batch 6240 trainingloss 0.6886953974383626
iteration 3 batch 6250 trainingloss 0.6931471805599453
iteration 3 batch 6260 trainingloss 0.6931471805599453
iteration 3 batch 6270 trainingloss 0.6931471805599453
iteration 3 batch 6280 trainingloss 0.6931471805599453
iteration 3 batch 6290 trainingloss 0.6901793251455568
iteration 3 batch 6300 trainingloss 0.6931471805599453
iteration 3 batch 6310 trainingloss 0.6931471805599453
iteration 3 batch 6320 trainingloss 0.6931471805599453
iteration 3 batch 6330 trainingloss 0.6931471805599453
iteration 3 batch 6340 trainingloss 0.6931471805599453
iteration 3 batch 6350 trainingloss 0.6931471805599453
iteration 3 batch 6360 trainingloss 0.6931471805599453
iteration 3 batch 6370 trainingloss 0.6916632528527511
iteration 3 batch 6380 trainingloss 0.6931471805599453
iteration 3 batch 6390 trainingloss 0.6931471805599453
iteration 3 batch 6400 trainingloss 0.6931471805599453
iteration 3 batch 6410 trainingloss 0.6916632528527511
iteration 3 batch 6420 trainingloss 0.6931471805599453
iteration 3 batch 6430 trainingloss 0.6931471805599453
iteration 3 batch 6440 trainingloss 0.6931471805599453
iteration 3 batch 6450 trainingloss 0.6931471805599453
iteration 3 batch 6460 trainingloss 0.6931471805599453
iteration 3 batch 6470 trainingloss 0.6931471805599453
iteration 3 batch 6480 trainingloss 0.6931471805599453
iteration 3 batch 6490 trainingloss 0.6931471805599453
iteration 3 batch 6500 trainingloss 0.6916632528527511
iteration 3 batch 6510 trainingloss 0.6931471805599453
iteration 3 batch 6520 trainingloss 0.6931471805599453
iteration 3 batch 6530 trainingloss 0.6931471805599453
iteration 3 batch 6540 trainingloss 0.6931471805599453
iteration 3 batch 6550 trainingloss 0.6931471805599453
iteration 3 batch 6560 trainingloss 0.6931471805599453
iteration 3 batch 6570 trainingloss 0.6931471805599453
iteration 3 batch 6580 trainingloss 0.6931471805599453
iteration 3 batch 6590 trainingloss 0.6931471805599453
iteration 3 batch 6600 trainingloss 0.6931471805599453
iteration 3 batch 6610 trainingloss 0.6931471805599453
iteration 3 batch 6620 trainingloss 0.6931471805599453
iteration 3 batch 6630 trainingloss 0.6931471805599453
iteration 3 batch 6640 trainingloss 0.6931471805599453
iteration 3 batch 6650 trainingloss 0.6931471805599453
iteration 3 batch 6660 trainingloss 0.6931471805599453
iteration 3 batch 6670 trainingloss 0.6931471805599453
iteration 3 batch 6680 trainingloss 0.6931471805599453
iteration 3 batch 6690 trainingloss 0.6931471805599453
iteration 3 batch 6700 trainingloss 0.6931471805599453
iteration 3 batch 6710 trainingloss 0.6931471805599453
iteration 3 batch 6720 trainingloss 0.6931471805599453
iteration 3 batch 6730 trainingloss 0.6931471805599453
iteration 3 batch 6740 trainingloss 0.6931471805599453
iteration 3 batch 6750 trainingloss 0.6931471805599453
iteration 3 batch 6760 trainingloss 0.6931471805599453
iteration 3 batch 6770 trainingloss 0.6931471805599453
iteration 3 batch 6780 trainingloss 0.6931471805599453
iteration 3 batch 6790 trainingloss 0.6931471805599453
iteration 3 batch 6800 trainingloss 0.6931471805599453
iteration 3 batch 6810 trainingloss 0.6931471805599453
iteration 3 batch 6820 trainingloss 0.6931471805599453
iteration 3 batch 6830 trainingloss 0.6931471805599453
iteration 3 batch 6840 trainingloss 0.6931471805599453
iteration 3 batch 6850 trainingloss 0.6931471805599453
iteration 3 batch 6860 trainingloss 0.6931471805599453
iteration 3 batch 6870 trainingloss 0.6931471805599453
iteration 3 batch 6880 trainingloss 0.6931471805599453
iteration 3 batch 6890 trainingloss 0.6931471805599453
iteration 3 batch 6900 trainingloss 0.6931471805599453
iteration 3 batch 6910 trainingloss 0.6931471805599453
iteration 3 batch 6920 trainingloss 0.6931471805599453
iteration 3 batch 6930 trainingloss 0.6931471805599453
iteration 3 batch 6940 trainingloss 0.6931471805599453
iteration 3 batch 6950 trainingloss 0.6931471805599453
iteration 3 batch 6960 trainingloss 0.6931471805599453
iteration 3 batch 6970 trainingloss 0.6931471805599453
iteration 3 batch 6980 trainingloss 0.6931471805599453
iteration 3 batch 6990 trainingloss 0.6931471805599453
iteration 3 batch 7000 trainingloss 0.6931471805599453
iteration 3 batch 7010 trainingloss 0.6931471805599453
iteration 3 batch 7020 trainingloss 0.6931471805599453
iteration 3 batch 7030 trainingloss 0.6931471805599453
iteration 3 batch 7040 trainingloss 0.6931471805599453
iteration 3 batch 7050 trainingloss 0.6931471805599453
iteration 3 batch 7060 trainingloss 0.6931471805599453
iteration 3 batch 7070 trainingloss 0.6931471805599453
iteration 3 batch 7080 trainingloss 0.6931471805599453
iteration 3 batch 7090 trainingloss 0.6931471805599453
iteration 3 batch 7100 trainingloss 0.6931471805599453
iteration 3 batch 7110 trainingloss 0.6931471805599453
iteration 3 batch 7120 trainingloss 0.6931471805599453
iteration 3 batch 7130 trainingloss 0.6931471805599453
iteration 3 batch 7140 trainingloss 0.6931471805599453
iteration 3 batch 7150 trainingloss 0.6931471805599453
iteration 3 batch 7160 trainingloss 0.6931471805599453
iteration 3 batch 7170 trainingloss 0.6931471805599453
iteration 3 batch 7180 trainingloss 0.6931471805599453
iteration 3 batch 7190 trainingloss 0.6931471805599453
iteration 3 batch 7200 trainingloss 0.6931471805599453
iteration 3 batch 7210 trainingloss 0.6931471805599453
iteration 3 batch 7220 trainingloss 0.6931471805599453
iteration 3 batch 7230 trainingloss 0.6931471805599453
iteration 3 batch 7240 trainingloss 0.6931471805599453
iteration 3 batch 7250 trainingloss 0.6931471805599453
iteration 3 batch 7260 trainingloss 0.6931471805599453
iteration 3 batch 7270 trainingloss 0.6931471805599453
iteration 3 batch 7280 trainingloss 0.6931471805599453
iteration 3 batch 7290 trainingloss 0.6931471805599453
iteration 3 batch 7300 trainingloss 0.6931471805599453
iteration 3 batch 7310 trainingloss 0.6931471805599453
iteration 3 batch 7320 trainingloss 0.6931471805599453
iteration 3 batch 7330 trainingloss 0.6931471805599453
iteration 3 batch 7340 trainingloss 0.6931471805599453
iteration 3 batch 7350 trainingloss 0.6931471805599453
iteration 3 batch 7360 trainingloss 0.6931471805599453
iteration 3 batch 7370 trainingloss 0.6901793251455568
iteration 3 batch 7380 trainingloss 0.6931471805599453
iteration 3 batch 7390 trainingloss 0.6931471805599453
iteration 3 batch 7400 trainingloss 0.6931471805599453
iteration 3 batch 7410 trainingloss 0.6931471805599453
iteration 3 batch 7420 trainingloss 0.6931471805599453
iteration 3 batch 7430 trainingloss 0.6931471805599453
iteration 3 batch 7440 trainingloss 0.6931471805599453
iteration 3 batch 7450 trainingloss 0.6931471805599453
iteration 3 batch 7460 trainingloss 0.6916632528527511
iteration 3 batch 7470 trainingloss 0.6931471805599453
iteration 3 batch 7480 trainingloss 0.6931471805599453
iteration 3 batch 7490 trainingloss 0.6931471805599453
iteration 3 batch 7500 trainingloss 0.6931471805599453
iteration 3 batch 7510 trainingloss 0.6931471805599453
iteration 3 batch 7520 trainingloss 0.6931471805599453
iteration 3 batch 7530 trainingloss 0.6931471805599453
iteration 3 batch 7540 trainingloss 0.6931471805599453
iteration 3 batch 7550 trainingloss 0.6931471805599453
iteration 3 batch 7560 trainingloss 0.6931471805599453
iteration 3 batch 7570 trainingloss 0.6931471805599453
iteration 3 batch 7580 trainingloss 0.6931471805599453
iteration 3 batch 7590 trainingloss 0.6931471805599453
iteration 3 batch 7600 trainingloss 0.6931471805599453
iteration 3 batch 7610 trainingloss 0.6931471805599453
iteration 3 batch 7620 trainingloss 0.6931471805599453
iteration 3 batch 7630 trainingloss 0.6931471805599453
iteration 3 batch 7640 trainingloss 0.6931471805599453
iteration 3 batch 7650 trainingloss 0.6931471805599453
iteration 3 batch 7660 trainingloss 0.6931471805599453
iteration 3 batch 7670 trainingloss 0.6931471805599453
iteration 3 batch 7680 trainingloss 0.6931471805599453
iteration 3 batch 7690 trainingloss 0.6916632528527511
iteration 3 batch 7700 trainingloss 0.6931471805599453
iteration 3 batch 7710 trainingloss 0.6931471805599453
iteration 3 batch 7720 trainingloss 0.6916632528527511
iteration 3 batch 7730 trainingloss 0.6931471805599453
iteration 3 batch 7740 trainingloss 0.6931471805599453
iteration 3 batch 7750 trainingloss 0.6931471805599453
iteration 3 batch 7760 trainingloss 0.6916632528527511
iteration 3 batch 7770 trainingloss 0.6931471805599453
iteration 3 batch 7780 trainingloss 0.6931471805599453
iteration 3 batch 7790 trainingloss 0.6931471805599453
iteration 3 batch 7800 trainingloss 0.6931471805599453
iteration 3 batch 7810 trainingloss 0.6931471805599453
iteration 3 batch 7820 trainingloss 0.6931471805599453
iteration 3 batch 7830 trainingloss 0.6916632528527511
iteration 3 batch 7840 trainingloss 0.6931471805599453
iteration 3 batch 7850 trainingloss 0.6931471805599453
iteration 3 batch 7860 trainingloss 0.6931471805599453
iteration 3 batch 7870 trainingloss 0.6931471805599453
iteration 3 batch 7880 trainingloss 0.6931471805599453
iteration 3 batch 7890 trainingloss 0.6931471805599453
iteration 3 batch 7900 trainingloss 0.6931471805599453
iteration 3 batch 7910 trainingloss 0.6931471805599453
iteration 3 batch 7920 trainingloss 0.6931471805599453
iteration 3 batch 7930 trainingloss 0.6931471805599453
iteration 3 batch 7940 trainingloss 0.6931471805599453
iteration 3 batch 7950 trainingloss 0.6931471805599453
iteration 3 batch 7960 trainingloss 0.6931471805599453
iteration 3 batch 7970 trainingloss 0.6931471805599453
iteration 3 batch 7980 trainingloss 0.6931471805599453
iteration 3 batch 7990 trainingloss 0.6931471805599453
iteration 3 batch 8000 trainingloss 0.6931471805599453
iteration 3 batch 8010 trainingloss 0.6931471805599453
iteration 3 batch 8020 trainingloss 0.6931471805599453
iteration 3 batch 8030 trainingloss 0.6931471805599453
iteration 3 batch 8040 trainingloss 0.6931471805599453
iteration 3 batch 8050 trainingloss 0.6931471805599453
iteration 3 batch 8060 trainingloss 0.6931471805599453
iteration 3 batch 8070 trainingloss 0.6931471805599453
iteration 3 batch 8080 trainingloss 0.6916632528527511
iteration 3 batch 8090 trainingloss 0.6931471805599453
iteration 3 batch 8100 trainingloss 0.6931471805599453
iteration 3 batch 8110 trainingloss 0.6931471805599453
iteration 3 batch 8120 trainingloss 0.6931471805599453
iteration 3 batch 8130 trainingloss 0.6931471805599453
iteration 3 batch 8140 trainingloss 0.6931471805599453
iteration 3 batch 8150 trainingloss 0.6931471805599453
iteration 3 batch 8160 trainingloss 0.6931471805599453
iteration 3 batch 8170 trainingloss 0.6931471805599453
iteration 3 batch 8180 trainingloss 0.6931471805599453
iteration 3 batch 8190 trainingloss 0.6931471805599453
iteration 3 batch 8200 trainingloss 0.6931471805599453
iteration 3 batch 8210 trainingloss 0.6931471805599453
iteration 3 batch 8220 trainingloss 0.6931471805599453
iteration 3 batch 8230 trainingloss 0.6931471805599453
iteration 3 batch 8240 trainingloss 0.6931471805599453
iteration 3 batch 8250 trainingloss 0.6931471805599453
iteration 3 batch 8260 trainingloss 0.6931471805599453
iteration 3 batch 8270 trainingloss 0.6931471805599453
iteration 3 batch 8280 trainingloss 0.6931471805599453
iteration 3 batch 8290 trainingloss 0.6916632528527511
iteration 3 batch 8300 trainingloss 0.6931471805599453
iteration 3 batch 8310 trainingloss 0.6931471805599453
iteration 3 batch 8320 trainingloss 0.6931471805599453
iteration 3 batch 8330 trainingloss 0.6931471805599453
iteration 3 batch 8340 trainingloss 0.6931471805599453
iteration 3 batch 8350 trainingloss 0.6931471805599453
iteration 3 batch 8360 trainingloss 0.6931471805599453
iteration 3 batch 8370 trainingloss 0.6931471805599453
iteration 3 batch 8380 trainingloss 0.6931471805599453
iteration 3 batch 8390 trainingloss 0.6931471805599453
iteration 3 batch 8400 trainingloss 0.6931471805599453
iteration 3 batch 8410 trainingloss 0.6931471805599453
iteration 3 batch 8420 trainingloss 0.6931471805599453
iteration 3 batch 8430 trainingloss 0.6931471805599453
iteration 3 batch 8440 trainingloss 0.6931471805599453
iteration 3 batch 8450 trainingloss 0.6931471805599453
iteration 3 batch 8460 trainingloss 0.6931471805599453
iteration 3 batch 8470 trainingloss 0.6931471805599453
iteration 3 batch 8480 trainingloss 0.6931471805599453
iteration 3 batch 8490 trainingloss 0.6931471805599453
iteration 3 batch 8500 trainingloss 0.6931471805599453
iteration 3 batch 8510 trainingloss 0.6931471805599453
iteration 3 batch 8520 trainingloss 0.6916632528527511
iteration 3 batch 8530 trainingloss 0.6931471805599453
iteration 3 batch 8540 trainingloss 0.6931471805599453
iteration 3 batch 8550 trainingloss 0.6931471805599453
iteration 3 batch 8560 trainingloss 0.6931471805599453
iteration 3 batch 8570 trainingloss 0.6931471805599453
iteration 3 batch 8580 trainingloss 0.6931471805599453
iteration 3 batch 8590 trainingloss 0.6931471805599453
iteration 3 batch 8600 trainingloss 0.6931471805599453
iteration 3 batch 8610 trainingloss 0.6916632528527511
iteration 3 batch 8620 trainingloss 0.6931471805599453
iteration 3 batch 8630 trainingloss 0.6931471805599453
iteration 3 batch 8640 trainingloss 0.6931471805599453
iteration 3 batch 8650 trainingloss 0.6931471805599453
iteration 3 batch 8660 trainingloss 0.6931471805599453
iteration 3 batch 8670 trainingloss 0.6931471805599453
iteration 3 batch 8680 trainingloss 0.6931471805599453
iteration 3 batch 8690 trainingloss 0.6931471805599453
iteration 3 batch 8700 trainingloss 0.6931471805599453
iteration 3 batch 8710 trainingloss 0.6931471805599453
iteration 3 batch 8720 trainingloss 0.6931471805599453
iteration 3 batch 8730 trainingloss 0.6931471805599453
iteration 3 batch 8740 trainingloss 0.6931471805599453
iteration 3 batch 8750 trainingloss 0.6931471805599453
iteration 3 batch 8760 trainingloss 0.6931471805599453
iteration 3 batch 8770 trainingloss 0.6931471805599453
iteration 3 batch 8780 trainingloss 0.6931471805599453
iteration 3 batch 8790 trainingloss 0.6931471805599453
iteration 3 batch 8800 trainingloss 0.6931471805599453
iteration 3 batch 8810 trainingloss 0.6931471805599453
iteration 3 batch 8820 trainingloss 0.6931471805599453
iteration 3 batch 8830 trainingloss 0.6931471805599453
iteration 3 batch 8840 trainingloss 0.6931471805599453
iteration 3 batch 8850 trainingloss 0.6931471805599453
iteration 3 batch 8860 trainingloss 0.6916632528527511
iteration 3 batch 8870 trainingloss 0.6931471805599453
iteration 3 batch 8880 trainingloss 0.6931471805599453
iteration 3 batch 8890 trainingloss 0.6931471805599453
iteration 3 batch 8900 trainingloss 0.6931471805599453
iteration 3 batch 8910 trainingloss 0.6931471805599453
iteration 3 batch 8920 trainingloss 0.6931471805599453
iteration 3 batch 8930 trainingloss 0.6931471805599453
iteration 3 batch 8940 trainingloss 0.6931471805599453
iteration 3 batch 8950 trainingloss 0.6931471805599453
iteration 3 batch 8960 trainingloss 0.6931471805599453
iteration 3 batch 8970 trainingloss 0.6931471805599453
iteration 3 batch 8980 trainingloss 0.6931471805599453
iteration 3 batch 8990 trainingloss 0.6931471805599453
iteration 3 batch 9000 trainingloss 0.6931471805599453
iteration 3 batch 9010 trainingloss 0.6931471805599453
iteration 3 batch 9020 trainingloss 0.6931471805599453
iteration 3 batch 9030 trainingloss 0.6931471805599453
iteration 3 batch 9040 trainingloss 0.6931471805599453
iteration 3 batch 9050 trainingloss 0.6931471805599453
iteration 3 batch 9060 trainingloss 0.6931471805599453
iteration 3 batch 9070 trainingloss 0.6931471805599453
iteration 3 batch 9080 trainingloss 0.6931471805599453
iteration 3 batch 9090 trainingloss 0.6931471805599453
iteration 3 batch 9100 trainingloss 0.6931471805599453
iteration 3 batch 9110 trainingloss 0.6931471805599453
iteration 3 batch 9120 trainingloss 0.6931471805599453
iteration 3 batch 9130 trainingloss 0.6931471805599453
iteration 3 batch 9140 trainingloss 0.6931471805599453
iteration 3 batch 9150 trainingloss 0.6931471805599453
iteration 3 batch 9160 trainingloss 0.6931471805599453
iteration 3 batch 9170 trainingloss 0.6931471805599453
iteration 3 batch 9180 trainingloss 0.6931471805599453
iteration 3 batch 9190 trainingloss 0.6916632528527511
iteration 3 batch 9200 trainingloss 0.6931471805599453
iteration 3 batch 9210 trainingloss 0.6931471805599453
iteration 3 batch 9220 trainingloss 0.6901793251455568
iteration 3 batch 9230 trainingloss 0.6916632528527511
iteration 3 batch 9240 trainingloss 0.6931471805599453
iteration 3 batch 9250 trainingloss 0.6931471805599453
iteration 3 batch 9260 trainingloss 0.6931471805599453
iteration 3 batch 9270 trainingloss 0.6931471805599453
iteration 3 batch 9280 trainingloss 0.6931471805599453
iteration 3 batch 9290 trainingloss 0.6931471805599453
iteration 3 batch 9300 trainingloss 0.6931471805599453
iteration 3 batch 9310 trainingloss 0.6931471805599453
iteration 3 batch 9320 trainingloss 0.6931471805599453
iteration 3 batch 9330 trainingloss 0.6931471805599453
iteration 3 batch 9340 trainingloss 0.6931471805599453
iteration 3 batch 9350 trainingloss 0.6931471805599453
iteration 3 batch 9360 trainingloss 0.6931471805599453
iteration 3 batch 9370 trainingloss 0.6916632528527511
iteration 3 batch 9380 trainingloss 0.6931471805599453
iteration 3 batch 9390 trainingloss 0.6931471805599453
iteration 3 batch 9400 trainingloss 0.6931471805599453
iteration 3 batch 9410 trainingloss 0.6931471805599453
iteration 3 batch 9420 trainingloss 0.6931471805599453
iteration 3 batch 9430 trainingloss 0.6931471805599453
iteration 3 batch 9440 trainingloss 0.6916632528527511
iteration 3 batch 9450 trainingloss 0.6931471805599453
iteration 3 batch 9460 trainingloss 0.6931471805599453
iteration 3 batch 9470 trainingloss 0.6931471805599453
iteration 3 batch 9480 trainingloss 0.6916632528527511
iteration 3 batch 9490 trainingloss 0.6931471805599453
iteration 3 batch 9500 trainingloss 0.6931471805599453
iteration 3 batch 9510 trainingloss 0.6931471805599453
iteration 3 batch 9520 trainingloss 0.6931471805599453
iteration 3 batch 9530 trainingloss 0.6931471805599453
iteration 3 batch 9540 trainingloss 0.6931471805599453
iteration 3 batch 9550 trainingloss 0.6916632528527511
iteration 3 batch 9560 trainingloss 0.6931471805599453
iteration 3 batch 9570 trainingloss 0.6916632528527511
iteration 3 batch 9580 trainingloss 0.6931471805599453
iteration 3 batch 9590 trainingloss 0.6931471805599453
iteration 3 batch 9600 trainingloss 0.6931471805599453
iteration 3 batch 9610 trainingloss 0.6931471805599453
iteration 3 batch 9620 trainingloss 0.6931471805599453
iteration 3 batch 9630 trainingloss 0.6931471805599453
iteration 3 batch 9640 trainingloss 0.6931471805599453
iteration 3 batch 9650 trainingloss 0.6931471805599453
iteration 3 batch 9660 trainingloss 0.6931471805599453
iteration 3 batch 9670 trainingloss 0.6931471805599453
iteration 3 batch 9680 trainingloss 0.6931471805599453
iteration 3 batch 9690 trainingloss 0.6916632528527511
iteration 3 batch 9700 trainingloss 0.6931471805599453
iteration 3 batch 9710 trainingloss 0.6931471805599453
iteration 3 batch 9720 trainingloss 0.6931471805599453
iteration 3 batch 9730 trainingloss 0.6931471805599453
iteration 3 batch 9740 trainingloss 0.6931471805599453
iteration 3 batch 9750 trainingloss 0.6931471805599453
iteration 3 batch 9760 trainingloss 0.6931471805599453
iteration 3 batch 9770 trainingloss 0.6931471805599453
iteration 3 batch 9780 trainingloss 0.6931471805599453
iteration 3 batch 9790 trainingloss 0.6931471805599453
iteration 3 batch 9800 trainingloss 0.6931471805599453
iteration 3 batch 9810 trainingloss 0.6931471805599453
iteration 3 batch 9820 trainingloss 0.6931471805599453
iteration 3 batch 9830 trainingloss 0.6931471805599453
iteration 3 batch 9840 trainingloss 0.6931471805599453
iteration 3 batch 9850 trainingloss 0.6931471805599453
iteration 3 batch 9860 trainingloss 0.6931471805599453
iteration 3 batch 9870 trainingloss 0.6931471805599453
iteration 3 batch 9880 trainingloss 0.6931471805599453
iteration 3 batch 9890 trainingloss 0.6931471805599453
iteration 3 batch 9900 trainingloss 0.6931471805599453
iteration 3 batch 9910 trainingloss 0.6931471805599453
iteration 3 batch 9920 trainingloss 0.6931471805599453
iteration 3 batch 9930 trainingloss 0.6931471805599453
iteration 3 batch 9940 trainingloss 0.6931471805599453
iteration 3 batch 9950 trainingloss 0.6931471805599453
iteration 3 batch 9960 trainingloss 0.6931471805599453
iteration 3 batch 9970 trainingloss 0.6916632528527511
iteration 3 batch 9980 trainingloss 0.6931471805599453
iteration 3 batch 9990 trainingloss 0.6931471805599453
iteration 3 batch 10000 trainingloss 0.6931471805599453
iteration 3 batch 10010 trainingloss 0.6931471805599453
iteration 3 batch 10020 trainingloss 0.6931471805599453
iteration 3 batch 10030 trainingloss 0.6931471805599453
iteration 3 batch 10040 trainingloss 0.6931471805599453
iteration 3 batch 10050 trainingloss 0.6931471805599453
iteration 3 batch 10060 trainingloss 0.6931471805599453
iteration 3 batch 10070 trainingloss 0.6931471805599453
iteration 3 batch 10080 trainingloss 0.6931471805599453
iteration 3 batch 10090 trainingloss 0.6931471805599453
iteration 3 batch 10100 trainingloss 0.6931471805599453
iteration 3 batch 10110 trainingloss 0.6931471805599453
iteration 3 batch 10120 trainingloss 0.6931471805599453
iteration 3 batch 10130 trainingloss 0.6931471805599453
iteration 3 batch 10140 trainingloss 0.6931471805599453
iteration 3 batch 10150 trainingloss 0.6931471805599453
iteration 3 batch 10160 trainingloss 0.6931471805599453
iteration 3 batch 10170 trainingloss 0.6931471805599453
iteration 3 batch 10180 trainingloss 0.6931471805599453
iteration 3 batch 10190 trainingloss 0.6931471805599453
iteration 3 batch 10200 trainingloss 0.6931471805599453
iteration 3 batch 10210 trainingloss 0.6931471805599453
iteration 3 batch 10220 trainingloss 0.6916632528527511
iteration 3 batch 10230 trainingloss 0.6916632528527511
iteration 3 batch 10240 trainingloss 0.6931471805599453
iteration 3 batch 10250 trainingloss 0.6931471805599453
iteration 3 batch 10260 trainingloss 0.6931471805599453
iteration 3 batch 10270 trainingloss 0.6931471805599453
iteration 3 batch 10280 trainingloss 0.6931471805599453
iteration 3 batch 10290 trainingloss 0.6916632528527511
iteration 3 batch 10300 trainingloss 0.6931471805599453
iteration 3 batch 10310 trainingloss 0.6931471805599453
iteration 3 batch 10320 trainingloss 0.6931471805599453
iteration 3 batch 10330 trainingloss 0.6931471805599453
iteration 3 batch 10340 trainingloss 0.6931471805599453
iteration 3 batch 10350 trainingloss 0.6916632528527511
iteration 3 batch 10360 trainingloss 0.6931471805599453
iteration 3 batch 10370 trainingloss 0.6931471805599453
iteration 3 batch 10380 trainingloss 0.6931471805599453
iteration 3 batch 10390 trainingloss 0.6931471805599453
iteration 3 batch 10400 trainingloss 0.6931471805599453
iteration 3 batch 10410 trainingloss 0.6931471805599453
iteration 3 batch 10420 trainingloss 0.6931471805599453
iteration 3 batch 10430 trainingloss 0.6931471805599453
iteration 3 batch 10440 trainingloss 0.6931471805599453
iteration 3 batch 10450 trainingloss 0.6931471805599453
iteration 3 batch 10460 trainingloss 0.6931471805599453
iteration 3 batch 10470 trainingloss 0.6931471805599453
iteration 3 batch 10480 trainingloss 0.6931471805599453
iteration 3 batch 10490 trainingloss 0.6931471805599453
iteration 3 batch 10500 trainingloss 0.6931471805599453
iteration 3 batch 10510 trainingloss 0.6931471805599453
iteration 3 batch 10520 trainingloss 0.6931471805599453
iteration 3 batch 10530 trainingloss 0.6931471805599453
iteration 3 batch 10540 trainingloss 0.6931471805599453
iteration 3 batch 10550 trainingloss 0.6931471805599453
iteration 3 batch 10560 trainingloss 0.6931471805599453
iteration 3 batch 10570 trainingloss 0.6931471805599453
iteration 3 batch 10580 trainingloss 0.6931471805599453
iteration 3 batch 10590 trainingloss 0.6931471805599453
iteration 3 batch 10600 trainingloss 0.6931471805599453
iteration 3 batch 10610 trainingloss 0.6931471805599453
iteration 3 batch 10620 trainingloss 0.6931471805599453
iteration 3 batch 10630 trainingloss 0.6931471805599453
iteration 3 batch 10640 trainingloss 0.6931471805599453
iteration 3 batch 10650 trainingloss 0.6931471805599453
iteration 3 batch 10660 trainingloss 0.6916632528527511
iteration 3 batch 10670 trainingloss 0.6931471805599453
iteration 3 batch 10680 trainingloss 0.6931471805599453
iteration 3 batch 10690 trainingloss 0.6931471805599453
iteration 3 batch 10700 trainingloss 0.6931471805599453
iteration 3 batch 10710 trainingloss 0.6931471805599453
iteration 3 batch 10720 trainingloss 0.6916632528527511
iteration 3 batch 10730 trainingloss 0.6931471805599453
iteration 3 batch 10740 trainingloss 0.6931471805599453
iteration 3 batch 10750 trainingloss 0.6931471805599453
iteration 3 batch 10760 trainingloss 0.6931471805599453
iteration 3 batch 10770 trainingloss 0.6931471805599453
iteration 3 batch 10780 trainingloss 0.6931471805599453
iteration 3 batch 10790 trainingloss 0.6931471805599453
iteration 3 batch 10800 trainingloss 0.6931471805599453
iteration 3 batch 10810 trainingloss 0.6931471805599453
iteration 3 batch 10820 trainingloss 0.6931471805599453
iteration 3 batch 10830 trainingloss 0.6931471805599453
iteration 3 batch 10840 trainingloss 0.6931471805599453
iteration 3 batch 10850 trainingloss 0.6931471805599453
iteration 3 batch 10860 trainingloss 0.6901793251455568
iteration 3 batch 10870 trainingloss 0.6931471805599453
iteration 3 batch 10880 trainingloss 0.6916632528527511
iteration 3 batch 10890 trainingloss 0.6931471805599453
iteration 3 batch 10900 trainingloss 0.6916632528527511
iteration 3 batch 10910 trainingloss 0.6916632528527511
iteration 3 batch 10920 trainingloss 0.6931471805599453
iteration 3 batch 10930 trainingloss 0.6931471805599453
iteration 3 batch 10940 trainingloss 0.6931471805599453
iteration 3 batch 10950 trainingloss 0.6931471805599453
iteration 3 batch 10960 trainingloss 0.6931471805599453
iteration 3 batch 10970 trainingloss 0.6931471805599453
iteration 3 batch 10980 trainingloss 0.6916632528527511
iteration 3 batch 10990 trainingloss 0.6931471805599453
iteration 3 batch 11000 trainingloss 0.6931471805599453
iteration 3 batch 11010 trainingloss 0.6931471805599453
iteration 3 batch 11020 trainingloss 0.6931471805599453
iteration 3 batch 11030 trainingloss 0.6931471805599453
iteration 3 batch 11040 trainingloss 0.6931471805599453
iteration 3 batch 11050 trainingloss 0.6931471805599453
iteration 3 batch 11060 trainingloss 0.6931471805599453
iteration 3 batch 11070 trainingloss 0.6931471805599453
iteration 3 batch 11080 trainingloss 0.6916632528527511
iteration 3 batch 11090 trainingloss 0.6931471805599453
iteration 3 batch 11100 trainingloss 0.6931471805599453
iteration 3 batch 11110 trainingloss 0.6931471805599453
iteration 3 batch 11120 trainingloss 0.6931471805599453
iteration 3 batch 11130 trainingloss 0.6931471805599453
iteration 3 batch 11140 trainingloss 0.6931471805599453
iteration 3 batch 11150 trainingloss 0.6931471805599453
iteration 3 batch 11160 trainingloss 0.6931471805599453
iteration 3 batch 11170 trainingloss 0.6931471805599453
iteration 3 batch 11180 trainingloss 0.6916632528527511
iteration 3 batch 11190 trainingloss 0.6931471805599453
iteration 3 batch 11200 trainingloss 0.6931471805599453
iteration 3 batch 11210 trainingloss 0.6931471805599453
iteration 3 batch 11220 trainingloss 0.6931471805599453
iteration 3 batch 11230 trainingloss 0.6931471805599453
iteration 3 batch 11240 trainingloss 0.6931471805599453
iteration 3 batch 11250 trainingloss 0.6931471805599453
iteration 3 batch 11260 trainingloss 0.6931471805599453
iteration 3 batch 11270 trainingloss 0.6931471805599453
iteration 3 batch 11280 trainingloss 0.6931471805599453
iteration 3 batch 11290 trainingloss 0.6931471805599453
iteration 3 batch 11300 trainingloss 0.6931471805599453
iteration 3 batch 11310 trainingloss 0.6931471805599453
iteration 3 batch 11320 trainingloss 0.6931471805599453
iteration 3 batch 11330 trainingloss 0.6931471805599453
iteration 3 batch 11340 trainingloss 0.6931471805599453
iteration 3 batch 11350 trainingloss 0.6931471805599453
iteration 3 batch 11360 trainingloss 0.6931471805599453
iteration 3 batch 11370 trainingloss 0.6931471805599453
iteration 3 batch 11380 trainingloss 0.6931471805599453
iteration 3 batch 11390 trainingloss 0.6916632528527511
iteration 3 batch 11400 trainingloss 0.6931471805599453
iteration 3 batch 11410 trainingloss 0.6916632528527511
iteration 3 batch 11420 trainingloss 0.6931471805599453
iteration 3 batch 11430 trainingloss 0.6931471805599453
iteration 3 batch 11440 trainingloss 0.6931471805599453
iteration 3 batch 11450 trainingloss 0.6931471805599453
iteration 3 batch 11460 trainingloss 0.6931471805599453
iteration 3 batch 11470 trainingloss 0.6931471805599453
iteration 3 batch 11480 trainingloss 0.6931471805599453
iteration 3 batch 11490 trainingloss 0.6931471805599453
iteration 3 batch 11500 trainingloss 0.6931471805599453
iteration 3 batch 11510 trainingloss 0.6931471805599453
iteration 3 batch 11520 trainingloss 0.6931471805599453
iteration 3 batch 11530 trainingloss 0.6931471805599453
iteration 3 batch 11540 trainingloss 0.6931471805599453
iteration 3 batch 11550 trainingloss 0.6931471805599453
iteration 3 batch 11560 trainingloss 0.6931471805599453
iteration 3 batch 11570 trainingloss 0.6931471805599453
iteration 3 batch 11580 trainingloss 0.6931471805599453
iteration 3 batch 11590 trainingloss 0.6931471805599453
iteration 3 batch 11600 trainingloss 0.6931471805599453
iteration 3 batch 11610 trainingloss 0.6931471805599453
iteration 3 batch 11620 trainingloss 0.6931471805599453
iteration 3 batch 11630 trainingloss 0.6931471805599453
iteration 3 batch 11640 trainingloss 0.6931471805599453
iteration 3 batch 11650 trainingloss 0.6916632528527511
iteration 3 batch 11660 trainingloss 0.6931471805599453
iteration 3 batch 11670 trainingloss 0.6931471805599453
iteration 3 batch 11680 trainingloss 0.6931471805599453
iteration 3 batch 11690 trainingloss 0.6931471805599453
iteration 3 batch 11700 trainingloss 0.6931471805599453
iteration 3 batch 11710 trainingloss 0.6931471805599453
iteration 3 batch 11720 trainingloss 0.6931471805599453
iteration 3 batch 11730 trainingloss 0.6916632528527511
iteration 3 batch 11740 trainingloss 0.6931471805599453
iteration 3 batch 11750 trainingloss 0.6916632528527511
iteration 3 batch 11760 trainingloss 0.6931471805599453
iteration 3 batch 11770 trainingloss 0.6931471805599453
iteration 3 batch 11780 trainingloss 0.6916632528527511
iteration 3 batch 11790 trainingloss 0.6931471805599453
iteration 3 batch 11800 trainingloss 0.6931471805599453
iteration 3 batch 11810 trainingloss 0.6931471805599453
iteration 3 batch 11820 trainingloss 0.6931471805599453
iteration 3 batch 11830 trainingloss 0.6931471805599453
iteration 3 batch 11840 trainingloss 0.6931471805599453
iteration 3 batch 11850 trainingloss 0.6931471805599453
iteration 3 batch 11860 trainingloss 0.6931471805599453
iteration 3 batch 11870 trainingloss 0.6931471805599453
iteration 3 batch 11880 trainingloss 0.6931471805599453
iteration 3 batch 11890 trainingloss 0.6931471805599453
iteration 3 batch 11900 trainingloss 0.6931471805599453
iteration 3 batch 11910 trainingloss 0.6931471805599453
iteration 3 batch 11920 trainingloss 0.6931471805599453
iteration 3 batch 11930 trainingloss 0.6931471805599453
iteration 3 batch 11940 trainingloss 0.6931471805599453
iteration 3 batch 11950 trainingloss 0.6931471805599453
iteration 3 batch 11960 trainingloss 0.6931471805599453
iteration 3 batch 11970 trainingloss 0.6916632528527511
iteration 3 batch 11980 trainingloss 0.6931471805599453
iteration 3 batch 11990 trainingloss 0.6931471805599453
iteration 3 batch 12000 trainingloss 0.6931471805599453
iteration 3 batch 12010 trainingloss 0.6916632528527511
iteration 3 batch 12020 trainingloss 0.6931471805599453
iteration 3 batch 12030 trainingloss 0.6931471805599453
iteration 3 batch 12040 trainingloss 0.6931471805599453
iteration 3 batch 12050 trainingloss 0.6931471805599453
iteration 3 batch 12060 trainingloss 0.6931471805599453
iteration 3 batch 12070 trainingloss 0.6931471805599453
iteration 3 batch 12080 trainingloss 0.6931471805599453
iteration 3 batch 12090 trainingloss 0.6931471805599453
iteration 3 batch 12100 trainingloss 0.6931471805599453
iteration 3 batch 12110 trainingloss 0.6931471805599453
iteration 3 batch 12120 trainingloss 0.6931471805599453
iteration 3 batch 12130 trainingloss 0.6931471805599453
iteration 3 batch 12140 trainingloss 0.6931471805599453
iteration 3 batch 12150 trainingloss 0.6931471805599453
iteration 3 batch 12160 trainingloss 0.6931471805599453
iteration 3 batch 12170 trainingloss 0.6931471805599453
iteration 3 batch 12180 trainingloss 0.6931471805599453
iteration 3 batch 12190 trainingloss 0.6931471805599453
iteration 3 batch 12200 trainingloss 0.6931471805599453
iteration 3 batch 12210 trainingloss 0.6931471805599453
iteration 3 batch 12220 trainingloss 0.6916632528527511
iteration 3 batch 12230 trainingloss 0.6931471805599453
iteration 3 batch 12240 trainingloss 0.6931471805599453
iteration 3 batch 12250 trainingloss 0.6931471805599453
iteration 3 batch 12260 trainingloss 0.6931471805599453
iteration 3 batch 12270 trainingloss 0.6931471805599453
iteration 3 batch 12280 trainingloss 0.6931471805599453
iteration 3 batch 12290 trainingloss 0.6931471805599453
iteration 3 batch 12300 trainingloss 0.6931471805599453
iteration 3 batch 12310 trainingloss 0.6931471805599453
iteration 3 batch 12320 trainingloss 0.6931471805599453
iteration 3 batch 12330 trainingloss 0.6931471805599453
iteration 3 batch 12340 trainingloss 0.6931471805599453
iteration 3 batch 12350 trainingloss 0.6931471805599453
iteration 3 batch 12360 trainingloss 0.6931471805599453
iteration 3 batch 12370 trainingloss 0.6931471805599453
iteration 3 batch 12380 trainingloss 0.6931471805599453
iteration 3 batch 12390 trainingloss 0.6931471805599453
iteration 3 batch 12400 trainingloss 0.6931471805599453
iteration 3 batch 12410 trainingloss 0.6916632528527511
iteration 3 batch 12420 trainingloss 0.6931471805599453
iteration 3 batch 12430 trainingloss 0.6931471805599453
iteration 3 batch 12440 trainingloss 0.6931471805599453
iteration 3 batch 12450 trainingloss 0.6931471805599453
iteration 3 batch 12460 trainingloss 0.6931471805599453
iteration 3 batch 12470 trainingloss 0.6931471805599453
iteration 3 batch 12480 trainingloss 0.6931471805599453
iteration 3 batch 12490 trainingloss 0.6916632528527511
iteration 3 batch 12500 trainingloss 0.6931471805599453
iteration 3 batch 12510 trainingloss 0.6931471805599453
iteration 3 batch 12520 trainingloss 0.6931471805599453
iteration 3 batch 12530 trainingloss 0.6931471805599453
iteration 3 batch 12540 trainingloss 0.6931471805599453
iteration 3 batch 12550 trainingloss 0.6931471805599453
iteration 3 batch 12560 trainingloss 0.6931471805599453
iteration 3 batch 12570 trainingloss 0.6931471805599453
iteration 3 batch 12580 trainingloss 0.6931471805599453
iteration 3 batch 12590 trainingloss 0.6931471805599453
iteration 3 batch 12600 trainingloss 0.6931471805599453
iteration 3 batch 12610 trainingloss 0.6931471805599453
iteration 3 batch 12620 trainingloss 0.6931471805599453
iteration 3 batch 12630 trainingloss 0.6931471805599453
iteration 3 batch 12640 trainingloss 0.6931471805599453
iteration 3 batch 12650 trainingloss 0.6931471805599453
iteration 3 batch 12660 trainingloss 0.6931471805599453
iteration 3 batch 12670 trainingloss 0.6931471805599453
iteration 3 batch 12680 trainingloss 0.6916632528527511
iteration 3 batch 12690 trainingloss 0.6931471805599453
iteration 3 batch 12700 trainingloss 0.6931471805599453
iteration 3 batch 12710 trainingloss 0.6931471805599453
iteration 3 batch 12720 trainingloss 0.6931471805599453
iteration 3 batch 12730 trainingloss 0.6931471805599453
iteration 3 batch 12740 trainingloss 0.6931471805599453
iteration 3 batch 12750 trainingloss 0.6931471805599453
iteration 3 batch 12760 trainingloss 0.6931471805599453
iteration 3 batch 12770 trainingloss 0.6931471805599453
iteration 3 batch 12780 trainingloss 0.6931471805599453
iteration 3 batch 12790 trainingloss 0.6931471805599453
iteration 3 batch 12800 trainingloss 0.6916632528527511
iteration 3 batch 12810 trainingloss 0.6931471805599453
iteration 3 batch 12820 trainingloss 0.6931471805599453
iteration 3 batch 12830 trainingloss 0.6931471805599453
iteration 3 batch 12840 trainingloss 0.6931471805599453
iteration 3 batch 12850 trainingloss 0.6931471805599453
iteration 3 batch 12860 trainingloss 0.6931471805599453
iteration 3 batch 12870 trainingloss 0.6931471805599453
iteration 3 batch 12880 trainingloss 0.6931471805599453
iteration 3 batch 12890 trainingloss 0.6931471805599453
iteration 3 batch 12900 trainingloss 0.6931471805599453
iteration 3 batch 12910 trainingloss 0.6931471805599453
iteration 3 batch 12920 trainingloss 0.6931471805599453
iteration 3 batch 12930 trainingloss 0.6931471805599453
iteration 3 batch 12940 trainingloss 0.6931471805599453
iteration 3 batch 12950 trainingloss 0.6916632528527511
iteration 3 batch 12960 trainingloss 0.6931471805599453
iteration 3 batch 12970 trainingloss 0.6931471805599453
iteration 3 batch 12980 trainingloss 0.6931471805599453
iteration 3 batch 12990 trainingloss 0.6931471805599453
iteration 3 batch 13000 trainingloss 0.6931471805599453
iteration 3 batch 13010 trainingloss 0.6931471805599453
iteration 3 batch 13020 trainingloss 0.6931471805599453
iteration 3 batch 13030 trainingloss 0.6931471805599453
iteration 3 batch 13040 trainingloss 0.6931471805599453
iteration 3 batch 13050 trainingloss 0.6931471805599453
iteration 3 batch 13060 trainingloss 0.6931471805599453
iteration 3 batch 13070 trainingloss 0.6931471805599453
iteration 3 batch 13080 trainingloss 0.6931471805599453
iteration 3 batch 13090 trainingloss 0.6931471805599453
iteration 3 batch 13100 trainingloss 0.6931471805599453
iteration 3 batch 13110 trainingloss 0.6931471805599453
iteration 3 batch 13120 trainingloss 0.6931471805599453
iteration 3 batch 13130 trainingloss 0.6931471805599453
iteration 3 batch 13140 trainingloss 0.6931471805599453
iteration 3 batch 13150 trainingloss 0.6931471805599453
iteration 3 batch 13160 trainingloss 0.6931471805599453
iteration 3 batch 13170 trainingloss 0.6931471805599453
iteration 3 batch 13180 trainingloss 0.6931471805599453
iteration 3 batch 13190 trainingloss 0.6931471805599453
iteration 3 batch 13200 trainingloss 0.6931471805599453
iteration 3 batch 13210 trainingloss 0.6901793251455568
iteration 3 batch 13220 trainingloss 0.6931471805599453
iteration 3 batch 13230 trainingloss 0.6931471805599453
iteration 3 batch 13240 trainingloss 0.6931471805599453
iteration 3 batch 13250 trainingloss 0.6931471805599453
iteration 3 batch 13260 trainingloss 0.6931471805599453
iteration 3 batch 13270 trainingloss 0.6931471805599453
iteration 3 batch 13280 trainingloss 0.6931471805599453
iteration 3 batch 13290 trainingloss 0.6931471805599453
iteration 3 batch 13300 trainingloss 0.6931471805599453
iteration 3 batch 13310 trainingloss 0.6931471805599453
iteration 3 batch 13320 trainingloss 0.6916632528527511
iteration 3 batch 13330 trainingloss 0.6931471805599453
iteration 3 batch 13340 trainingloss 0.6931471805599453
iteration 3 batch 13350 trainingloss 0.6931471805599453
iteration 3 batch 13360 trainingloss 0.6931471805599453
iteration 3 batch 13370 trainingloss 0.6931471805599453
iteration 3 batch 13380 trainingloss 0.6931471805599453
iteration 3 batch 13390 trainingloss 0.6931471805599453
iteration 3 batch 13400 trainingloss 0.6901793251455568
iteration 3 batch 13410 trainingloss 0.6931471805599453
iteration 3 batch 13420 trainingloss 0.6931471805599453
iteration 3 batch 13430 trainingloss 0.6931471805599453
iteration 3 batch 13440 trainingloss 0.6916632528527511
iteration 3 batch 13450 trainingloss 0.6931471805599453
iteration 3 batch 13460 trainingloss 0.6931471805599453
iteration 3 batch 13470 trainingloss 0.6931471805599453
iteration 3 batch 13480 trainingloss 0.6931471805599453
iteration 3 batch 13490 trainingloss 0.6931471805599453
iteration 3 batch 13500 trainingloss 0.6931471805599453
iteration 3 batch 13510 trainingloss 0.6931471805599453
iteration 3 batch 13520 trainingloss 0.6931471805599453
iteration 3 batch 13530 trainingloss 0.6931471805599453
iteration 3 batch 13540 trainingloss 0.6931471805599453
iteration 3 batch 13550 trainingloss 0.6931471805599453
iteration 3 batch 13560 trainingloss 0.6931471805599453
iteration 3 batch 13570 trainingloss 0.6931471805599453
iteration 3 batch 13580 trainingloss 0.6931471805599453
iteration 3 batch 13590 trainingloss 0.6931471805599453
iteration 3 batch 13600 trainingloss 0.6931471805599453
iteration 3 batch 13610 trainingloss 0.6916632528527511
iteration 3 batch 13620 trainingloss 0.6916632528527511
iteration 3 batch 13630 trainingloss 0.6931471805599453
iteration 3 batch 13640 trainingloss 0.6931471805599453
iteration 3 batch 13650 trainingloss 0.6931471805599453
iteration 3 batch 13660 trainingloss 0.6931471805599453
iteration 3 batch 13670 trainingloss 0.6931471805599453
iteration 3 batch 13680 trainingloss 0.6931471805599453
iteration 3 batch 13690 trainingloss 0.6931471805599453
iteration 3 batch 13700 trainingloss 0.6931471805599453
iteration 3 batch 13710 trainingloss 0.6931471805599453
iteration 3 batch 13720 trainingloss 0.6931471805599453
iteration 3 batch 13730 trainingloss 0.6931471805599453
iteration 3 batch 13740 trainingloss 0.6931471805599453
iteration 3 batch 13750 trainingloss 0.6916632528527511
iteration 3 batch 13760 trainingloss 0.6931471805599453
iteration 3 batch 13770 trainingloss 0.6931471805599453
iteration 3 batch 13780 trainingloss 0.6931471805599453
iteration 3 batch 13790 trainingloss 0.6931471805599453
iteration 3 batch 13800 trainingloss 0.6931471805599453
iteration 3 batch 13810 trainingloss 0.6931471805599453
iteration 3 batch 13820 trainingloss 0.6931471805599453
iteration 3 batch 13830 trainingloss 0.6916632528527511
iteration 3 batch 13840 trainingloss 0.6931471805599453
iteration 3 batch 13850 trainingloss 0.6916632528527511
iteration 3 batch 13860 trainingloss 0.6931471805599453
iteration 3 batch 13870 trainingloss 0.6916632528527511
iteration 3 batch 13880 trainingloss 0.6931471805599453
iteration 3 batch 13890 trainingloss 0.6931471805599453
iteration 3 batch 13900 trainingloss 0.6931471805599453
iteration 3 batch 13910 trainingloss 0.6931471805599453
iteration 3 batch 13920 trainingloss 0.6931471805599453
iteration 3 batch 13930 trainingloss 0.6916632528527511
iteration 3 batch 13940 trainingloss 0.6931471805599453
iteration 3 batch 13950 trainingloss 0.6931471805599453
iteration 3 batch 13960 trainingloss 0.6931471805599453
iteration 3 batch 13970 trainingloss 0.6931471805599453
iteration 3 batch 13980 trainingloss 0.6931471805599453
iteration 3 batch 13990 trainingloss 0.6931471805599453
iteration 3 batch 14000 trainingloss 0.6931471805599453
iteration 3 batch 14010 trainingloss 0.6931471805599453
iteration 3 batch 14020 trainingloss 0.6931471805599453
iteration 3 batch 14030 trainingloss 0.6931471805599453
iteration 3 batch 14040 trainingloss 0.6931471805599453
iteration 3 batch 14050 trainingloss 0.6916632528527511
iteration 3 batch 14060 trainingloss 0.6916632528527511
iteration 3 batch 14070 trainingloss 0.6931471805599453
iteration 3 batch 14080 trainingloss 0.6931471805599453
iteration 3 batch 14090 trainingloss 0.6931471805599453
iteration 3 batch 14100 trainingloss 0.6931471805599453
iteration 3 batch 14110 trainingloss 0.6931471805599453
iteration 3 batch 14120 trainingloss 0.6931471805599453
iteration 3 batch 14130 trainingloss 0.6931471805599453
iteration 3 batch 14140 trainingloss 0.6931471805599453
iteration 3 batch 14150 trainingloss 0.6916632528527511
iteration 3 batch 14160 trainingloss 0.6931471805599453
iteration 3 batch 14170 trainingloss 0.6931471805599453
iteration 3 batch 14180 trainingloss 0.6931471805599453
iteration 3 batch 14190 trainingloss 0.6931471805599453
iteration 3 batch 14200 trainingloss 0.6931471805599453
iteration 3 batch 14210 trainingloss 0.6931471805599453
iteration 3 batch 14220 trainingloss 0.6931471805599453
iteration 3 batch 14230 trainingloss 0.6931471805599453
iteration 3 batch 14240 trainingloss 0.6931471805599453
iteration 3 batch 14250 trainingloss 0.6931471805599453
iteration 3 batch 14260 trainingloss 0.6931471805599453
iteration 3 batch 14270 trainingloss 0.6931471805599453
iteration 3 batch 14280 trainingloss 0.6931471805599453
iteration 3 batch 14290 trainingloss 0.6931471805599453
iteration 3 batch 14300 trainingloss 0.6931471805599453
iteration 3 batch 14310 trainingloss 0.6931471805599453
iteration 3 batch 14320 trainingloss 0.6931471805599453
iteration 3 batch 14330 trainingloss 0.6931471805599453
iteration 3 batch 14340 trainingloss 0.6931471805599453
iteration 3 batch 14350 trainingloss 0.6916632528527511
iteration 3 batch 14360 trainingloss 0.6931471805599453
iteration 3 batch 14370 trainingloss 0.6931471805599453
iteration 3 batch 14380 trainingloss 0.6931471805599453
iteration 3 batch 14390 trainingloss 0.6931471805599453
iteration 3 batch 14400 trainingloss 0.6931471805599453
iteration 3 batch 14410 trainingloss 0.6931471805599453
iteration 3 batch 14420 trainingloss 0.6931471805599453
iteration 3 batch 14430 trainingloss 0.6931471805599453
iteration 3 batch 14440 trainingloss 0.6931471805599453
iteration 3 batch 14450 trainingloss 0.6931471805599453
iteration 3 batch 14460 trainingloss 0.6931471805599453
iteration 3 batch 14470 trainingloss 0.6931471805599453
iteration 3 batch 14480 trainingloss 0.6931471805599453
iteration 3 batch 14490 trainingloss 0.6931471805599453
iteration 3 batch 14500 trainingloss 0.6931471805599453
iteration 3 batch 14510 trainingloss 0.6931471805599453
iteration 3 batch 14520 trainingloss 0.6931471805599453
iteration 3 batch 14530 trainingloss 0.6931471805599453
iteration 3 batch 14540 trainingloss 0.6931471805599453
iteration 3 batch 14550 trainingloss 0.6931471805599453
iteration 3 batch 14560 trainingloss 0.6931471805599453
iteration 3 batch 14570 trainingloss 0.6931471805599453
iteration 3 batch 14580 trainingloss 0.6916632528527511
iteration 3 batch 14590 trainingloss 0.6931471805599453
iteration 3 batch 14600 trainingloss 0.6931471805599453
iteration 3 batch 14610 trainingloss 0.6931471805599453
iteration 3 batch 14620 trainingloss 0.6931471805599453
iteration 3 batch 14630 trainingloss 0.6916632528527511
iteration 3 batch 14640 trainingloss 0.6931471805599453
iteration 3 batch 14650 trainingloss 0.6931471805599453
iteration 3 batch 14660 trainingloss 0.6931471805599453
iteration 3 batch 14670 trainingloss 0.6931471805599453
iteration 3 batch 14680 trainingloss 0.6931471805599453
iteration 3 batch 14690 trainingloss 0.6931471805599453
iteration 3 batch 14700 trainingloss 0.6931471805599453
iteration 3 batch 14710 trainingloss 0.6931471805599453
iteration 3 batch 14720 trainingloss 0.6931471805599453
iteration 3 batch 14730 trainingloss 0.6931471805599453
iteration 3 batch 14740 trainingloss 0.6931471805599453
iteration 3 batch 14750 trainingloss 0.6931471805599453
iteration 3 batch 14760 trainingloss 0.6931471805599453
iteration 3 batch 14770 trainingloss 0.6931471805599453
iteration 3 batch 14780 trainingloss 0.6931471805599453
iteration 3 batch 14790 trainingloss 0.6931471805599453
iteration 3 batch 14800 trainingloss 0.6931471805599453
iteration 3 batch 14810 trainingloss 0.6931471805599453
iteration 3 batch 14820 trainingloss 0.6931471805599453
iteration 3 batch 14830 trainingloss 0.6931471805599453
iteration 3 batch 14840 trainingloss 0.6931471805599453
iteration 3 batch 14850 trainingloss 0.6931471805599453
iteration 3 batch 14860 trainingloss 0.6931471805599453
iteration 3 batch 14870 trainingloss 0.6931471805599453
iteration 3 batch 14880 trainingloss 0.6931471805599453
iteration 3 batch 14890 trainingloss 0.6931471805599453
iteration 3 batch 14900 trainingloss 0.6931471805599453
iteration 3 batch 14910 trainingloss 0.6931471805599453
iteration 3 batch 14920 trainingloss 0.6931471805599453
iteration 3 batch 14930 trainingloss 0.6916632528527511
iteration 3 batch 14940 trainingloss 0.6931471805599453
iteration 3 batch 14950 trainingloss 0.6931471805599453
iteration 3 batch 14960 trainingloss 0.6931471805599453
iteration 3 batch 14970 trainingloss 0.6931471805599453
iteration 3 batch 14980 trainingloss 0.6916632528527511
iteration 3 batch 14990 trainingloss 0.6931471805599453
iteration 3 batch 15000 trainingloss 0.6931471805599453
iteration 3 batch 15010 trainingloss 0.6931471805599453
iteration 3 batch 15020 trainingloss 0.6931471805599453
iteration 3 batch 15030 trainingloss 0.6931471805599453
iteration 3 batch 15040 trainingloss 0.6931471805599453
iteration 3 batch 15050 trainingloss 0.6931471805599453
iteration 3 batch 15060 trainingloss 0.6931471805599453
iteration 3 batch 15070 trainingloss 0.6931471805599453
iteration 3 batch 15080 trainingloss 0.6931471805599453
iteration 3 batch 15090 trainingloss 0.6931471805599453
iteration 3 batch 15100 trainingloss 0.6916632528527511
iteration 3 batch 15110 trainingloss 0.6931471805599453
iteration 3 batch 15120 trainingloss 0.6931471805599453
iteration 3 batch 15130 trainingloss 0.6931471805599453
iteration 3 batch 15140 trainingloss 0.6931471805599453
iteration 3 batch 15150 trainingloss 0.6916632528527511
iteration 3 batch 15160 trainingloss 0.6931471805599453
iteration 3 batch 15170 trainingloss 0.6931471805599453
iteration 3 batch 15180 trainingloss 0.6931471805599453
iteration 3 batch 15190 trainingloss 0.6931471805599453
iteration 3 batch 15200 trainingloss 0.6931471805599453
iteration 3 batch 15210 trainingloss 0.6931471805599453
iteration 3 batch 15220 trainingloss 0.6931471805599453
iteration 3 batch 15230 trainingloss 0.6931471805599453
iteration 3 batch 15240 trainingloss 0.6931471805599453
iteration 3 batch 15250 trainingloss 0.6931471805599453
iteration 3 batch 15260 trainingloss 0.6931471805599453
iteration 3 batch 15270 trainingloss 0.6931471805599453
iteration 3 batch 15280 trainingloss 0.6931471805599453
iteration 3 batch 15290 trainingloss 0.6931471805599453
iteration 3 batch 15300 trainingloss 0.6931471805599453
iteration 3 batch 15310 trainingloss 0.6931471805599453
iteration 3 batch 15320 trainingloss 0.6931471805599453
iteration 3 batch 15330 trainingloss 0.6931471805599453
iteration 3 batch 15340 trainingloss 0.6931471805599453
iteration 3 batch 15350 trainingloss 0.6931471805599453
iteration 3 batch 15360 trainingloss 0.6931471805599453
iteration 3 batch 15370 trainingloss 0.6916632528527511
iteration 3 batch 15380 trainingloss 0.6931471805599453
iteration 3 batch 15390 trainingloss 0.6916632528527511
iteration 3 batch 15400 trainingloss 0.6931471805599453
iteration 3 batch 15410 trainingloss 0.6931471805599453
iteration 3 batch 15420 trainingloss 0.6931471805599453
iteration 3 batch 15430 trainingloss 0.6931471805599453
iteration 3 batch 15440 trainingloss 0.6931471805599453
iteration 3 batch 15450 trainingloss 0.6931471805599453
iteration 3 batch 15460 trainingloss 0.6931471805599453
iteration 3 batch 15470 trainingloss 0.6931471805599453
iteration 3 batch 15480 trainingloss 0.6931471805599453
iteration 3 batch 15490 trainingloss 0.6916632528527511
iteration 3 batch 15500 trainingloss 0.6931471805599453
iteration 3 batch 15510 trainingloss 0.6931471805599453
iteration 3 batch 15520 trainingloss 0.6931471805599453
iteration 3 batch 15530 trainingloss 0.6931471805599453
iteration 3 batch 15540 trainingloss 0.6916632528527511
iteration 3 batch 15550 trainingloss 0.6931471805599453
iteration 3 batch 15560 trainingloss 0.6931471805599453
iteration 3 batch 15570 trainingloss 0.6931471805599453
iteration 3 batch 15580 trainingloss 0.6931471805599453
iteration 3 batch 15590 trainingloss 0.6931471805599453
iteration 3 batch 15600 trainingloss 0.6931471805599453
iteration 3 batch 15610 trainingloss 0.6931471805599453
iteration 3 batch 15620 trainingloss 0.6931471805599453
iteration 3 batch 15630 trainingloss 0.6931471805599453
iteration 3 batch 15640 trainingloss 0.6931471805599453
iteration 3 batch 15650 trainingloss 0.6931471805599453
iteration 3 batch 15660 trainingloss 0.6931471805599453
iteration 3 batch 15670 trainingloss 0.6916632528527511
iteration 3 batch 15680 trainingloss 0.6931471805599453
iteration 3 batch 15690 trainingloss 0.6931471805599453
iteration 3 batch 15700 trainingloss 0.6931471805599453
iteration 3 batch 15710 trainingloss 0.6931471805599453
iteration 3 batch 15720 trainingloss 0.6931471805599453
iteration 3 batch 15730 trainingloss 0.6931471805599453
iteration 3 batch 15740 trainingloss 0.6931471805599453
iteration 3 batch 15750 trainingloss 0.6931471805599453
iteration 3 batch 15760 trainingloss 0.6931471805599453
iteration 3 batch 15770 trainingloss 0.6931471805599453
iteration 3 batch 15780 trainingloss 0.6931471805599453
iteration 3 batch 15790 trainingloss 0.6931471805599453
iteration 3 batch 15800 trainingloss 0.6931471805599453
iteration 3 batch 15810 trainingloss 0.6931471805599453
iteration 3 batch 15820 trainingloss 0.6931471805599453
iteration 3 batch 15830 trainingloss 0.6931471805599453
iteration 3 batch 15840 trainingloss 0.6931471805599453
iteration 3 batch 15850 trainingloss 0.6931471805599453
iteration 3 batch 15860 trainingloss 0.6931471805599453
iteration 3 batch 15870 trainingloss 0.6931471805599453
iteration 3 batch 15880 trainingloss 0.6931471805599453
iteration 3 batch 15890 trainingloss 0.6931471805599453
iteration 3 batch 15900 trainingloss 0.6931471805599453
iteration 3 batch 15910 trainingloss 0.6931471805599453
iteration 3 batch 15920 trainingloss 0.6931471805599453
iteration 3 batch 15930 trainingloss 0.6931471805599453
iteration 3 batch 15940 trainingloss 0.6931471805599453
iteration 3 batch 15950 trainingloss 0.6931471805599453
iteration 3 batch 15960 trainingloss 0.6931471805599453
iteration 3 batch 15970 trainingloss 0.6931471805599453
iteration 3 batch 15980 trainingloss 0.6931471805599453
iteration 3 batch 15990 trainingloss 0.6931471805599453
iteration 3 batch 16000 trainingloss 0.6931471805599453
iteration 3 batch 16010 trainingloss 0.6931471805599453
iteration 3 batch 16020 trainingloss 0.6931471805599453
iteration 3 batch 16030 trainingloss 0.6931471805599453
iteration 3 batch 16040 trainingloss 0.6931471805599453
iteration 3 batch 16050 trainingloss 0.6931471805599453
iteration 3 batch 16060 trainingloss 0.6931471805599453
iteration 3 batch 16070 trainingloss 0.6931471805599453
iteration 3 batch 16080 trainingloss 0.6931471805599453
iteration 3 batch 16090 trainingloss 0.6931471805599453
iteration 3 batch 16100 trainingloss 0.6931471805599453
iteration 3 batch 16110 trainingloss 0.6916632528527511
iteration 3 batch 16120 trainingloss 0.6916632528527511
iteration 3 batch 16130 trainingloss 0.6931471805599453
iteration 3 batch 16140 trainingloss 0.6931471805599453
iteration 3 batch 16150 trainingloss 0.6931471805599453
iteration 3 batch 16160 trainingloss 0.6931471805599453
iteration 3 batch 16170 trainingloss 0.6931471805599453
iteration 3 batch 16180 trainingloss 0.6931471805599453
iteration 3 batch 16190 trainingloss 0.6931471805599453
iteration 3 batch 16200 trainingloss 0.6931471805599453
iteration 3 batch 16210 trainingloss 0.6916632528527511
iteration 3 batch 16220 trainingloss 0.6931471805599453
iteration 3 batch 16230 trainingloss 0.6931471805599453
iteration 3 batch 16240 trainingloss 0.6931471805599453
iteration 3 batch 16250 trainingloss 0.6931471805599453
iteration 3 batch 16260 trainingloss 0.6931471805599453
iteration 3 batch 16270 trainingloss 0.6931471805599453
iteration 3 batch 16280 trainingloss 0.6931471805599453
iteration 3 batch 16290 trainingloss 0.6931471805599453
iteration 3 batch 16300 trainingloss 0.6931471805599453
iteration 3 batch 16310 trainingloss 0.6931471805599453
iteration 3 batch 16320 trainingloss 0.6931471805599453
iteration 3 batch 16330 trainingloss 0.6931471805599453
iteration 3 batch 16340 trainingloss 0.6916632528527511
iteration 3 batch 16350 trainingloss 0.6931471805599453
iteration 3 batch 16360 trainingloss 0.6931471805599453
iteration 3 batch 16370 trainingloss 0.6931471805599453
iteration 3 batch 16380 trainingloss 0.6931471805599453
iteration 3 batch 16390 trainingloss 0.6916632528527511
iteration 3 batch 16400 trainingloss 0.6931471805599453
iteration 3 batch 16410 trainingloss 0.6931471805599453
iteration 3 batch 16420 trainingloss 0.6931471805599453
iteration 3 batch 16430 trainingloss 0.6931471805599453
iteration 3 batch 16440 trainingloss 0.6931471805599453
iteration 3 batch 16450 trainingloss 0.6931471805599453
iteration 3 batch 16460 trainingloss 0.6931471805599453
iteration 3 batch 16470 trainingloss 0.6931471805599453
iteration 3 batch 16480 trainingloss 0.6931471805599453
iteration 3 batch 16490 trainingloss 0.6931471805599453
iteration 3 batch 16500 trainingloss 0.6931471805599453
iteration 3 batch 16510 trainingloss 0.6931471805599453
iteration 3 batch 16520 trainingloss 0.6931471805599453
iteration 3 batch 16530 trainingloss 0.6931471805599453
iteration 3 batch 16540 trainingloss 0.6931471805599453
iteration 3 batch 16550 trainingloss 0.6931471805599453
iteration 3 batch 16560 trainingloss 0.6916632528527511
iteration 3 batch 16570 trainingloss 0.6931471805599453
iteration 3 batch 16580 trainingloss 0.6931471805599453
iteration 3 batch 16590 trainingloss 0.6931471805599453
iteration 3 batch 16600 trainingloss 0.6931471805599453
iteration 3 batch 16610 trainingloss 0.6931471805599453
iteration 3 batch 16620 trainingloss 0.6931471805599453
iteration 3 batch 16630 trainingloss 0.6931471805599453
iteration 3 batch 16640 trainingloss 0.6916632528527511
iteration 3 batch 16650 trainingloss 0.6931471805599453
iteration 3 batch 16660 trainingloss 0.6931471805599453
iteration 3 batch 16670 trainingloss 0.6931471805599453
iteration 3 batch 16680 trainingloss 0.6931471805599453
iteration 3 batch 16690 trainingloss 0.6931471805599453
iteration 3 batch 16700 trainingloss 0.6931471805599453
iteration 3 batch 16710 trainingloss 0.6931471805599453
iteration 3 batch 16720 trainingloss 0.6931471805599453
iteration 3 batch 16730 trainingloss 0.6931471805599453
iteration 3 batch 16740 trainingloss 0.6931471805599453
iteration 3 batch 16750 trainingloss 0.6931471805599453
iteration 3 batch 16760 trainingloss 0.6931471805599453
iteration 3 batch 16770 trainingloss 0.6931471805599453
iteration 3 batch 16780 trainingloss 0.6931471805599453
iteration 3 batch 16790 trainingloss 0.6931471805599453
iteration 3 batch 16800 trainingloss 0.6931471805599453
iteration 3 batch 16810 trainingloss 0.6931471805599453
iteration 3 batch 16820 trainingloss 0.6931471805599453
iteration 3 batch 16830 trainingloss 0.6931471805599453
iteration 3 batch 16840 trainingloss 0.6931471805599453
iteration 3 batch 16850 trainingloss 0.6931471805599453
iteration 3 batch 16860 trainingloss 0.6931471805599453
iteration 3 batch 16870 trainingloss 0.6931471805599453
iteration 3 batch 16880 trainingloss 0.6931471805599453
iteration 3 batch 16890 trainingloss 0.6931471805599453
iteration 3 batch 16900 trainingloss 0.6916632528527511
iteration 3 batch 16910 trainingloss 0.6931471805599453
iteration 3 batch 16920 trainingloss 0.6931471805599453
iteration 3 batch 16930 trainingloss 0.6931471805599453
iteration 3 batch 16940 trainingloss 0.6931471805599453
iteration 3 batch 16950 trainingloss 0.6931471805599453
iteration 3 batch 16960 trainingloss 0.6931471805599453
iteration 3 batch 16970 trainingloss 0.6931471805599453
iteration 3 batch 16980 trainingloss 0.6931471805599453
iteration 3 batch 16990 trainingloss 0.6916632528527511
iteration 3 batch 17000 trainingloss 0.6931471805599453
iteration 3 batch 17010 trainingloss 0.6931471805599453
iteration 3 batch 17020 trainingloss 0.6931471805599453
iteration 3 batch 17030 trainingloss 0.6931471805599453
iteration 3 batch 17040 trainingloss 0.6931471805599453
iteration 3 batch 17050 trainingloss 0.6916632528527511
iteration 3 batch 17060 trainingloss 0.6931471805599453
iteration 3 batch 17070 trainingloss 0.6931471805599453
iteration 3 batch 17080 trainingloss 0.6931471805599453
iteration 3 batch 17090 trainingloss 0.6931471805599453
iteration 3 batch 17100 trainingloss 0.6931471805599453
iteration 3 batch 17110 trainingloss 0.6931471805599453
iteration 3 batch 17120 trainingloss 0.6931471805599453
iteration 3 batch 17130 trainingloss 0.6931471805599453
iteration 3 batch 17140 trainingloss 0.6931471805599453
iteration 3 batch 17150 trainingloss 0.6931471805599453
iteration 3 batch 17160 trainingloss 0.6931471805599453
iteration 3 batch 17170 trainingloss 0.6931471805599453
iteration 3 batch 17180 trainingloss 0.6931471805599453
iteration 3 batch 17190 trainingloss 0.6931471805599453
iteration 3 batch 17200 trainingloss 0.6931471805599453
iteration 3 batch 17210 trainingloss 0.6931471805599453
iteration 3 batch 17220 trainingloss 0.6931471805599453
iteration 3 batch 17230 trainingloss 0.6931471805599453
iteration 3 batch 17240 trainingloss 0.6931471805599453
iteration 3 batch 17250 trainingloss 0.6916632528527511
iteration 3 batch 17260 trainingloss 0.6931471805599453
iteration 3 batch 17270 trainingloss 0.6931471805599453
iteration 3 batch 17280 trainingloss 0.6931471805599453
iteration 3 batch 17290 trainingloss 0.6931471805599453
iteration 3 batch 17300 trainingloss 0.6931471805599453
iteration 3 batch 17310 trainingloss 0.6931471805599453
iteration 3 batch 17320 trainingloss 0.6931471805599453
iteration 3 batch 17330 trainingloss 0.6916632528527511
iteration 3 batch 17340 trainingloss 0.6931471805599453
iteration 3 batch 17350 trainingloss 0.6931471805599453
iteration 3 batch 17360 trainingloss 0.6931471805599453
iteration 3 batch 17370 trainingloss 0.6931471805599453
iteration 3 batch 17380 trainingloss 0.6931471805599453
iteration 3 batch 17390 trainingloss 0.6931471805599453
iteration 3 batch 17400 trainingloss 0.6931471805599453
iteration 3 batch 17410 trainingloss 0.6931471805599453
iteration 3 batch 17420 trainingloss 0.6931471805599453
iteration 3 batch 17430 trainingloss 0.6931471805599453
iteration 3 batch 17440 trainingloss 0.6931471805599453
iteration 3 batch 17450 trainingloss 0.6916632528527511
iteration 3 batch 17460 trainingloss 0.6931471805599453
iteration 3 batch 17470 trainingloss 0.6931471805599453
iteration 3 batch 17480 trainingloss 0.6931471805599453
iteration 3 batch 17490 trainingloss 0.6931471805599453
iteration 3 batch 17500 trainingloss 0.6916632528527511
iteration 3 batch 17510 trainingloss 0.6916632528527511
iteration 3 batch 17520 trainingloss 0.6931471805599453
iteration 3 batch 17530 trainingloss 0.6931471805599453
iteration 3 batch 17540 trainingloss 0.6931471805599453
iteration 3 batch 17550 trainingloss 0.6931471805599453
iteration 3 batch 17560 trainingloss 0.6931471805599453
iteration 3 batch 17570 trainingloss 0.6931471805599453
iteration 3 batch 17580 trainingloss 0.6931471805599453
iteration 3 batch 17590 trainingloss 0.6931471805599453
iteration 3 batch 17600 trainingloss 0.6931471805599453
iteration 3 batch 17610 trainingloss 0.6931471805599453
iteration 3 batch 17620 trainingloss 0.6931471805599453
iteration 3 batch 17630 trainingloss 0.6931471805599453
iteration 3 batch 17640 trainingloss 0.6931471805599453
iteration 3 batch 17650 trainingloss 0.6931471805599453
iteration 3 batch 17660 trainingloss 0.6931471805599453
iteration 3 batch 17670 trainingloss 0.6931471805599453
iteration 3 batch 17680 trainingloss 0.6931471805599453
iteration 3 batch 17690 trainingloss 0.6931471805599453
iteration 3 batch 17700 trainingloss 0.6931471805599453
iteration 3 batch 17710 trainingloss 0.6931471805599453
iteration 3 batch 17720 trainingloss 0.6931471805599453
iteration 3 batch 17730 trainingloss 0.6916632528527511
iteration 3 batch 17740 trainingloss 0.6931471805599453
iteration 3 batch 17750 trainingloss 0.6931471805599453
iteration 3 batch 17760 trainingloss 0.6931471805599453
iteration 3 batch 17770 trainingloss 0.6931471805599453
iteration 3 batch 17780 trainingloss 0.6931471805599453
iteration 3 batch 17790 trainingloss 0.6931471805599453
iteration 3 batch 17800 trainingloss 0.6931471805599453
iteration 3 batch 17810 trainingloss 0.6931471805599453
iteration 3 batch 17820 trainingloss 0.6931471805599453
iteration 3 batch 17830 trainingloss 0.6916632528527511
iteration 3 batch 17840 trainingloss 0.6931471805599453
iteration 3 batch 17850 trainingloss 0.6931471805599453
iteration 3 batch 17860 trainingloss 0.6931471805599453
iteration 3 batch 17870 trainingloss 0.6931471805599453
iteration 3 batch 17880 trainingloss 0.6931471805599453
iteration 3 batch 17890 trainingloss 0.6931471805599453
iteration 3 batch 17900 trainingloss 0.6931471805599453
iteration 3 batch 17910 trainingloss 0.6931471805599453
iteration 3 batch 17920 trainingloss 0.6931471805599453
iteration 3 batch 17930 trainingloss 0.6931471805599453
iteration 3 batch 17940 trainingloss 0.6931471805599453
iteration 3 batch 17950 trainingloss 0.6931471805599453
iteration 3 batch 17960 trainingloss 0.6931471805599453
iteration 3 batch 17970 trainingloss 0.6931471805599453
iteration 3 batch 17980 trainingloss 0.6931471805599453
iteration 3 batch 17990 trainingloss 0.6931471805599453
iteration 3 batch 18000 trainingloss 0.6931471805599453
iteration 3 batch 18010 trainingloss 0.6931471805599453
iteration 3 batch 18020 trainingloss 0.6931471805599453
iteration 3 batch 18030 trainingloss 0.6931471805599453
iteration 3 batch 18040 trainingloss 0.6931471805599453
iteration 3 batch 18050 trainingloss 0.6931471805599453
iteration 3 batch 18060 trainingloss 0.6916632528527511
iteration 3 batch 18070 trainingloss 0.6931471805599453
iteration 3 batch 18080 trainingloss 0.6931471805599453
iteration 3 batch 18090 trainingloss 0.6931471805599453
iteration 3 batch 18100 trainingloss 0.6931471805599453
iteration 3 batch 18110 trainingloss 0.6931471805599453
iteration 3 batch 18120 trainingloss 0.6931471805599453
iteration 3 batch 18130 trainingloss 0.6931471805599453
iteration 3 batch 18140 trainingloss 0.6931471805599453
iteration 3 batch 18150 trainingloss 0.6916632528527511
iteration 3 batch 18160 trainingloss 0.6931471805599453
iteration 3 batch 18170 trainingloss 0.6931471805599453
iteration 3 batch 18180 trainingloss 0.6916632528527511
iteration 3 batch 18190 trainingloss 0.6931471805599453
iteration 3 batch 18200 trainingloss 0.6931471805599453
iteration 3 batch 18210 trainingloss 0.6931471805599453
iteration 3 batch 18220 trainingloss 0.6931471805599453
iteration 3 batch 18230 trainingloss 0.6931471805599453
iteration 3 batch 18240 trainingloss 0.6931471805599453
iteration 3 batch 18250 trainingloss 0.6931471805599453
iteration 3 batch 18260 trainingloss 0.6931471805599453
iteration 3 batch 18270 trainingloss 0.6931471805599453
iteration 3 batch 18280 trainingloss 0.6931471805599453
iteration 3 batch 18290 trainingloss 0.6931471805599453
iteration 3 batch 18300 trainingloss 0.6931471805599453
iteration 3 batch 18310 trainingloss 0.6931471805599453
iteration 3 batch 18320 trainingloss 0.6931471805599453
iteration 3 batch 18330 trainingloss 0.6931471805599453
iteration 3 batch 18340 trainingloss 0.6916632528527511
iteration 3 batch 18350 trainingloss 0.6931471805599453
iteration 3 batch 18360 trainingloss 0.6931471805599453
iteration 3 batch 18370 trainingloss 0.6931471805599453
iteration 3 batch 18380 trainingloss 0.6931471805599453
iteration 3 batch 18390 trainingloss 0.6901793251455568
iteration 3 batch 18400 trainingloss 0.6931471805599453
iteration 3 batch 18410 trainingloss 0.6931471805599453
iteration 3 batch 18420 trainingloss 0.6931471805599453
iteration 3 batch 18430 trainingloss 0.6931471805599453
iteration 3 batch 18440 trainingloss 0.6931471805599453
iteration 3 batch 18450 trainingloss 0.6931471805599453
iteration 3 batch 18460 trainingloss 0.6931471805599453
iteration 3 batch 18470 trainingloss 0.6931471805599453
iteration 3 batch 18480 trainingloss 0.6931471805599453
iteration 3 batch 18490 trainingloss 0.6931471805599453
iteration 3 batch 18500 trainingloss 0.6931471805599453
iteration 3 batch 18510 trainingloss 0.6931471805599453
iteration 3 batch 18520 trainingloss 0.6931471805599453
iteration 3 batch 18530 trainingloss 0.6931471805599453
iteration 3 batch 18540 trainingloss 0.6931471805599453
iteration 3 batch 18550 trainingloss 0.6931471805599453
iteration 3 batch 18560 trainingloss 0.6931471805599453
iteration 3 batch 18570 trainingloss 0.6931471805599453
iteration 3 batch 18580 trainingloss 0.6931471805599453
iteration 3 batch 18590 trainingloss 0.6916632528527511
iteration 3 batch 18600 trainingloss 0.6931471805599453
iteration 3 batch 18610 trainingloss 0.6931471805599453
iteration 4 batch 0 trainingloss 0.6931471805599453
iteration 4 batch 10 trainingloss 0.6931471805599453
iteration 4 batch 20 trainingloss 0.6931471805599453
iteration 4 batch 30 trainingloss 0.6931471805599453
iteration 4 batch 40 trainingloss 0.6931471805599453
iteration 4 batch 50 trainingloss 0.6916632528527511
iteration 4 batch 60 trainingloss 0.6931471805599453
iteration 4 batch 70 trainingloss 0.6931471805599453
iteration 4 batch 80 trainingloss 0.6931471805599453
iteration 4 batch 90 trainingloss 0.6931471805599453
iteration 4 batch 100 trainingloss 0.6931471805599453
iteration 4 batch 110 trainingloss 0.6931471805599453
iteration 4 batch 120 trainingloss 0.6931471805599453
iteration 4 batch 130 trainingloss 0.6931471805599453
iteration 4 batch 140 trainingloss 0.6931471805599453
iteration 4 batch 150 trainingloss 0.6931471805599453
iteration 4 batch 160 trainingloss 0.6931471805599453
iteration 4 batch 170 trainingloss 0.6931471805599453
iteration 4 batch 180 trainingloss 0.6931471805599453
iteration 4 batch 190 trainingloss 0.6931471805599453
iteration 4 batch 200 trainingloss 0.6931471805599453
iteration 4 batch 210 trainingloss 0.6931471805599453
iteration 4 batch 220 trainingloss 0.6931471805599453
iteration 4 batch 230 trainingloss 0.6931471805599453
iteration 4 batch 240 trainingloss 0.6931471805599453
iteration 4 batch 250 trainingloss 0.6931471805599453
iteration 4 batch 260 trainingloss 0.6931471805599453
iteration 4 batch 270 trainingloss 0.6931471805599453
iteration 4 batch 280 trainingloss 0.6931471805599453
iteration 4 batch 290 trainingloss 0.6931471805599453
iteration 4 batch 300 trainingloss 0.6931471805599453
iteration 4 batch 310 trainingloss 0.6931471805599453
iteration 4 batch 320 trainingloss 0.6931471805599453
iteration 4 batch 330 trainingloss 0.6931471805599453
iteration 4 batch 340 trainingloss 0.6931471805599453
iteration 4 batch 350 trainingloss 0.6931471805599453
iteration 4 batch 360 trainingloss 0.6931471805599453
iteration 4 batch 370 trainingloss 0.6931471805599453
iteration 4 batch 380 trainingloss 0.6931471805599453
iteration 4 batch 390 trainingloss 0.6931471805599453
iteration 4 batch 400 trainingloss 0.6916632528527511
iteration 4 batch 410 trainingloss 0.6931471805599453
iteration 4 batch 420 trainingloss 0.6931471805599453
iteration 4 batch 430 trainingloss 0.6916632528527511
iteration 4 batch 440 trainingloss 0.6931471805599453
iteration 4 batch 450 trainingloss 0.6931471805599453
iteration 4 batch 460 trainingloss 0.6931471805599453
iteration 4 batch 470 trainingloss 0.6931471805599453
iteration 4 batch 480 trainingloss 0.6931471805599453
iteration 4 batch 490 trainingloss 0.6931471805599453
iteration 4 batch 500 trainingloss 0.6931471805599453
iteration 4 batch 510 trainingloss 0.6931471805599453
iteration 4 batch 520 trainingloss 0.6931471805599453
iteration 4 batch 530 trainingloss 0.6931471805599453
iteration 4 batch 540 trainingloss 0.6931471805599453
iteration 4 batch 550 trainingloss 0.6931471805599453
iteration 4 batch 560 trainingloss 0.6931471805599453
iteration 4 batch 570 trainingloss 0.6931471805599453
iteration 4 batch 580 trainingloss 0.6931471805599453
iteration 4 batch 590 trainingloss 0.6931471805599453
iteration 4 batch 600 trainingloss 0.6931471805599453
iteration 4 batch 610 trainingloss 0.6931471805599453
iteration 4 batch 620 trainingloss 0.6931471805599453
iteration 4 batch 630 trainingloss 0.6931471805599453
iteration 4 batch 640 trainingloss 0.6931471805599453
iteration 4 batch 650 trainingloss 0.6931471805599453
iteration 4 batch 660 trainingloss 0.6931471805599453
iteration 4 batch 670 trainingloss 0.6931471805599453
iteration 4 batch 680 trainingloss 0.6931471805599453
iteration 4 batch 690 trainingloss 0.6916632528527511
iteration 4 batch 700 trainingloss 0.6931471805599453
iteration 4 batch 710 trainingloss 0.6931471805599453
iteration 4 batch 720 trainingloss 0.6931471805599453
iteration 4 batch 730 trainingloss 0.6931471805599453
iteration 4 batch 740 trainingloss 0.6931471805599453
iteration 4 batch 750 trainingloss 0.6931471805599453
iteration 4 batch 760 trainingloss 0.6931471805599453
iteration 4 batch 770 trainingloss 0.6931471805599453
iteration 4 batch 780 trainingloss 0.6931471805599453
iteration 4 batch 790 trainingloss 0.6931471805599453
iteration 4 batch 800 trainingloss 0.6931471805599453
iteration 4 batch 810 trainingloss 0.6931471805599453
iteration 4 batch 820 trainingloss 0.6916632528527511
iteration 4 batch 830 trainingloss 0.6916632528527511
iteration 4 batch 840 trainingloss 0.6931471805599453
iteration 4 batch 850 trainingloss 0.6931471805599453
iteration 4 batch 860 trainingloss 0.6931471805599453
iteration 4 batch 870 trainingloss 0.6931471805599453
iteration 4 batch 880 trainingloss 0.6916632528527511
iteration 4 batch 890 trainingloss 0.6931471805599453
iteration 4 batch 900 trainingloss 0.6931471805599453
iteration 4 batch 910 trainingloss 0.6931471805599453
iteration 4 batch 920 trainingloss 0.6931471805599453
iteration 4 batch 930 trainingloss 0.6931471805599453
iteration 4 batch 940 trainingloss 0.6931471805599453
iteration 4 batch 950 trainingloss 0.6931471805599453
iteration 4 batch 960 trainingloss 0.6931471805599453
iteration 4 batch 970 trainingloss 0.6931471805599453
iteration 4 batch 980 trainingloss 0.6916632528527511
iteration 4 batch 990 trainingloss 0.6931471805599453
iteration 4 batch 1000 trainingloss 0.6931471805599453
iteration 4 batch 1010 trainingloss 0.6931471805599453
iteration 4 batch 1020 trainingloss 0.6931471805599453
iteration 4 batch 1030 trainingloss 0.6931471805599453
iteration 4 batch 1040 trainingloss 0.6931471805599453
iteration 4 batch 1050 trainingloss 0.6931471805599453
iteration 4 batch 1060 trainingloss 0.6931471805599453
iteration 4 batch 1070 trainingloss 0.6931471805599453
iteration 4 batch 1080 trainingloss 0.6931471805599453
iteration 4 batch 1090 trainingloss 0.6931471805599453
iteration 4 batch 1100 trainingloss 0.6931471805599453
iteration 4 batch 1110 trainingloss 0.6931471805599453
iteration 4 batch 1120 trainingloss 0.6931471805599453
iteration 4 batch 1130 trainingloss 0.6931471805599453
iteration 4 batch 1140 trainingloss 0.6931471805599453
iteration 4 batch 1150 trainingloss 0.6931471805599453
iteration 4 batch 1160 trainingloss 0.6931471805599453
iteration 4 batch 1170 trainingloss 0.6931471805599453
iteration 4 batch 1180 trainingloss 0.6931471805599453
iteration 4 batch 1190 trainingloss 0.6931471805599453
iteration 4 batch 1200 trainingloss 0.6931471805599453
iteration 4 batch 1210 trainingloss 0.6931471805599453
iteration 4 batch 1220 trainingloss 0.6931471805599453
iteration 4 batch 1230 trainingloss 0.6916632528527511
iteration 4 batch 1240 trainingloss 0.6931471805599453
iteration 4 batch 1250 trainingloss 0.6931471805599453
iteration 4 batch 1260 trainingloss 0.6931471805599453
iteration 4 batch 1270 trainingloss 0.6931471805599453
iteration 4 batch 1280 trainingloss 0.6931471805599453
iteration 4 batch 1290 trainingloss 0.6931471805599453
iteration 4 batch 1300 trainingloss 0.6931471805599453
iteration 4 batch 1310 trainingloss 0.6931471805599453
iteration 4 batch 1320 trainingloss 0.6931471805599453
iteration 4 batch 1330 trainingloss 0.6931471805599453
iteration 4 batch 1340 trainingloss 0.6931471805599453
iteration 4 batch 1350 trainingloss 0.6931471805599453
iteration 4 batch 1360 trainingloss 0.6931471805599453
iteration 4 batch 1370 trainingloss 0.6931471805599453
iteration 4 batch 1380 trainingloss 0.6931471805599453
iteration 4 batch 1390 trainingloss 0.6931471805599453
iteration 4 batch 1400 trainingloss 0.6931471805599453
iteration 4 batch 1410 trainingloss 0.6931471805599453
iteration 4 batch 1420 trainingloss 0.6931471805599453
iteration 4 batch 1430 trainingloss 0.6931471805599453
iteration 4 batch 1440 trainingloss 0.6931471805599453
iteration 4 batch 1450 trainingloss 0.6931471805599453
iteration 4 batch 1460 trainingloss 0.6931471805599453
iteration 4 batch 1470 trainingloss 0.6931471805599453
iteration 4 batch 1480 trainingloss 0.6931471805599453
iteration 4 batch 1490 trainingloss 0.6931471805599453
iteration 4 batch 1500 trainingloss 0.6931471805599453
iteration 4 batch 1510 trainingloss 0.6931471805599453
iteration 4 batch 1520 trainingloss 0.6931471805599453
iteration 4 batch 1530 trainingloss 0.6931471805599453
iteration 4 batch 1540 trainingloss 0.6931471805599453
iteration 4 batch 1550 trainingloss 0.6931471805599453
iteration 4 batch 1560 trainingloss 0.6931471805599453
iteration 4 batch 1570 trainingloss 0.6931471805599453
iteration 4 batch 1580 trainingloss 0.6931471805599453
iteration 4 batch 1590 trainingloss 0.6931471805599453
iteration 4 batch 1600 trainingloss 0.6931471805599453
iteration 4 batch 1610 trainingloss 0.6931471805599453
iteration 4 batch 1620 trainingloss 0.6931471805599453
iteration 4 batch 1630 trainingloss 0.6916632528527511
iteration 4 batch 1640 trainingloss 0.6931471805599453
iteration 4 batch 1650 trainingloss 0.6931471805599453
iteration 4 batch 1660 trainingloss 0.6931471805599453
iteration 4 batch 1670 trainingloss 0.6916632528527511
iteration 4 batch 1680 trainingloss 0.6931471805599453
iteration 4 batch 1690 trainingloss 0.6931471805599453
iteration 4 batch 1700 trainingloss 0.6931471805599453
iteration 4 batch 1710 trainingloss 0.6931471805599453
iteration 4 batch 1720 trainingloss 0.6931471805599453
iteration 4 batch 1730 trainingloss 0.6931471805599453
iteration 4 batch 1740 trainingloss 0.6916632528527511
iteration 4 batch 1750 trainingloss 0.6931471805599453
iteration 4 batch 1760 trainingloss 0.6931471805599453
iteration 4 batch 1770 trainingloss 0.6931471805599453
iteration 4 batch 1780 trainingloss 0.6931471805599453
iteration 4 batch 1790 trainingloss 0.6931471805599453
iteration 4 batch 1800 trainingloss 0.6931471805599453
iteration 4 batch 1810 trainingloss 0.6931471805599453
iteration 4 batch 1820 trainingloss 0.6931471805599453
iteration 4 batch 1830 trainingloss 0.6931471805599453
iteration 4 batch 1840 trainingloss 0.6931471805599453
iteration 4 batch 1850 trainingloss 0.6916632528527511
iteration 4 batch 1860 trainingloss 0.6931471805599453
iteration 4 batch 1870 trainingloss 0.6931471805599453
iteration 4 batch 1880 trainingloss 0.6931471805599453
iteration 4 batch 1890 trainingloss 0.6931471805599453
iteration 4 batch 1900 trainingloss 0.6931471805599453
iteration 4 batch 1910 trainingloss 0.6931471805599453
iteration 4 batch 1920 trainingloss 0.6931471805599453
iteration 4 batch 1930 trainingloss 0.6931471805599453
iteration 4 batch 1940 trainingloss 0.6931471805599453
iteration 4 batch 1950 trainingloss 0.6931471805599453
iteration 4 batch 1960 trainingloss 0.6931471805599453
iteration 4 batch 1970 trainingloss 0.6931471805599453
iteration 4 batch 1980 trainingloss 0.6931471805599453
iteration 4 batch 1990 trainingloss 0.6931471805599453
iteration 4 batch 2000 trainingloss 0.6931471805599453
iteration 4 batch 2010 trainingloss 0.6931471805599453
iteration 4 batch 2020 trainingloss 0.6931471805599453
iteration 4 batch 2030 trainingloss 0.6931471805599453
iteration 4 batch 2040 trainingloss 0.6931471805599453
iteration 4 batch 2050 trainingloss 0.6931471805599453
iteration 4 batch 2060 trainingloss 0.6931471805599453
iteration 4 batch 2070 trainingloss 0.6931471805599453
iteration 4 batch 2080 trainingloss 0.6931471805599453
iteration 4 batch 2090 trainingloss 0.6931471805599453
iteration 4 batch 2100 trainingloss 0.6931471805599453
iteration 4 batch 2110 trainingloss 0.6931471805599453
iteration 4 batch 2120 trainingloss 0.6931471805599453
iteration 4 batch 2130 trainingloss 0.6931471805599453
iteration 4 batch 2140 trainingloss 0.6931471805599453
iteration 4 batch 2150 trainingloss 0.6931471805599453
iteration 4 batch 2160 trainingloss 0.6931471805599453
iteration 4 batch 2170 trainingloss 0.6931471805599453
iteration 4 batch 2180 trainingloss 0.6931471805599453
iteration 4 batch 2190 trainingloss 0.6931471805599453
iteration 4 batch 2200 trainingloss 0.6931471805599453
iteration 4 batch 2210 trainingloss 0.6931471805599453
iteration 4 batch 2220 trainingloss 0.6931471805599453
iteration 4 batch 2230 trainingloss 0.6931471805599453
iteration 4 batch 2240 trainingloss 0.6931471805599453
iteration 4 batch 2250 trainingloss 0.6931471805599453
iteration 4 batch 2260 trainingloss 0.6931471805599453
iteration 4 batch 2270 trainingloss 0.6931471805599453
iteration 4 batch 2280 trainingloss 0.6931471805599453
iteration 4 batch 2290 trainingloss 0.6931471805599453
iteration 4 batch 2300 trainingloss 0.6931471805599453
iteration 4 batch 2310 trainingloss 0.6931471805599453
iteration 4 batch 2320 trainingloss 0.6931471805599453
iteration 4 batch 2330 trainingloss 0.6931471805599453
iteration 4 batch 2340 trainingloss 0.6931471805599453
iteration 4 batch 2350 trainingloss 0.6931471805599453
iteration 4 batch 2360 trainingloss 0.6931471805599453
iteration 4 batch 2370 trainingloss 0.6931471805599453
iteration 4 batch 2380 trainingloss 0.6931471805599453
iteration 4 batch 2390 trainingloss 0.6931471805599453
iteration 4 batch 2400 trainingloss 0.6931471805599453
iteration 4 batch 2410 trainingloss 0.6931471805599453
iteration 4 batch 2420 trainingloss 0.6931471805599453
iteration 4 batch 2430 trainingloss 0.6931471805599453
iteration 4 batch 2440 trainingloss 0.6931471805599453
iteration 4 batch 2450 trainingloss 0.6931471805599453
iteration 4 batch 2460 trainingloss 0.6931471805599453
iteration 4 batch 2470 trainingloss 0.6931471805599453
iteration 4 batch 2480 trainingloss 0.6931471805599453
iteration 4 batch 2490 trainingloss 0.6931471805599453
iteration 4 batch 2500 trainingloss 0.6931471805599453
iteration 4 batch 2510 trainingloss 0.6931471805599453
iteration 4 batch 2520 trainingloss 0.6931471805599453
iteration 4 batch 2530 trainingloss 0.6931471805599453
iteration 4 batch 2540 trainingloss 0.6931471805599453
iteration 4 batch 2550 trainingloss 0.6931471805599453
iteration 4 batch 2560 trainingloss 0.6931471805599453
iteration 4 batch 2570 trainingloss 0.6931471805599453
iteration 4 batch 2580 trainingloss 0.6931471805599453
iteration 4 batch 2590 trainingloss 0.6931471805599453
iteration 4 batch 2600 trainingloss 0.6931471805599453
iteration 4 batch 2610 trainingloss 0.6931471805599453
iteration 4 batch 2620 trainingloss 0.6931471805599453
iteration 4 batch 2630 trainingloss 0.6931471805599453
iteration 4 batch 2640 trainingloss 0.6931471805599453
iteration 4 batch 2650 trainingloss 0.6931471805599453
iteration 4 batch 2660 trainingloss 0.6931471805599453
iteration 4 batch 2670 trainingloss 0.6931471805599453
iteration 4 batch 2680 trainingloss 0.6931471805599453
iteration 4 batch 2690 trainingloss 0.6931471805599453
iteration 4 batch 2700 trainingloss 0.6931471805599453
iteration 4 batch 2710 trainingloss 0.6931471805599453
iteration 4 batch 2720 trainingloss 0.6931471805599453
iteration 4 batch 2730 trainingloss 0.6916632528527511
iteration 4 batch 2740 trainingloss 0.6931471805599453
iteration 4 batch 2750 trainingloss 0.6931471805599453
iteration 4 batch 2760 trainingloss 0.6931471805599453
iteration 4 batch 2770 trainingloss 0.6931471805599453
iteration 4 batch 2780 trainingloss 0.6931471805599453
iteration 4 batch 2790 trainingloss 0.6916632528527511
iteration 4 batch 2800 trainingloss 0.6931471805599453
iteration 4 batch 2810 trainingloss 0.6931471805599453
iteration 4 batch 2820 trainingloss 0.6931471805599453
iteration 4 batch 2830 trainingloss 0.6931471805599453
iteration 4 batch 2840 trainingloss 0.6931471805599453
iteration 4 batch 2850 trainingloss 0.6931471805599453
iteration 4 batch 2860 trainingloss 0.6931471805599453
iteration 4 batch 2870 trainingloss 0.6931471805599453
iteration 4 batch 2880 trainingloss 0.6931471805599453
iteration 4 batch 2890 trainingloss 0.6931471805599453
iteration 4 batch 2900 trainingloss 0.6931471805599453
iteration 4 batch 2910 trainingloss 0.6931471805599453
iteration 4 batch 2920 trainingloss 0.6916632528527511
iteration 4 batch 2930 trainingloss 0.6931471805599453
iteration 4 batch 2940 trainingloss 0.6931471805599453
iteration 4 batch 2950 trainingloss 0.6931471805599453
iteration 4 batch 2960 trainingloss 0.6931471805599453
iteration 4 batch 2970 trainingloss 0.6931471805599453
iteration 4 batch 2980 trainingloss 0.6931471805599453
iteration 4 batch 2990 trainingloss 0.6931471805599453
iteration 4 batch 3000 trainingloss 0.6931471805599453
iteration 4 batch 3010 trainingloss 0.6931471805599453
iteration 4 batch 3020 trainingloss 0.6931471805599453
iteration 4 batch 3030 trainingloss 0.6931471805599453
iteration 4 batch 3040 trainingloss 0.6931471805599453
iteration 4 batch 3050 trainingloss 0.6931471805599453
iteration 4 batch 3060 trainingloss 0.6931471805599453
iteration 4 batch 3070 trainingloss 0.6931471805599453
iteration 4 batch 3080 trainingloss 0.6931471805599453
iteration 4 batch 3090 trainingloss 0.6931471805599453
iteration 4 batch 3100 trainingloss 0.6931471805599453
iteration 4 batch 3110 trainingloss 0.6931471805599453
iteration 4 batch 3120 trainingloss 0.6931471805599453
iteration 4 batch 3130 trainingloss 0.6931471805599453
iteration 4 batch 3140 trainingloss 0.6931471805599453
iteration 4 batch 3150 trainingloss 0.6931471805599453
iteration 4 batch 3160 trainingloss 0.6931471805599453
iteration 4 batch 3170 trainingloss 0.6931471805599453
iteration 4 batch 3180 trainingloss 0.6931471805599453
iteration 4 batch 3190 trainingloss 0.6931471805599453
iteration 4 batch 3200 trainingloss 0.6931471805599453
iteration 4 batch 3210 trainingloss 0.6931471805599453
iteration 4 batch 3220 trainingloss 0.6931471805599453
iteration 4 batch 3230 trainingloss 0.6931471805599453
iteration 4 batch 3240 trainingloss 0.6916632528527511
iteration 4 batch 3250 trainingloss 0.6931471805599453
iteration 4 batch 3260 trainingloss 0.6931471805599453
iteration 4 batch 3270 trainingloss 0.6931471805599453
iteration 4 batch 3280 trainingloss 0.6931471805599453
iteration 4 batch 3290 trainingloss 0.6931471805599453
iteration 4 batch 3300 trainingloss 0.6931471805599453
iteration 4 batch 3310 trainingloss 0.6931471805599453
iteration 4 batch 3320 trainingloss 0.6931471805599453
iteration 4 batch 3330 trainingloss 0.6931471805599453
iteration 4 batch 3340 trainingloss 0.6931471805599453
iteration 4 batch 3350 trainingloss 0.6931471805599453
iteration 4 batch 3360 trainingloss 0.6916632528527511
iteration 4 batch 3370 trainingloss 0.6931471805599453
iteration 4 batch 3380 trainingloss 0.6931471805599453
iteration 4 batch 3390 trainingloss 0.6931471805599453
iteration 4 batch 3400 trainingloss 0.6931471805599453
iteration 4 batch 3410 trainingloss 0.6931471805599453
iteration 4 batch 3420 trainingloss 0.6931471805599453
iteration 4 batch 3430 trainingloss 0.6931471805599453
iteration 4 batch 3440 trainingloss 0.6931471805599453
iteration 4 batch 3450 trainingloss 0.6931471805599453
iteration 4 batch 3460 trainingloss 0.6931471805599453
iteration 4 batch 3470 trainingloss 0.6931471805599453
iteration 4 batch 3480 trainingloss 0.6931471805599453
iteration 4 batch 3490 trainingloss 0.6931471805599453
iteration 4 batch 3500 trainingloss 0.6931471805599453
iteration 4 batch 3510 trainingloss 0.6931471805599453
iteration 4 batch 3520 trainingloss 0.6931471805599453
iteration 4 batch 3530 trainingloss 0.6931471805599453
iteration 4 batch 3540 trainingloss 0.6931471805599453
iteration 4 batch 3550 trainingloss 0.6931471805599453
iteration 4 batch 3560 trainingloss 0.6931471805599453
iteration 4 batch 3570 trainingloss 0.6931471805599453
iteration 4 batch 3580 trainingloss 0.6931471805599453
iteration 4 batch 3590 trainingloss 0.6916632528527511
iteration 4 batch 3600 trainingloss 0.6931471805599453
iteration 4 batch 3610 trainingloss 0.6931471805599453
iteration 4 batch 3620 trainingloss 0.6931471805599453
iteration 4 batch 3630 trainingloss 0.6916632528527511
iteration 4 batch 3640 trainingloss 0.6931471805599453
iteration 4 batch 3650 trainingloss 0.6931471805599453
iteration 4 batch 3660 trainingloss 0.6931471805599453
iteration 4 batch 3670 trainingloss 0.6931471805599453
iteration 4 batch 3680 trainingloss 0.6931471805599453
iteration 4 batch 3690 trainingloss 0.6931471805599453
iteration 4 batch 3700 trainingloss 0.6931471805599453
iteration 4 batch 3710 trainingloss 0.6916632528527511
iteration 4 batch 3720 trainingloss 0.6931471805599453
iteration 4 batch 3730 trainingloss 0.6931471805599453
iteration 4 batch 3740 trainingloss 0.6931471805599453
iteration 4 batch 3750 trainingloss 0.6931471805599453
iteration 4 batch 3760 trainingloss 0.6916632528527511
iteration 4 batch 3770 trainingloss 0.6931471805599453
iteration 4 batch 3780 trainingloss 0.6931471805599453
iteration 4 batch 3790 trainingloss 0.6916632528527511
iteration 4 batch 3800 trainingloss 0.6931471805599453
iteration 4 batch 3810 trainingloss 0.6931471805599453
iteration 4 batch 3820 trainingloss 0.6931471805599453
iteration 4 batch 3830 trainingloss 0.6931471805599453
iteration 4 batch 3840 trainingloss 0.6931471805599453
iteration 4 batch 3850 trainingloss 0.6931471805599453
iteration 4 batch 3860 trainingloss 0.6931471805599453
iteration 4 batch 3870 trainingloss 0.6931471805599453
iteration 4 batch 3880 trainingloss 0.6931471805599453
iteration 4 batch 3890 trainingloss 0.6931471805599453
iteration 4 batch 3900 trainingloss 0.6931471805599453
iteration 4 batch 3910 trainingloss 0.6931471805599453
iteration 4 batch 3920 trainingloss 0.6931471805599453
iteration 4 batch 3930 trainingloss 0.6931471805599453
iteration 4 batch 3940 trainingloss 0.6931471805599453
iteration 4 batch 3950 trainingloss 0.6931471805599453
iteration 4 batch 3960 trainingloss 0.6931471805599453
iteration 4 batch 3970 trainingloss 0.6931471805599453
iteration 4 batch 3980 trainingloss 0.6931471805599453
iteration 4 batch 3990 trainingloss 0.6931471805599453
iteration 4 batch 4000 trainingloss 0.6931471805599453
iteration 4 batch 4010 trainingloss 0.6931471805599453
iteration 4 batch 4020 trainingloss 0.6931471805599453
iteration 4 batch 4030 trainingloss 0.6931471805599453
iteration 4 batch 4040 trainingloss 0.6931471805599453
iteration 4 batch 4050 trainingloss 0.6931471805599453
iteration 4 batch 4060 trainingloss 0.6931471805599453
iteration 4 batch 4070 trainingloss 0.6931471805599453
iteration 4 batch 4080 trainingloss 0.6931471805599453
iteration 4 batch 4090 trainingloss 0.6931471805599453
iteration 4 batch 4100 trainingloss 0.6931471805599453
iteration 4 batch 4110 trainingloss 0.6931471805599453
iteration 4 batch 4120 trainingloss 0.6931471805599453
iteration 4 batch 4130 trainingloss 0.6931471805599453
iteration 4 batch 4140 trainingloss 0.6931471805599453
iteration 4 batch 4150 trainingloss 0.6916632528527511
iteration 4 batch 4160 trainingloss 0.6916632528527511
iteration 4 batch 4170 trainingloss 0.6931471805599453
iteration 4 batch 4180 trainingloss 0.6931471805599453
iteration 4 batch 4190 trainingloss 0.6931471805599453
iteration 4 batch 4200 trainingloss 0.6931471805599453
iteration 4 batch 4210 trainingloss 0.6931471805599453
iteration 4 batch 4220 trainingloss 0.6931471805599453
iteration 4 batch 4230 trainingloss 0.6931471805599453
iteration 4 batch 4240 trainingloss 0.6931471805599453
iteration 4 batch 4250 trainingloss 0.6931471805599453
iteration 4 batch 4260 trainingloss 0.6931471805599453
iteration 4 batch 4270 trainingloss 0.6931471805599453
iteration 4 batch 4280 trainingloss 0.6931471805599453
iteration 4 batch 4290 trainingloss 0.6916632528527511
iteration 4 batch 4300 trainingloss 0.6931471805599453
iteration 4 batch 4310 trainingloss 0.6931471805599453
iteration 4 batch 4320 trainingloss 0.6931471805599453
iteration 4 batch 4330 trainingloss 0.6931471805599453
iteration 4 batch 4340 trainingloss 0.6931471805599453
iteration 4 batch 4350 trainingloss 0.6931471805599453
iteration 4 batch 4360 trainingloss 0.6931471805599453
iteration 4 batch 4370 trainingloss 0.6931471805599453
iteration 4 batch 4380 trainingloss 0.6931471805599453
iteration 4 batch 4390 trainingloss 0.6931471805599453
iteration 4 batch 4400 trainingloss 0.6931471805599453
iteration 4 batch 4410 trainingloss 0.6931471805599453
iteration 4 batch 4420 trainingloss 0.6931471805599453
iteration 4 batch 4430 trainingloss 0.6931471805599453
iteration 4 batch 4440 trainingloss 0.6931471805599453
iteration 4 batch 4450 trainingloss 0.6931471805599453
iteration 4 batch 4460 trainingloss 0.6931471805599453
iteration 4 batch 4470 trainingloss 0.6931471805599453
iteration 4 batch 4480 trainingloss 0.6931471805599453
iteration 4 batch 4490 trainingloss 0.6931471805599453
iteration 4 batch 4500 trainingloss 0.6931471805599453
iteration 4 batch 4510 trainingloss 0.6931471805599453
iteration 4 batch 4520 trainingloss 0.6931471805599453
iteration 4 batch 4530 trainingloss 0.6931471805599453
iteration 4 batch 4540 trainingloss 0.6931471805599453
iteration 4 batch 4550 trainingloss 0.6916632528527511
iteration 4 batch 4560 trainingloss 0.6931471805599453
iteration 4 batch 4570 trainingloss 0.6931471805599453
iteration 4 batch 4580 trainingloss 0.6931471805599453
iteration 4 batch 4590 trainingloss 0.6931471805599453
iteration 4 batch 4600 trainingloss 0.6931471805599453
iteration 4 batch 4610 trainingloss 0.6916632528527511
iteration 4 batch 4620 trainingloss 0.6931471805599453
iteration 4 batch 4630 trainingloss 0.6931471805599453
iteration 4 batch 4640 trainingloss 0.6931471805599453
iteration 4 batch 4650 trainingloss 0.6931471805599453
iteration 4 batch 4660 trainingloss 0.6931471805599453
iteration 4 batch 4670 trainingloss 0.6931471805599453
iteration 4 batch 4680 trainingloss 0.6931471805599453
iteration 4 batch 4690 trainingloss 0.6931471805599453
iteration 4 batch 4700 trainingloss 0.6931471805599453
iteration 4 batch 4710 trainingloss 0.6931471805599453
iteration 4 batch 4720 trainingloss 0.6931471805599453
iteration 4 batch 4730 trainingloss 0.6931471805599453
iteration 4 batch 4740 trainingloss 0.6931471805599453
iteration 4 batch 4750 trainingloss 0.6931471805599453
iteration 4 batch 4760 trainingloss 0.6931471805599453
iteration 4 batch 4770 trainingloss 0.6931471805599453
iteration 4 batch 4780 trainingloss 0.6931471805599453
iteration 4 batch 4790 trainingloss 0.6916632528527511
iteration 4 batch 4800 trainingloss 0.6931471805599453
iteration 4 batch 4810 trainingloss 0.6931471805599453
iteration 4 batch 4820 trainingloss 0.6931471805599453
iteration 4 batch 4830 trainingloss 0.6931471805599453
iteration 4 batch 4840 trainingloss 0.6931471805599453
iteration 4 batch 4850 trainingloss 0.6931471805599453
iteration 4 batch 4860 trainingloss 0.6931471805599453
iteration 4 batch 4870 trainingloss 0.6931471805599453
iteration 4 batch 4880 trainingloss 0.6931471805599453
iteration 4 batch 4890 trainingloss 0.6931471805599453
iteration 4 batch 4900 trainingloss 0.6931471805599453
iteration 4 batch 4910 trainingloss 0.6931471805599453
iteration 4 batch 4920 trainingloss 0.6931471805599453
iteration 4 batch 4930 trainingloss 0.6931471805599453
iteration 4 batch 4940 trainingloss 0.6931471805599453
iteration 4 batch 4950 trainingloss 0.6931471805599453
iteration 4 batch 4960 trainingloss 0.6931471805599453
iteration 4 batch 4970 trainingloss 0.6931471805599453
iteration 4 batch 4980 trainingloss 0.6931471805599453
iteration 4 batch 4990 trainingloss 0.6916632528527511
iteration 4 batch 5000 trainingloss 0.6931471805599453
iteration 4 batch 5010 trainingloss 0.6931471805599453
iteration 4 batch 5020 trainingloss 0.6931471805599453
iteration 4 batch 5030 trainingloss 0.6931471805599453
iteration 4 batch 5040 trainingloss 0.6931471805599453
iteration 4 batch 5050 trainingloss 0.6916632528527511
iteration 4 batch 5060 trainingloss 0.6931471805599453
iteration 4 batch 5070 trainingloss 0.6931471805599453
iteration 4 batch 5080 trainingloss 0.6931471805599453
iteration 4 batch 5090 trainingloss 0.6916632528527511
iteration 4 batch 5100 trainingloss 0.6931471805599453
iteration 4 batch 5110 trainingloss 0.6931471805599453
iteration 4 batch 5120 trainingloss 0.6931471805599453
iteration 4 batch 5130 trainingloss 0.6931471805599453
iteration 4 batch 5140 trainingloss 0.6931471805599453
iteration 4 batch 5150 trainingloss 0.6931471805599453
iteration 4 batch 5160 trainingloss 0.6931471805599453
iteration 4 batch 5170 trainingloss 0.6931471805599453
iteration 4 batch 5180 trainingloss 0.6931471805599453
iteration 4 batch 5190 trainingloss 0.6931471805599453
iteration 4 batch 5200 trainingloss 0.6931471805599453
iteration 4 batch 5210 trainingloss 0.6916632528527511
iteration 4 batch 5220 trainingloss 0.6931471805599453
iteration 4 batch 5230 trainingloss 0.6931471805599453
iteration 4 batch 5240 trainingloss 0.6931471805599453
iteration 4 batch 5250 trainingloss 0.6931471805599453
iteration 4 batch 5260 trainingloss 0.6916632528527511
iteration 4 batch 5270 trainingloss 0.6931471805599453
iteration 4 batch 5280 trainingloss 0.6931471805599453
iteration 4 batch 5290 trainingloss 0.6931471805599453
iteration 4 batch 5300 trainingloss 0.6931471805599453
iteration 4 batch 5310 trainingloss 0.6931471805599453
iteration 4 batch 5320 trainingloss 0.6931471805599453
iteration 4 batch 5330 trainingloss 0.6931471805599453
iteration 4 batch 5340 trainingloss 0.6931471805599453
iteration 4 batch 5350 trainingloss 0.6931471805599453
iteration 4 batch 5360 trainingloss 0.6931471805599453
iteration 4 batch 5370 trainingloss 0.6931471805599453
iteration 4 batch 5380 trainingloss 0.6931471805599453
iteration 4 batch 5390 trainingloss 0.6931471805599453
iteration 4 batch 5400 trainingloss 0.6931471805599453
iteration 4 batch 5410 trainingloss 0.6931471805599453
iteration 4 batch 5420 trainingloss 0.6931471805599453
iteration 4 batch 5430 trainingloss 0.6931471805599453
iteration 4 batch 5440 trainingloss 0.6931471805599453
iteration 4 batch 5450 trainingloss 0.6931471805599453
iteration 4 batch 5460 trainingloss 0.6931471805599453
iteration 4 batch 5470 trainingloss 0.6931471805599453
iteration 4 batch 5480 trainingloss 0.6931471805599453
iteration 4 batch 5490 trainingloss 0.6916632528527511
iteration 4 batch 5500 trainingloss 0.6931471805599453
iteration 4 batch 5510 trainingloss 0.6931471805599453
iteration 4 batch 5520 trainingloss 0.6931471805599453
iteration 4 batch 5530 trainingloss 0.6931471805599453
iteration 4 batch 5540 trainingloss 0.6931471805599453
iteration 4 batch 5550 trainingloss 0.6931471805599453
iteration 4 batch 5560 trainingloss 0.6916632528527511
iteration 4 batch 5570 trainingloss 0.6931471805599453
iteration 4 batch 5580 trainingloss 0.6916632528527511
iteration 4 batch 5590 trainingloss 0.6931471805599453
iteration 4 batch 5600 trainingloss 0.6931471805599453
iteration 4 batch 5610 trainingloss 0.6931471805599453
iteration 4 batch 5620 trainingloss 0.6931471805599453
iteration 4 batch 5630 trainingloss 0.6931471805599453
iteration 4 batch 5640 trainingloss 0.6931471805599453
iteration 4 batch 5650 trainingloss 0.6916632528527511
iteration 4 batch 5660 trainingloss 0.6931471805599453
iteration 4 batch 5670 trainingloss 0.6931471805599453
iteration 4 batch 5680 trainingloss 0.6931471805599453
iteration 4 batch 5690 trainingloss 0.6931471805599453
iteration 4 batch 5700 trainingloss 0.6931471805599453
iteration 4 batch 5710 trainingloss 0.6931471805599453
iteration 4 batch 5720 trainingloss 0.6931471805599453
iteration 4 batch 5730 trainingloss 0.6931471805599453
iteration 4 batch 5740 trainingloss 0.6931471805599453
iteration 4 batch 5750 trainingloss 0.6931471805599453
iteration 4 batch 5760 trainingloss 0.6931471805599453
iteration 4 batch 5770 trainingloss 0.6931471805599453
iteration 4 batch 5780 trainingloss 0.6916632528527511
iteration 4 batch 5790 trainingloss 0.6931471805599453
iteration 4 batch 5800 trainingloss 0.6931471805599453
iteration 4 batch 5810 trainingloss 0.6931471805599453
iteration 4 batch 5820 trainingloss 0.6931471805599453
iteration 4 batch 5830 trainingloss 0.6931471805599453
iteration 4 batch 5840 trainingloss 0.6931471805599453
iteration 4 batch 5850 trainingloss 0.6931471805599453
iteration 4 batch 5860 trainingloss 0.6931471805599453
iteration 4 batch 5870 trainingloss 0.6931471805599453
iteration 4 batch 5880 trainingloss 0.6931471805599453
iteration 4 batch 5890 trainingloss 0.6931471805599453
iteration 4 batch 5900 trainingloss 0.6931471805599453
iteration 4 batch 5910 trainingloss 0.6931471805599453
iteration 4 batch 5920 trainingloss 0.6931471805599453
iteration 4 batch 5930 trainingloss 0.6931471805599453
iteration 4 batch 5940 trainingloss 0.6931471805599453
iteration 4 batch 5950 trainingloss 0.6931471805599453
iteration 4 batch 5960 trainingloss 0.6931471805599453
iteration 4 batch 5970 trainingloss 0.6931471805599453
iteration 4 batch 5980 trainingloss 0.6931471805599453
iteration 4 batch 5990 trainingloss 0.6931471805599453
iteration 4 batch 6000 trainingloss 0.6931471805599453
iteration 4 batch 6010 trainingloss 0.6931471805599453
iteration 4 batch 6020 trainingloss 0.6931471805599453
iteration 4 batch 6030 trainingloss 0.6931471805599453
iteration 4 batch 6040 trainingloss 0.6931471805599453
iteration 4 batch 6050 trainingloss 0.6931471805599453
iteration 4 batch 6060 trainingloss 0.6931471805599453
iteration 4 batch 6070 trainingloss 0.6931471805599453
iteration 4 batch 6080 trainingloss 0.6931471805599453
iteration 4 batch 6090 trainingloss 0.6916632528527511
iteration 4 batch 6100 trainingloss 0.6931471805599453
iteration 4 batch 6110 trainingloss 0.6931471805599453
iteration 4 batch 6120 trainingloss 0.6931471805599453
iteration 4 batch 6130 trainingloss 0.6931471805599453
iteration 4 batch 6140 trainingloss 0.6931471805599453
iteration 4 batch 6150 trainingloss 0.6931471805599453
iteration 4 batch 6160 trainingloss 0.6931471805599453
iteration 4 batch 6170 trainingloss 0.6916632528527511
iteration 4 batch 6180 trainingloss 0.6916632528527511
iteration 4 batch 6190 trainingloss 0.6931471805599453
iteration 4 batch 6200 trainingloss 0.6931471805599453
iteration 4 batch 6210 trainingloss 0.6931471805599453
iteration 4 batch 6220 trainingloss 0.6931471805599453
iteration 4 batch 6230 trainingloss 0.6916632528527511
iteration 4 batch 6240 trainingloss 0.6901793251455568
iteration 4 batch 6250 trainingloss 0.6931471805599453
iteration 4 batch 6260 trainingloss 0.6931471805599453
iteration 4 batch 6270 trainingloss 0.6931471805599453
iteration 4 batch 6280 trainingloss 0.6931471805599453
iteration 4 batch 6290 trainingloss 0.6901793251455568
iteration 4 batch 6300 trainingloss 0.6931471805599453
iteration 4 batch 6310 trainingloss 0.6931471805599453
iteration 4 batch 6320 trainingloss 0.6931471805599453
iteration 4 batch 6330 trainingloss 0.6931471805599453
iteration 4 batch 6340 trainingloss 0.6931471805599453
iteration 4 batch 6350 trainingloss 0.6931471805599453
iteration 4 batch 6360 trainingloss 0.6931471805599453
iteration 4 batch 6370 trainingloss 0.6916632528527511
iteration 4 batch 6380 trainingloss 0.6931471805599453
iteration 4 batch 6390 trainingloss 0.6931471805599453
iteration 4 batch 6400 trainingloss 0.6931471805599453
iteration 4 batch 6410 trainingloss 0.6916632528527511
iteration 4 batch 6420 trainingloss 0.6931471805599453
iteration 4 batch 6430 trainingloss 0.6931471805599453
iteration 4 batch 6440 trainingloss 0.6931471805599453
iteration 4 batch 6450 trainingloss 0.6931471805599453
iteration 4 batch 6460 trainingloss 0.6931471805599453
iteration 4 batch 6470 trainingloss 0.6931471805599453
iteration 4 batch 6480 trainingloss 0.6931471805599453
iteration 4 batch 6490 trainingloss 0.6931471805599453
iteration 4 batch 6500 trainingloss 0.6916632528527511
iteration 4 batch 6510 trainingloss 0.6931471805599453
iteration 4 batch 6520 trainingloss 0.6931471805599453
iteration 4 batch 6530 trainingloss 0.6931471805599453
iteration 4 batch 6540 trainingloss 0.6931471805599453
iteration 4 batch 6550 trainingloss 0.6931471805599453
iteration 4 batch 6560 trainingloss 0.6931471805599453
iteration 4 batch 6570 trainingloss 0.6931471805599453
iteration 4 batch 6580 trainingloss 0.6931471805599453
iteration 4 batch 6590 trainingloss 0.6931471805599453
iteration 4 batch 6600 trainingloss 0.6931471805599453
iteration 4 batch 6610 trainingloss 0.6931471805599453
iteration 4 batch 6620 trainingloss 0.6931471805599453
iteration 4 batch 6630 trainingloss 0.6931471805599453
iteration 4 batch 6640 trainingloss 0.6931471805599453
iteration 4 batch 6650 trainingloss 0.6931471805599453
iteration 4 batch 6660 trainingloss 0.6931471805599453
iteration 4 batch 6670 trainingloss 0.6931471805599453
iteration 4 batch 6680 trainingloss 0.6931471805599453
iteration 4 batch 6690 trainingloss 0.6931471805599453
iteration 4 batch 6700 trainingloss 0.6931471805599453
iteration 4 batch 6710 trainingloss 0.6931471805599453
iteration 4 batch 6720 trainingloss 0.6931471805599453
iteration 4 batch 6730 trainingloss 0.6931471805599453
iteration 4 batch 6740 trainingloss 0.6931471805599453
iteration 4 batch 6750 trainingloss 0.6931471805599453
iteration 4 batch 6760 trainingloss 0.6931471805599453
iteration 4 batch 6770 trainingloss 0.6931471805599453
iteration 4 batch 6780 trainingloss 0.6931471805599453
iteration 4 batch 6790 trainingloss 0.6931471805599453
iteration 4 batch 6800 trainingloss 0.6931471805599453
iteration 4 batch 6810 trainingloss 0.6931471805599453
iteration 4 batch 6820 trainingloss 0.6931471805599453
iteration 4 batch 6830 trainingloss 0.6931471805599453
iteration 4 batch 6840 trainingloss 0.6931471805599453
iteration 4 batch 6850 trainingloss 0.6931471805599453
iteration 4 batch 6860 trainingloss 0.6931471805599453
iteration 4 batch 6870 trainingloss 0.6931471805599453
iteration 4 batch 6880 trainingloss 0.6931471805599453
iteration 4 batch 6890 trainingloss 0.6931471805599453
iteration 4 batch 6900 trainingloss 0.6931471805599453
iteration 4 batch 6910 trainingloss 0.6931471805599453
iteration 4 batch 6920 trainingloss 0.6931471805599453
iteration 4 batch 6930 trainingloss 0.6931471805599453
iteration 4 batch 6940 trainingloss 0.6931471805599453
iteration 4 batch 6950 trainingloss 0.6931471805599453
iteration 4 batch 6960 trainingloss 0.6931471805599453
iteration 4 batch 6970 trainingloss 0.6931471805599453
iteration 4 batch 6980 trainingloss 0.6931471805599453
iteration 4 batch 6990 trainingloss 0.6931471805599453
iteration 4 batch 7000 trainingloss 0.6931471805599453
iteration 4 batch 7010 trainingloss 0.6931471805599453
iteration 4 batch 7020 trainingloss 0.6931471805599453
iteration 4 batch 7030 trainingloss 0.6931471805599453
iteration 4 batch 7040 trainingloss 0.6931471805599453
iteration 4 batch 7050 trainingloss 0.6931471805599453
iteration 4 batch 7060 trainingloss 0.6931471805599453
iteration 4 batch 7070 trainingloss 0.6931471805599453
iteration 4 batch 7080 trainingloss 0.6931471805599453
iteration 4 batch 7090 trainingloss 0.6931471805599453
iteration 4 batch 7100 trainingloss 0.6931471805599453
iteration 4 batch 7110 trainingloss 0.6931471805599453
iteration 4 batch 7120 trainingloss 0.6931471805599453
iteration 4 batch 7130 trainingloss 0.6931471805599453
iteration 4 batch 7140 trainingloss 0.6931471805599453
iteration 4 batch 7150 trainingloss 0.6931471805599453
iteration 4 batch 7160 trainingloss 0.6931471805599453
iteration 4 batch 7170 trainingloss 0.6931471805599453
iteration 4 batch 7180 trainingloss 0.6931471805599453
iteration 4 batch 7190 trainingloss 0.6931471805599453
iteration 4 batch 7200 trainingloss 0.6931471805599453
iteration 4 batch 7210 trainingloss 0.6931471805599453
iteration 4 batch 7220 trainingloss 0.6931471805599453
iteration 4 batch 7230 trainingloss 0.6931471805599453
iteration 4 batch 7240 trainingloss 0.6931471805599453
iteration 4 batch 7250 trainingloss 0.6931471805599453
iteration 4 batch 7260 trainingloss 0.6931471805599453
iteration 4 batch 7270 trainingloss 0.6931471805599453
iteration 4 batch 7280 trainingloss 0.6931471805599453
iteration 4 batch 7290 trainingloss 0.6931471805599453
iteration 4 batch 7300 trainingloss 0.6931471805599453
iteration 4 batch 7310 trainingloss 0.6931471805599453
iteration 4 batch 7320 trainingloss 0.6931471805599453
iteration 4 batch 7330 trainingloss 0.6931471805599453
iteration 4 batch 7340 trainingloss 0.6931471805599453
iteration 4 batch 7350 trainingloss 0.6931471805599453
iteration 4 batch 7360 trainingloss 0.6931471805599453
iteration 4 batch 7370 trainingloss 0.6901793251455568
iteration 4 batch 7380 trainingloss 0.6931471805599453
iteration 4 batch 7390 trainingloss 0.6931471805599453
iteration 4 batch 7400 trainingloss 0.6931471805599453
iteration 4 batch 7410 trainingloss 0.6931471805599453
iteration 4 batch 7420 trainingloss 0.6931471805599453
iteration 4 batch 7430 trainingloss 0.6931471805599453
iteration 4 batch 7440 trainingloss 0.6931471805599453
iteration 4 batch 7450 trainingloss 0.6931471805599453
iteration 4 batch 7460 trainingloss 0.6916632528527511
iteration 4 batch 7470 trainingloss 0.6931471805599453
iteration 4 batch 7480 trainingloss 0.6931471805599453
iteration 4 batch 7490 trainingloss 0.6931471805599453
iteration 4 batch 7500 trainingloss 0.6931471805599453
iteration 4 batch 7510 trainingloss 0.6931471805599453
iteration 4 batch 7520 trainingloss 0.6931471805599453
iteration 4 batch 7530 trainingloss 0.6931471805599453
iteration 4 batch 7540 trainingloss 0.6931471805599453
iteration 4 batch 7550 trainingloss 0.6931471805599453
iteration 4 batch 7560 trainingloss 0.6931471805599453
iteration 4 batch 7570 trainingloss 0.6931471805599453
iteration 4 batch 7580 trainingloss 0.6931471805599453
iteration 4 batch 7590 trainingloss 0.6931471805599453
iteration 4 batch 7600 trainingloss 0.6931471805599453
iteration 4 batch 7610 trainingloss 0.6931471805599453
iteration 4 batch 7620 trainingloss 0.6931471805599453
iteration 4 batch 7630 trainingloss 0.6931471805599453
iteration 4 batch 7640 trainingloss 0.6931471805599453
iteration 4 batch 7650 trainingloss 0.6931471805599453
iteration 4 batch 7660 trainingloss 0.6931471805599453
iteration 4 batch 7670 trainingloss 0.6931471805599453
iteration 4 batch 7680 trainingloss 0.6931471805599453
iteration 4 batch 7690 trainingloss 0.6916632528527511
iteration 4 batch 7700 trainingloss 0.6931471805599453
iteration 4 batch 7710 trainingloss 0.6931471805599453
iteration 4 batch 7720 trainingloss 0.6916632528527511
iteration 4 batch 7730 trainingloss 0.6931471805599453
iteration 4 batch 7740 trainingloss 0.6931471805599453
iteration 4 batch 7750 trainingloss 0.6931471805599453
iteration 4 batch 7760 trainingloss 0.6916632528527511
iteration 4 batch 7770 trainingloss 0.6931471805599453
iteration 4 batch 7780 trainingloss 0.6931471805599453
iteration 4 batch 7790 trainingloss 0.6931471805599453
iteration 4 batch 7800 trainingloss 0.6931471805599453
iteration 4 batch 7810 trainingloss 0.6931471805599453
iteration 4 batch 7820 trainingloss 0.6931471805599453
iteration 4 batch 7830 trainingloss 0.6931471805599453
iteration 4 batch 7840 trainingloss 0.6931471805599453
iteration 4 batch 7850 trainingloss 0.6931471805599453
iteration 4 batch 7860 trainingloss 0.6931471805599453
iteration 4 batch 7870 trainingloss 0.6931471805599453
iteration 4 batch 7880 trainingloss 0.6931471805599453
iteration 4 batch 7890 trainingloss 0.6931471805599453
iteration 4 batch 7900 trainingloss 0.6931471805599453
iteration 4 batch 7910 trainingloss 0.6931471805599453
iteration 4 batch 7920 trainingloss 0.6931471805599453
iteration 4 batch 7930 trainingloss 0.6931471805599453
iteration 4 batch 7940 trainingloss 0.6931471805599453
iteration 4 batch 7950 trainingloss 0.6931471805599453
iteration 4 batch 7960 trainingloss 0.6931471805599453
iteration 4 batch 7970 trainingloss 0.6931471805599453
iteration 4 batch 7980 trainingloss 0.6931471805599453
iteration 4 batch 7990 trainingloss 0.6931471805599453
iteration 4 batch 8000 trainingloss 0.6931471805599453
iteration 4 batch 8010 trainingloss 0.6931471805599453
iteration 4 batch 8020 trainingloss 0.6931471805599453
iteration 4 batch 8030 trainingloss 0.6931471805599453
iteration 4 batch 8040 trainingloss 0.6931471805599453
iteration 4 batch 8050 trainingloss 0.6931471805599453
iteration 4 batch 8060 trainingloss 0.6931471805599453
iteration 4 batch 8070 trainingloss 0.6931471805599453
iteration 4 batch 8080 trainingloss 0.6916632528527511
iteration 4 batch 8090 trainingloss 0.6931471805599453
iteration 4 batch 8100 trainingloss 0.6931471805599453
iteration 4 batch 8110 trainingloss 0.6931471805599453
iteration 4 batch 8120 trainingloss 0.6931471805599453
iteration 4 batch 8130 trainingloss 0.6931471805599453
iteration 4 batch 8140 trainingloss 0.6931471805599453
iteration 4 batch 8150 trainingloss 0.6931471805599453
iteration 4 batch 8160 trainingloss 0.6931471805599453
iteration 4 batch 8170 trainingloss 0.6931471805599453
iteration 4 batch 8180 trainingloss 0.6931471805599453
iteration 4 batch 8190 trainingloss 0.6931471805599453
iteration 4 batch 8200 trainingloss 0.6931471805599453
iteration 4 batch 8210 trainingloss 0.6931471805599453
iteration 4 batch 8220 trainingloss 0.6931471805599453
iteration 4 batch 8230 trainingloss 0.6931471805599453
iteration 4 batch 8240 trainingloss 0.6931471805599453
iteration 4 batch 8250 trainingloss 0.6931471805599453
iteration 4 batch 8260 trainingloss 0.6931471805599453
iteration 4 batch 8270 trainingloss 0.6931471805599453
iteration 4 batch 8280 trainingloss 0.6931471805599453
iteration 4 batch 8290 trainingloss 0.6916632528527511
iteration 4 batch 8300 trainingloss 0.6931471805599453
iteration 4 batch 8310 trainingloss 0.6931471805599453
iteration 4 batch 8320 trainingloss 0.6931471805599453
iteration 4 batch 8330 trainingloss 0.6931471805599453
iteration 4 batch 8340 trainingloss 0.6931471805599453
iteration 4 batch 8350 trainingloss 0.6931471805599453
iteration 4 batch 8360 trainingloss 0.6931471805599453
iteration 4 batch 8370 trainingloss 0.6931471805599453
iteration 4 batch 8380 trainingloss 0.6931471805599453
iteration 4 batch 8390 trainingloss 0.6931471805599453
iteration 4 batch 8400 trainingloss 0.6931471805599453
iteration 4 batch 8410 trainingloss 0.6931471805599453
iteration 4 batch 8420 trainingloss 0.6931471805599453
iteration 4 batch 8430 trainingloss 0.6931471805599453
iteration 4 batch 8440 trainingloss 0.6931471805599453
iteration 4 batch 8450 trainingloss 0.6931471805599453
iteration 4 batch 8460 trainingloss 0.6931471805599453
iteration 4 batch 8470 trainingloss 0.6931471805599453
iteration 4 batch 8480 trainingloss 0.6931471805599453
iteration 4 batch 8490 trainingloss 0.6931471805599453
iteration 4 batch 8500 trainingloss 0.6931471805599453
iteration 4 batch 8510 trainingloss 0.6931471805599453
iteration 4 batch 8520 trainingloss 0.6916632528527511
iteration 4 batch 8530 trainingloss 0.6931471805599453
iteration 4 batch 8540 trainingloss 0.6931471805599453
iteration 4 batch 8550 trainingloss 0.6931471805599453
iteration 4 batch 8560 trainingloss 0.6931471805599453
iteration 4 batch 8570 trainingloss 0.6931471805599453
iteration 4 batch 8580 trainingloss 0.6931471805599453
iteration 4 batch 8590 trainingloss 0.6931471805599453
iteration 4 batch 8600 trainingloss 0.6931471805599453
iteration 4 batch 8610 trainingloss 0.6931471805599453
iteration 4 batch 8620 trainingloss 0.6931471805599453
iteration 4 batch 8630 trainingloss 0.6931471805599453
iteration 4 batch 8640 trainingloss 0.6931471805599453
iteration 4 batch 8650 trainingloss 0.6931471805599453
iteration 4 batch 8660 trainingloss 0.6931471805599453
iteration 4 batch 8670 trainingloss 0.6931471805599453
iteration 4 batch 8680 trainingloss 0.6931471805599453
iteration 4 batch 8690 trainingloss 0.6931471805599453
iteration 4 batch 8700 trainingloss 0.6931471805599453
iteration 4 batch 8710 trainingloss 0.6931471805599453
iteration 4 batch 8720 trainingloss 0.6931471805599453
iteration 4 batch 8730 trainingloss 0.6931471805599453
iteration 4 batch 8740 trainingloss 0.6931471805599453
iteration 4 batch 8750 trainingloss 0.6931471805599453
iteration 4 batch 8760 trainingloss 0.6931471805599453
iteration 4 batch 8770 trainingloss 0.6931471805599453
iteration 4 batch 8780 trainingloss 0.6931471805599453
iteration 4 batch 8790 trainingloss 0.6931471805599453
iteration 4 batch 8800 trainingloss 0.6931471805599453
iteration 4 batch 8810 trainingloss 0.6931471805599453
iteration 4 batch 8820 trainingloss 0.6931471805599453
iteration 4 batch 8830 trainingloss 0.6931471805599453
iteration 4 batch 8840 trainingloss 0.6931471805599453
iteration 4 batch 8850 trainingloss 0.6931471805599453
iteration 4 batch 8860 trainingloss 0.6916632528527511
iteration 4 batch 8870 trainingloss 0.6931471805599453
iteration 4 batch 8880 trainingloss 0.6931471805599453
iteration 4 batch 8890 trainingloss 0.6931471805599453
iteration 4 batch 8900 trainingloss 0.6931471805599453
iteration 4 batch 8910 trainingloss 0.6931471805599453
iteration 4 batch 8920 trainingloss 0.6931471805599453
iteration 4 batch 8930 trainingloss 0.6931471805599453
iteration 4 batch 8940 trainingloss 0.6931471805599453
iteration 4 batch 8950 trainingloss 0.6931471805599453
iteration 4 batch 8960 trainingloss 0.6931471805599453
iteration 4 batch 8970 trainingloss 0.6931471805599453
iteration 4 batch 8980 trainingloss 0.6931471805599453
iteration 4 batch 8990 trainingloss 0.6931471805599453
iteration 4 batch 9000 trainingloss 0.6931471805599453
iteration 4 batch 9010 trainingloss 0.6931471805599453
iteration 4 batch 9020 trainingloss 0.6931471805599453
iteration 4 batch 9030 trainingloss 0.6931471805599453
iteration 4 batch 9040 trainingloss 0.6931471805599453
iteration 4 batch 9050 trainingloss 0.6931471805599453
iteration 4 batch 9060 trainingloss 0.6931471805599453
iteration 4 batch 9070 trainingloss 0.6931471805599453
iteration 4 batch 9080 trainingloss 0.6931471805599453
iteration 4 batch 9090 trainingloss 0.6931471805599453
iteration 4 batch 9100 trainingloss 0.6931471805599453
iteration 4 batch 9110 trainingloss 0.6931471805599453
iteration 4 batch 9120 trainingloss 0.6931471805599453
iteration 4 batch 9130 trainingloss 0.6931471805599453
iteration 4 batch 9140 trainingloss 0.6931471805599453
iteration 4 batch 9150 trainingloss 0.6931471805599453
iteration 4 batch 9160 trainingloss 0.6931471805599453
iteration 4 batch 9170 trainingloss 0.6931471805599453
iteration 4 batch 9180 trainingloss 0.6931471805599453
iteration 4 batch 9190 trainingloss 0.6916632528527511
iteration 4 batch 9200 trainingloss 0.6931471805599453
iteration 4 batch 9210 trainingloss 0.6931471805599453
iteration 4 batch 9220 trainingloss 0.6916632528527511
iteration 4 batch 9230 trainingloss 0.6916632528527511
iteration 4 batch 9240 trainingloss 0.6931471805599453
iteration 4 batch 9250 trainingloss 0.6931471805599453
iteration 4 batch 9260 trainingloss 0.6931471805599453
iteration 4 batch 9270 trainingloss 0.6931471805599453
iteration 4 batch 9280 trainingloss 0.6931471805599453
iteration 4 batch 9290 trainingloss 0.6931471805599453
iteration 4 batch 9300 trainingloss 0.6931471805599453
iteration 4 batch 9310 trainingloss 0.6931471805599453
iteration 4 batch 9320 trainingloss 0.6931471805599453
iteration 4 batch 9330 trainingloss 0.6931471805599453
iteration 4 batch 9340 trainingloss 0.6931471805599453
iteration 4 batch 9350 trainingloss 0.6931471805599453
iteration 4 batch 9360 trainingloss 0.6931471805599453
iteration 4 batch 9370 trainingloss 0.6916632528527511
iteration 4 batch 9380 trainingloss 0.6931471805599453
iteration 4 batch 9390 trainingloss 0.6931471805599453
iteration 4 batch 9400 trainingloss 0.6931471805599453
iteration 4 batch 9410 trainingloss 0.6931471805599453
iteration 4 batch 9420 trainingloss 0.6931471805599453
iteration 4 batch 9430 trainingloss 0.6931471805599453
iteration 4 batch 9440 trainingloss 0.6916632528527511
iteration 4 batch 9450 trainingloss 0.6931471805599453
iteration 4 batch 9460 trainingloss 0.6931471805599453
iteration 4 batch 9470 trainingloss 0.6931471805599453
iteration 4 batch 9480 trainingloss 0.6916632528527511
iteration 4 batch 9490 trainingloss 0.6931471805599453
iteration 4 batch 9500 trainingloss 0.6931471805599453
iteration 4 batch 9510 trainingloss 0.6931471805599453
iteration 4 batch 9520 trainingloss 0.6931471805599453
iteration 4 batch 9530 trainingloss 0.6931471805599453
iteration 4 batch 9540 trainingloss 0.6931471805599453
iteration 4 batch 9550 trainingloss 0.6916632528527511
iteration 4 batch 9560 trainingloss 0.6931471805599453
iteration 4 batch 9570 trainingloss 0.6916632528527511
iteration 4 batch 9580 trainingloss 0.6931471805599453
iteration 4 batch 9590 trainingloss 0.6931471805599453
iteration 4 batch 9600 trainingloss 0.6931471805599453
iteration 4 batch 9610 trainingloss 0.6931471805599453
iteration 4 batch 9620 trainingloss 0.6931471805599453
iteration 4 batch 9630 trainingloss 0.6931471805599453
iteration 4 batch 9640 trainingloss 0.6931471805599453
iteration 4 batch 9650 trainingloss 0.6931471805599453
iteration 4 batch 9660 trainingloss 0.6931471805599453
iteration 4 batch 9670 trainingloss 0.6931471805599453
iteration 4 batch 9680 trainingloss 0.6931471805599453
iteration 4 batch 9690 trainingloss 0.6916632528527511
iteration 4 batch 9700 trainingloss 0.6931471805599453
iteration 4 batch 9710 trainingloss 0.6931471805599453
iteration 4 batch 9720 trainingloss 0.6931471805599453
iteration 4 batch 9730 trainingloss 0.6931471805599453
iteration 4 batch 9740 trainingloss 0.6931471805599453
iteration 4 batch 9750 trainingloss 0.6931471805599453
iteration 4 batch 9760 trainingloss 0.6931471805599453
iteration 4 batch 9770 trainingloss 0.6931471805599453
iteration 4 batch 9780 trainingloss 0.6931471805599453
iteration 4 batch 9790 trainingloss 0.6931471805599453
iteration 4 batch 9800 trainingloss 0.6931471805599453
iteration 4 batch 9810 trainingloss 0.6931471805599453
iteration 4 batch 9820 trainingloss 0.6931471805599453
iteration 4 batch 9830 trainingloss 0.6931471805599453
iteration 4 batch 9840 trainingloss 0.6931471805599453
iteration 4 batch 9850 trainingloss 0.6931471805599453
iteration 4 batch 9860 trainingloss 0.6931471805599453
iteration 4 batch 9870 trainingloss 0.6931471805599453
iteration 4 batch 9880 trainingloss 0.6931471805599453
iteration 4 batch 9890 trainingloss 0.6931471805599453
iteration 4 batch 9900 trainingloss 0.6931471805599453
iteration 4 batch 9910 trainingloss 0.6931471805599453
iteration 4 batch 9920 trainingloss 0.6931471805599453
iteration 4 batch 9930 trainingloss 0.6931471805599453
iteration 4 batch 9940 trainingloss 0.6931471805599453
iteration 4 batch 9950 trainingloss 0.6931471805599453
iteration 4 batch 9960 trainingloss 0.6931471805599453
iteration 4 batch 9970 trainingloss 0.6916632528527511
iteration 4 batch 9980 trainingloss 0.6931471805599453
iteration 4 batch 9990 trainingloss 0.6931471805599453
iteration 4 batch 10000 trainingloss 0.6931471805599453
iteration 4 batch 10010 trainingloss 0.6931471805599453
iteration 4 batch 10020 trainingloss 0.6931471805599453
iteration 4 batch 10030 trainingloss 0.6931471805599453
iteration 4 batch 10040 trainingloss 0.6931471805599453
iteration 4 batch 10050 trainingloss 0.6931471805599453
iteration 4 batch 10060 trainingloss 0.6931471805599453
iteration 4 batch 10070 trainingloss 0.6931471805599453
iteration 4 batch 10080 trainingloss 0.6931471805599453
iteration 4 batch 10090 trainingloss 0.6931471805599453
iteration 4 batch 10100 trainingloss 0.6931471805599453
iteration 4 batch 10110 trainingloss 0.6931471805599453
iteration 4 batch 10120 trainingloss 0.6931471805599453
iteration 4 batch 10130 trainingloss 0.6931471805599453
iteration 4 batch 10140 trainingloss 0.6931471805599453
iteration 4 batch 10150 trainingloss 0.6931471805599453
iteration 4 batch 10160 trainingloss 0.6931471805599453
iteration 4 batch 10170 trainingloss 0.6931471805599453
iteration 4 batch 10180 trainingloss 0.6931471805599453
iteration 4 batch 10190 trainingloss 0.6931471805599453
iteration 4 batch 10200 trainingloss 0.6931471805599453
iteration 4 batch 10210 trainingloss 0.6931471805599453
iteration 4 batch 10220 trainingloss 0.6916632528527511
iteration 4 batch 10230 trainingloss 0.6916632528527511
iteration 4 batch 10240 trainingloss 0.6931471805599453
iteration 4 batch 10250 trainingloss 0.6931471805599453
iteration 4 batch 10260 trainingloss 0.6931471805599453
iteration 4 batch 10270 trainingloss 0.6931471805599453
iteration 4 batch 10280 trainingloss 0.6931471805599453
iteration 4 batch 10290 trainingloss 0.6931471805599453
iteration 4 batch 10300 trainingloss 0.6931471805599453
iteration 4 batch 10310 trainingloss 0.6931471805599453
iteration 4 batch 10320 trainingloss 0.6931471805599453
iteration 4 batch 10330 trainingloss 0.6931471805599453
iteration 4 batch 10340 trainingloss 0.6931471805599453
iteration 4 batch 10350 trainingloss 0.6916632528527511
iteration 4 batch 10360 trainingloss 0.6931471805599453
iteration 4 batch 10370 trainingloss 0.6931471805599453
iteration 4 batch 10380 trainingloss 0.6931471805599453
iteration 4 batch 10390 trainingloss 0.6931471805599453
iteration 4 batch 10400 trainingloss 0.6931471805599453
iteration 4 batch 10410 trainingloss 0.6931471805599453
iteration 4 batch 10420 trainingloss 0.6931471805599453
iteration 4 batch 10430 trainingloss 0.6931471805599453
iteration 4 batch 10440 trainingloss 0.6931471805599453
iteration 4 batch 10450 trainingloss 0.6931471805599453
iteration 4 batch 10460 trainingloss 0.6931471805599453
iteration 4 batch 10470 trainingloss 0.6931471805599453
iteration 4 batch 10480 trainingloss 0.6931471805599453
iteration 4 batch 10490 trainingloss 0.6931471805599453
iteration 4 batch 10500 trainingloss 0.6931471805599453
iteration 4 batch 10510 trainingloss 0.6931471805599453
iteration 4 batch 10520 trainingloss 0.6931471805599453
iteration 4 batch 10530 trainingloss 0.6931471805599453
iteration 4 batch 10540 trainingloss 0.6931471805599453
iteration 4 batch 10550 trainingloss 0.6931471805599453
iteration 4 batch 10560 trainingloss 0.6931471805599453
iteration 4 batch 10570 trainingloss 0.6931471805599453
iteration 4 batch 10580 trainingloss 0.6931471805599453
iteration 4 batch 10590 trainingloss 0.6931471805599453
iteration 4 batch 10600 trainingloss 0.6931471805599453
iteration 4 batch 10610 trainingloss 0.6931471805599453
iteration 4 batch 10620 trainingloss 0.6931471805599453
iteration 4 batch 10630 trainingloss 0.6931471805599453
iteration 4 batch 10640 trainingloss 0.6931471805599453
iteration 4 batch 10650 trainingloss 0.6931471805599453
iteration 4 batch 10660 trainingloss 0.6916632528527511
iteration 4 batch 10670 trainingloss 0.6931471805599453
iteration 4 batch 10680 trainingloss 0.6931471805599453
iteration 4 batch 10690 trainingloss 0.6931471805599453
iteration 4 batch 10700 trainingloss 0.6931471805599453
iteration 4 batch 10710 trainingloss 0.6931471805599453
iteration 4 batch 10720 trainingloss 0.6916632528527511
iteration 4 batch 10730 trainingloss 0.6931471805599453
iteration 4 batch 10740 trainingloss 0.6931471805599453
iteration 4 batch 10750 trainingloss 0.6931471805599453
iteration 4 batch 10760 trainingloss 0.6931471805599453
iteration 4 batch 10770 trainingloss 0.6931471805599453
iteration 4 batch 10780 trainingloss 0.6931471805599453
iteration 4 batch 10790 trainingloss 0.6931471805599453
iteration 4 batch 10800 trainingloss 0.6931471805599453
iteration 4 batch 10810 trainingloss 0.6931471805599453
iteration 4 batch 10820 trainingloss 0.6931471805599453
iteration 4 batch 10830 trainingloss 0.6931471805599453
iteration 4 batch 10840 trainingloss 0.6931471805599453
iteration 4 batch 10850 trainingloss 0.6931471805599453
iteration 4 batch 10860 trainingloss 0.6901793251455568
iteration 4 batch 10870 trainingloss 0.6931471805599453
iteration 4 batch 10880 trainingloss 0.6931471805599453
iteration 4 batch 10890 trainingloss 0.6931471805599453
iteration 4 batch 10900 trainingloss 0.6931471805599453
iteration 4 batch 10910 trainingloss 0.6916632528527511
iteration 4 batch 10920 trainingloss 0.6931471805599453
iteration 4 batch 10930 trainingloss 0.6931471805599453
iteration 4 batch 10940 trainingloss 0.6931471805599453
iteration 4 batch 10950 trainingloss 0.6931471805599453
iteration 4 batch 10960 trainingloss 0.6931471805599453
iteration 4 batch 10970 trainingloss 0.6931471805599453
iteration 4 batch 10980 trainingloss 0.6916632528527511
iteration 4 batch 10990 trainingloss 0.6931471805599453
iteration 4 batch 11000 trainingloss 0.6931471805599453
iteration 4 batch 11010 trainingloss 0.6931471805599453
iteration 4 batch 11020 trainingloss 0.6931471805599453
iteration 4 batch 11030 trainingloss 0.6931471805599453
iteration 4 batch 11040 trainingloss 0.6931471805599453
iteration 4 batch 11050 trainingloss 0.6931471805599453
iteration 4 batch 11060 trainingloss 0.6931471805599453
iteration 4 batch 11070 trainingloss 0.6931471805599453
iteration 4 batch 11080 trainingloss 0.6916632528527511
iteration 4 batch 11090 trainingloss 0.6931471805599453
iteration 4 batch 11100 trainingloss 0.6931471805599453
iteration 4 batch 11110 trainingloss 0.6931471805599453
iteration 4 batch 11120 trainingloss 0.6931471805599453
iteration 4 batch 11130 trainingloss 0.6931471805599453
iteration 4 batch 11140 trainingloss 0.6931471805599453
iteration 4 batch 11150 trainingloss 0.6931471805599453
iteration 4 batch 11160 trainingloss 0.6931471805599453
iteration 4 batch 11170 trainingloss 0.6931471805599453
iteration 4 batch 11180 trainingloss 0.6916632528527511
iteration 4 batch 11190 trainingloss 0.6931471805599453
iteration 4 batch 11200 trainingloss 0.6931471805599453
iteration 4 batch 11210 trainingloss 0.6931471805599453
iteration 4 batch 11220 trainingloss 0.6931471805599453
iteration 4 batch 11230 trainingloss 0.6931471805599453
iteration 4 batch 11240 trainingloss 0.6931471805599453
iteration 4 batch 11250 trainingloss 0.6931471805599453
iteration 4 batch 11260 trainingloss 0.6931471805599453
iteration 4 batch 11270 trainingloss 0.6931471805599453
iteration 4 batch 11280 trainingloss 0.6931471805599453
iteration 4 batch 11290 trainingloss 0.6931471805599453
iteration 4 batch 11300 trainingloss 0.6931471805599453
iteration 4 batch 11310 trainingloss 0.6931471805599453
iteration 4 batch 11320 trainingloss 0.6931471805599453
iteration 4 batch 11330 trainingloss 0.6931471805599453
iteration 4 batch 11340 trainingloss 0.6931471805599453
iteration 4 batch 11350 trainingloss 0.6931471805599453
iteration 4 batch 11360 trainingloss 0.6931471805599453
iteration 4 batch 11370 trainingloss 0.6931471805599453
iteration 4 batch 11380 trainingloss 0.6931471805599453
iteration 4 batch 11390 trainingloss 0.6916632528527511
iteration 4 batch 11400 trainingloss 0.6931471805599453
iteration 4 batch 11410 trainingloss 0.6931471805599453
iteration 4 batch 11420 trainingloss 0.6931471805599453
iteration 4 batch 11430 trainingloss 0.6931471805599453
iteration 4 batch 11440 trainingloss 0.6931471805599453
iteration 4 batch 11450 trainingloss 0.6931471805599453
iteration 4 batch 11460 trainingloss 0.6931471805599453
iteration 4 batch 11470 trainingloss 0.6931471805599453
iteration 4 batch 11480 trainingloss 0.6931471805599453
iteration 4 batch 11490 trainingloss 0.6931471805599453
iteration 4 batch 11500 trainingloss 0.6931471805599453
iteration 4 batch 11510 trainingloss 0.6931471805599453
iteration 4 batch 11520 trainingloss 0.6931471805599453
iteration 4 batch 11530 trainingloss 0.6931471805599453
iteration 4 batch 11540 trainingloss 0.6931471805599453
iteration 4 batch 11550 trainingloss 0.6931471805599453
iteration 4 batch 11560 trainingloss 0.6931471805599453
iteration 4 batch 11570 trainingloss 0.6931471805599453
iteration 4 batch 11580 trainingloss 0.6931471805599453
iteration 4 batch 11590 trainingloss 0.6931471805599453
iteration 4 batch 11600 trainingloss 0.6931471805599453
iteration 4 batch 11610 trainingloss 0.6931471805599453
iteration 4 batch 11620 trainingloss 0.6931471805599453
iteration 4 batch 11630 trainingloss 0.6931471805599453
iteration 4 batch 11640 trainingloss 0.6931471805599453
iteration 4 batch 11650 trainingloss 0.6916632528527511
iteration 4 batch 11660 trainingloss 0.6931471805599453
iteration 4 batch 11670 trainingloss 0.6931471805599453
iteration 4 batch 11680 trainingloss 0.6931471805599453
iteration 4 batch 11690 trainingloss 0.6931471805599453
iteration 4 batch 11700 trainingloss 0.6931471805599453
iteration 4 batch 11710 trainingloss 0.6931471805599453
iteration 4 batch 11720 trainingloss 0.6931471805599453
iteration 4 batch 11730 trainingloss 0.6916632528527511
iteration 4 batch 11740 trainingloss 0.6931471805599453
iteration 4 batch 11750 trainingloss 0.6916632528527511
iteration 4 batch 11760 trainingloss 0.6931471805599453
iteration 4 batch 11770 trainingloss 0.6931471805599453
iteration 4 batch 11780 trainingloss 0.6916632528527511
iteration 4 batch 11790 trainingloss 0.6931471805599453
iteration 4 batch 11800 trainingloss 0.6931471805599453
iteration 4 batch 11810 trainingloss 0.6931471805599453
iteration 4 batch 11820 trainingloss 0.6931471805599453
iteration 4 batch 11830 trainingloss 0.6931471805599453
iteration 4 batch 11840 trainingloss 0.6931471805599453
iteration 4 batch 11850 trainingloss 0.6931471805599453
iteration 4 batch 11860 trainingloss 0.6931471805599453
iteration 4 batch 11870 trainingloss 0.6931471805599453
iteration 4 batch 11880 trainingloss 0.6931471805599453
iteration 4 batch 11890 trainingloss 0.6931471805599453
iteration 4 batch 11900 trainingloss 0.6931471805599453
iteration 4 batch 11910 trainingloss 0.6931471805599453
iteration 4 batch 11920 trainingloss 0.6931471805599453
iteration 4 batch 11930 trainingloss 0.6931471805599453
iteration 4 batch 11940 trainingloss 0.6931471805599453
iteration 4 batch 11950 trainingloss 0.6931471805599453
iteration 4 batch 11960 trainingloss 0.6931471805599453
iteration 4 batch 11970 trainingloss 0.6916632528527511
iteration 4 batch 11980 trainingloss 0.6931471805599453
iteration 4 batch 11990 trainingloss 0.6931471805599453
iteration 4 batch 12000 trainingloss 0.6931471805599453
iteration 4 batch 12010 trainingloss 0.6916632528527511
iteration 4 batch 12020 trainingloss 0.6931471805599453
iteration 4 batch 12030 trainingloss 0.6931471805599453
iteration 4 batch 12040 trainingloss 0.6931471805599453
iteration 4 batch 12050 trainingloss 0.6931471805599453
iteration 4 batch 12060 trainingloss 0.6931471805599453
iteration 4 batch 12070 trainingloss 0.6931471805599453
iteration 4 batch 12080 trainingloss 0.6931471805599453
iteration 4 batch 12090 trainingloss 0.6931471805599453
iteration 4 batch 12100 trainingloss 0.6931471805599453
iteration 4 batch 12110 trainingloss 0.6931471805599453
iteration 4 batch 12120 trainingloss 0.6931471805599453
iteration 4 batch 12130 trainingloss 0.6931471805599453
iteration 4 batch 12140 trainingloss 0.6931471805599453
iteration 4 batch 12150 trainingloss 0.6931471805599453
iteration 4 batch 12160 trainingloss 0.6931471805599453
iteration 4 batch 12170 trainingloss 0.6931471805599453
iteration 4 batch 12180 trainingloss 0.6931471805599453
iteration 4 batch 12190 trainingloss 0.6931471805599453
iteration 4 batch 12200 trainingloss 0.6931471805599453
iteration 4 batch 12210 trainingloss 0.6931471805599453
iteration 4 batch 12220 trainingloss 0.6916632528527511
iteration 4 batch 12230 trainingloss 0.6931471805599453
iteration 4 batch 12240 trainingloss 0.6931471805599453
iteration 4 batch 12250 trainingloss 0.6931471805599453
iteration 4 batch 12260 trainingloss 0.6931471805599453
iteration 4 batch 12270 trainingloss 0.6931471805599453
iteration 4 batch 12280 trainingloss 0.6931471805599453
iteration 4 batch 12290 trainingloss 0.6931471805599453
iteration 4 batch 12300 trainingloss 0.6931471805599453
iteration 4 batch 12310 trainingloss 0.6931471805599453
iteration 4 batch 12320 trainingloss 0.6931471805599453
iteration 4 batch 12330 trainingloss 0.6931471805599453
iteration 4 batch 12340 trainingloss 0.6931471805599453
iteration 4 batch 12350 trainingloss 0.6931471805599453
iteration 4 batch 12360 trainingloss 0.6931471805599453
iteration 4 batch 12370 trainingloss 0.6931471805599453
iteration 4 batch 12380 trainingloss 0.6931471805599453
iteration 4 batch 12390 trainingloss 0.6931471805599453
iteration 4 batch 12400 trainingloss 0.6931471805599453
iteration 4 batch 12410 trainingloss 0.6916632528527511
iteration 4 batch 12420 trainingloss 0.6931471805599453
iteration 4 batch 12430 trainingloss 0.6931471805599453
iteration 4 batch 12440 trainingloss 0.6931471805599453
iteration 4 batch 12450 trainingloss 0.6931471805599453
iteration 4 batch 12460 trainingloss 0.6931471805599453
iteration 4 batch 12470 trainingloss 0.6931471805599453
iteration 4 batch 12480 trainingloss 0.6931471805599453
iteration 4 batch 12490 trainingloss 0.6916632528527511
iteration 4 batch 12500 trainingloss 0.6931471805599453
iteration 4 batch 12510 trainingloss 0.6931471805599453
iteration 4 batch 12520 trainingloss 0.6931471805599453
iteration 4 batch 12530 trainingloss 0.6931471805599453
iteration 4 batch 12540 trainingloss 0.6931471805599453
iteration 4 batch 12550 trainingloss 0.6931471805599453
iteration 4 batch 12560 trainingloss 0.6931471805599453
iteration 4 batch 12570 trainingloss 0.6931471805599453
iteration 4 batch 12580 trainingloss 0.6931471805599453
iteration 4 batch 12590 trainingloss 0.6931471805599453
iteration 4 batch 12600 trainingloss 0.6931471805599453
iteration 4 batch 12610 trainingloss 0.6931471805599453
iteration 4 batch 12620 trainingloss 0.6931471805599453
iteration 4 batch 12630 trainingloss 0.6931471805599453
iteration 4 batch 12640 trainingloss 0.6931471805599453
iteration 4 batch 12650 trainingloss 0.6931471805599453
iteration 4 batch 12660 trainingloss 0.6931471805599453
iteration 4 batch 12670 trainingloss 0.6931471805599453
iteration 4 batch 12680 trainingloss 0.6916632528527511
iteration 4 batch 12690 trainingloss 0.6931471805599453
iteration 4 batch 12700 trainingloss 0.6931471805599453
iteration 4 batch 12710 trainingloss 0.6931471805599453
iteration 4 batch 12720 trainingloss 0.6931471805599453
iteration 4 batch 12730 trainingloss 0.6931471805599453
iteration 4 batch 12740 trainingloss 0.6931471805599453
iteration 4 batch 12750 trainingloss 0.6931471805599453
iteration 4 batch 12760 trainingloss 0.6931471805599453
iteration 4 batch 12770 trainingloss 0.6931471805599453
iteration 4 batch 12780 trainingloss 0.6931471805599453
iteration 4 batch 12790 trainingloss 0.6931471805599453
iteration 4 batch 12800 trainingloss 0.6916632528527511
iteration 4 batch 12810 trainingloss 0.6931471805599453
iteration 4 batch 12820 trainingloss 0.6931471805599453
iteration 4 batch 12830 trainingloss 0.6931471805599453
iteration 4 batch 12840 trainingloss 0.6931471805599453
iteration 4 batch 12850 trainingloss 0.6931471805599453
iteration 4 batch 12860 trainingloss 0.6931471805599453
iteration 4 batch 12870 trainingloss 0.6931471805599453
iteration 4 batch 12880 trainingloss 0.6931471805599453
iteration 4 batch 12890 trainingloss 0.6931471805599453
iteration 4 batch 12900 trainingloss 0.6931471805599453
iteration 4 batch 12910 trainingloss 0.6931471805599453
iteration 4 batch 12920 trainingloss 0.6931471805599453
iteration 4 batch 12930 trainingloss 0.6931471805599453
iteration 4 batch 12940 trainingloss 0.6931471805599453
iteration 4 batch 12950 trainingloss 0.6916632528527511
iteration 4 batch 12960 trainingloss 0.6931471805599453
iteration 4 batch 12970 trainingloss 0.6931471805599453
iteration 4 batch 12980 trainingloss 0.6931471805599453
iteration 4 batch 12990 trainingloss 0.6931471805599453
iteration 4 batch 13000 trainingloss 0.6931471805599453
iteration 4 batch 13010 trainingloss 0.6931471805599453
iteration 4 batch 13020 trainingloss 0.6931471805599453
iteration 4 batch 13030 trainingloss 0.6931471805599453
iteration 4 batch 13040 trainingloss 0.6931471805599453
iteration 4 batch 13050 trainingloss 0.6931471805599453
iteration 4 batch 13060 trainingloss 0.6931471805599453
iteration 4 batch 13070 trainingloss 0.6931471805599453
iteration 4 batch 13080 trainingloss 0.6931471805599453
iteration 4 batch 13090 trainingloss 0.6931471805599453
iteration 4 batch 13100 trainingloss 0.6931471805599453
iteration 4 batch 13110 trainingloss 0.6931471805599453
iteration 4 batch 13120 trainingloss 0.6931471805599453
iteration 4 batch 13130 trainingloss 0.6931471805599453
iteration 4 batch 13140 trainingloss 0.6931471805599453
iteration 4 batch 13150 trainingloss 0.6931471805599453
iteration 4 batch 13160 trainingloss 0.6931471805599453
iteration 4 batch 13170 trainingloss 0.6931471805599453
iteration 4 batch 13180 trainingloss 0.6931471805599453
iteration 4 batch 13190 trainingloss 0.6931471805599453
iteration 4 batch 13200 trainingloss 0.6931471805599453
iteration 4 batch 13210 trainingloss 0.6901793251455568
iteration 4 batch 13220 trainingloss 0.6931471805599453
iteration 4 batch 13230 trainingloss 0.6931471805599453
iteration 4 batch 13240 trainingloss 0.6931471805599453
iteration 4 batch 13250 trainingloss 0.6931471805599453
iteration 4 batch 13260 trainingloss 0.6931471805599453
iteration 4 batch 13270 trainingloss 0.6931471805599453
iteration 4 batch 13280 trainingloss 0.6931471805599453
iteration 4 batch 13290 trainingloss 0.6931471805599453
iteration 4 batch 13300 trainingloss 0.6931471805599453
iteration 4 batch 13310 trainingloss 0.6931471805599453
iteration 4 batch 13320 trainingloss 0.6916632528527511
iteration 4 batch 13330 trainingloss 0.6931471805599453
iteration 4 batch 13340 trainingloss 0.6931471805599453
iteration 4 batch 13350 trainingloss 0.6931471805599453
iteration 4 batch 13360 trainingloss 0.6931471805599453
iteration 4 batch 13370 trainingloss 0.6931471805599453
iteration 4 batch 13380 trainingloss 0.6931471805599453
iteration 4 batch 13390 trainingloss 0.6931471805599453
iteration 4 batch 13400 trainingloss 0.6901793251455568
iteration 4 batch 13410 trainingloss 0.6931471805599453
iteration 4 batch 13420 trainingloss 0.6931471805599453
iteration 4 batch 13430 trainingloss 0.6931471805599453
iteration 4 batch 13440 trainingloss 0.6916632528527511
iteration 4 batch 13450 trainingloss 0.6931471805599453
iteration 4 batch 13460 trainingloss 0.6931471805599453
iteration 4 batch 13470 trainingloss 0.6931471805599453
iteration 4 batch 13480 trainingloss 0.6931471805599453
iteration 4 batch 13490 trainingloss 0.6931471805599453
iteration 4 batch 13500 trainingloss 0.6931471805599453
iteration 4 batch 13510 trainingloss 0.6931471805599453
iteration 4 batch 13520 trainingloss 0.6931471805599453
iteration 4 batch 13530 trainingloss 0.6931471805599453
iteration 4 batch 13540 trainingloss 0.6931471805599453
iteration 4 batch 13550 trainingloss 0.6931471805599453
iteration 4 batch 13560 trainingloss 0.6931471805599453
iteration 4 batch 13570 trainingloss 0.6931471805599453
iteration 4 batch 13580 trainingloss 0.6931471805599453
iteration 4 batch 13590 trainingloss 0.6931471805599453
iteration 4 batch 13600 trainingloss 0.6931471805599453
iteration 4 batch 13610 trainingloss 0.6916632528527511
iteration 4 batch 13620 trainingloss 0.6916632528527511
iteration 4 batch 13630 trainingloss 0.6931471805599453
iteration 4 batch 13640 trainingloss 0.6931471805599453
iteration 4 batch 13650 trainingloss 0.6931471805599453
iteration 4 batch 13660 trainingloss 0.6931471805599453
iteration 4 batch 13670 trainingloss 0.6931471805599453
iteration 4 batch 13680 trainingloss 0.6931471805599453
iteration 4 batch 13690 trainingloss 0.6931471805599453
iteration 4 batch 13700 trainingloss 0.6931471805599453
iteration 4 batch 13710 trainingloss 0.6931471805599453
iteration 4 batch 13720 trainingloss 0.6931471805599453
iteration 4 batch 13730 trainingloss 0.6931471805599453
iteration 4 batch 13740 trainingloss 0.6931471805599453
iteration 4 batch 13750 trainingloss 0.6916632528527511
iteration 4 batch 13760 trainingloss 0.6931471805599453
iteration 4 batch 13770 trainingloss 0.6931471805599453
iteration 4 batch 13780 trainingloss 0.6931471805599453
iteration 4 batch 13790 trainingloss 0.6931471805599453
iteration 4 batch 13800 trainingloss 0.6931471805599453
iteration 4 batch 13810 trainingloss 0.6931471805599453
iteration 4 batch 13820 trainingloss 0.6931471805599453
iteration 4 batch 13830 trainingloss 0.6916632528527511
iteration 4 batch 13840 trainingloss 0.6931471805599453
iteration 4 batch 13850 trainingloss 0.6916632528527511
iteration 4 batch 13860 trainingloss 0.6931471805599453
iteration 4 batch 13870 trainingloss 0.6916632528527511
iteration 4 batch 13880 trainingloss 0.6931471805599453
iteration 4 batch 13890 trainingloss 0.6931471805599453
iteration 4 batch 13900 trainingloss 0.6931471805599453
iteration 4 batch 13910 trainingloss 0.6931471805599453
iteration 4 batch 13920 trainingloss 0.6931471805599453
iteration 4 batch 13930 trainingloss 0.6916632528527511
iteration 4 batch 13940 trainingloss 0.6931471805599453
iteration 4 batch 13950 trainingloss 0.6931471805599453
iteration 4 batch 13960 trainingloss 0.6931471805599453
iteration 4 batch 13970 trainingloss 0.6931471805599453
iteration 4 batch 13980 trainingloss 0.6931471805599453
iteration 4 batch 13990 trainingloss 0.6931471805599453
iteration 4 batch 14000 trainingloss 0.6931471805599453
iteration 4 batch 14010 trainingloss 0.6931471805599453
iteration 4 batch 14020 trainingloss 0.6931471805599453
iteration 4 batch 14030 trainingloss 0.6931471805599453
iteration 4 batch 14040 trainingloss 0.6931471805599453
iteration 4 batch 14050 trainingloss 0.6916632528527511
iteration 4 batch 14060 trainingloss 0.6916632528527511
iteration 4 batch 14070 trainingloss 0.6931471805599453
iteration 4 batch 14080 trainingloss 0.6931471805599453
iteration 4 batch 14090 trainingloss 0.6931471805599453
iteration 4 batch 14100 trainingloss 0.6931471805599453
iteration 4 batch 14110 trainingloss 0.6931471805599453
iteration 4 batch 14120 trainingloss 0.6931471805599453
iteration 4 batch 14130 trainingloss 0.6931471805599453
iteration 4 batch 14140 trainingloss 0.6931471805599453
iteration 4 batch 14150 trainingloss 0.6916632528527511
iteration 4 batch 14160 trainingloss 0.6931471805599453
iteration 4 batch 14170 trainingloss 0.6931471805599453
iteration 4 batch 14180 trainingloss 0.6931471805599453
iteration 4 batch 14190 trainingloss 0.6931471805599453
iteration 4 batch 14200 trainingloss 0.6931471805599453
iteration 4 batch 14210 trainingloss 0.6931471805599453
iteration 4 batch 14220 trainingloss 0.6931471805599453
iteration 4 batch 14230 trainingloss 0.6931471805599453
iteration 4 batch 14240 trainingloss 0.6931471805599453
iteration 4 batch 14250 trainingloss 0.6931471805599453
iteration 4 batch 14260 trainingloss 0.6931471805599453
iteration 4 batch 14270 trainingloss 0.6931471805599453
iteration 4 batch 14280 trainingloss 0.6931471805599453
iteration 4 batch 14290 trainingloss 0.6931471805599453
iteration 4 batch 14300 trainingloss 0.6931471805599453
iteration 4 batch 14310 trainingloss 0.6931471805599453
iteration 4 batch 14320 trainingloss 0.6931471805599453
iteration 4 batch 14330 trainingloss 0.6931471805599453
iteration 4 batch 14340 trainingloss 0.6931471805599453
iteration 4 batch 14350 trainingloss 0.6916632528527511
iteration 4 batch 14360 trainingloss 0.6931471805599453
iteration 4 batch 14370 trainingloss 0.6931471805599453
iteration 4 batch 14380 trainingloss 0.6931471805599453
iteration 4 batch 14390 trainingloss 0.6931471805599453
iteration 4 batch 14400 trainingloss 0.6931471805599453
iteration 4 batch 14410 trainingloss 0.6931471805599453
iteration 4 batch 14420 trainingloss 0.6931471805599453
iteration 4 batch 14430 trainingloss 0.6931471805599453
iteration 4 batch 14440 trainingloss 0.6931471805599453
iteration 4 batch 14450 trainingloss 0.6931471805599453
iteration 4 batch 14460 trainingloss 0.6931471805599453
iteration 4 batch 14470 trainingloss 0.6931471805599453
iteration 4 batch 14480 trainingloss 0.6931471805599453
iteration 4 batch 14490 trainingloss 0.6931471805599453
iteration 4 batch 14500 trainingloss 0.6931471805599453
iteration 4 batch 14510 trainingloss 0.6931471805599453
iteration 4 batch 14520 trainingloss 0.6931471805599453
iteration 4 batch 14530 trainingloss 0.6931471805599453
iteration 4 batch 14540 trainingloss 0.6931471805599453
iteration 4 batch 14550 trainingloss 0.6931471805599453
iteration 4 batch 14560 trainingloss 0.6931471805599453
iteration 4 batch 14570 trainingloss 0.6931471805599453
iteration 4 batch 14580 trainingloss 0.6916632528527511
iteration 4 batch 14590 trainingloss 0.6931471805599453
iteration 4 batch 14600 trainingloss 0.6931471805599453
iteration 4 batch 14610 trainingloss 0.6931471805599453
iteration 4 batch 14620 trainingloss 0.6931471805599453
iteration 4 batch 14630 trainingloss 0.6931471805599453
iteration 4 batch 14640 trainingloss 0.6931471805599453
iteration 4 batch 14650 trainingloss 0.6931471805599453
iteration 4 batch 14660 trainingloss 0.6931471805599453
iteration 4 batch 14670 trainingloss 0.6931471805599453
iteration 4 batch 14680 trainingloss 0.6931471805599453
iteration 4 batch 14690 trainingloss 0.6931471805599453
iteration 4 batch 14700 trainingloss 0.6931471805599453
iteration 4 batch 14710 trainingloss 0.6931471805599453
iteration 4 batch 14720 trainingloss 0.6931471805599453
iteration 4 batch 14730 trainingloss 0.6931471805599453
iteration 4 batch 14740 trainingloss 0.6931471805599453
iteration 4 batch 14750 trainingloss 0.6931471805599453
iteration 4 batch 14760 trainingloss 0.6931471805599453
iteration 4 batch 14770 trainingloss 0.6931471805599453
iteration 4 batch 14780 trainingloss 0.6931471805599453
iteration 4 batch 14790 trainingloss 0.6931471805599453
iteration 4 batch 14800 trainingloss 0.6931471805599453
iteration 4 batch 14810 trainingloss 0.6931471805599453
iteration 4 batch 14820 trainingloss 0.6931471805599453
iteration 4 batch 14830 trainingloss 0.6931471805599453
iteration 4 batch 14840 trainingloss 0.6931471805599453
iteration 4 batch 14850 trainingloss 0.6931471805599453
iteration 4 batch 14860 trainingloss 0.6931471805599453
iteration 4 batch 14870 trainingloss 0.6931471805599453
iteration 4 batch 14880 trainingloss 0.6931471805599453
iteration 4 batch 14890 trainingloss 0.6931471805599453
iteration 4 batch 14900 trainingloss 0.6931471805599453
iteration 4 batch 14910 trainingloss 0.6931471805599453
iteration 4 batch 14920 trainingloss 0.6931471805599453
iteration 4 batch 14930 trainingloss 0.6916632528527511
iteration 4 batch 14940 trainingloss 0.6931471805599453
iteration 4 batch 14950 trainingloss 0.6931471805599453
iteration 4 batch 14960 trainingloss 0.6931471805599453
iteration 4 batch 14970 trainingloss 0.6931471805599453
iteration 4 batch 14980 trainingloss 0.6916632528527511
iteration 4 batch 14990 trainingloss 0.6931471805599453
iteration 4 batch 15000 trainingloss 0.6931471805599453
iteration 4 batch 15010 trainingloss 0.6931471805599453
iteration 4 batch 15020 trainingloss 0.6931471805599453
iteration 4 batch 15030 trainingloss 0.6931471805599453
iteration 4 batch 15040 trainingloss 0.6931471805599453
iteration 4 batch 15050 trainingloss 0.6931471805599453
iteration 4 batch 15060 trainingloss 0.6931471805599453
iteration 4 batch 15070 trainingloss 0.6931471805599453
iteration 4 batch 15080 trainingloss 0.6931471805599453
iteration 4 batch 15090 trainingloss 0.6931471805599453
iteration 4 batch 15100 trainingloss 0.6916632528527511
iteration 4 batch 15110 trainingloss 0.6931471805599453
iteration 4 batch 15120 trainingloss 0.6931471805599453
iteration 4 batch 15130 trainingloss 0.6931471805599453
iteration 4 batch 15140 trainingloss 0.6931471805599453
iteration 4 batch 15150 trainingloss 0.6916632528527511
iteration 4 batch 15160 trainingloss 0.6931471805599453
iteration 4 batch 15170 trainingloss 0.6931471805599453
iteration 4 batch 15180 trainingloss 0.6931471805599453
iteration 4 batch 15190 trainingloss 0.6931471805599453
iteration 4 batch 15200 trainingloss 0.6931471805599453
iteration 4 batch 15210 trainingloss 0.6931471805599453
iteration 4 batch 15220 trainingloss 0.6931471805599453
iteration 4 batch 15230 trainingloss 0.6931471805599453
iteration 4 batch 15240 trainingloss 0.6931471805599453
iteration 4 batch 15250 trainingloss 0.6931471805599453
iteration 4 batch 15260 trainingloss 0.6931471805599453
iteration 4 batch 15270 trainingloss 0.6931471805599453
iteration 4 batch 15280 trainingloss 0.6931471805599453
iteration 4 batch 15290 trainingloss 0.6931471805599453
iteration 4 batch 15300 trainingloss 0.6931471805599453
iteration 4 batch 15310 trainingloss 0.6931471805599453
iteration 4 batch 15320 trainingloss 0.6931471805599453
iteration 4 batch 15330 trainingloss 0.6931471805599453
iteration 4 batch 15340 trainingloss 0.6931471805599453
iteration 4 batch 15350 trainingloss 0.6931471805599453
iteration 4 batch 15360 trainingloss 0.6931471805599453
iteration 4 batch 15370 trainingloss 0.6916632528527511
iteration 4 batch 15380 trainingloss 0.6931471805599453
iteration 4 batch 15390 trainingloss 0.6916632528527511
iteration 4 batch 15400 trainingloss 0.6931471805599453
iteration 4 batch 15410 trainingloss 0.6931471805599453
iteration 4 batch 15420 trainingloss 0.6931471805599453
iteration 4 batch 15430 trainingloss 0.6931471805599453
iteration 4 batch 15440 trainingloss 0.6931471805599453
iteration 4 batch 15450 trainingloss 0.6931471805599453
iteration 4 batch 15460 trainingloss 0.6931471805599453
iteration 4 batch 15470 trainingloss 0.6931471805599453
iteration 4 batch 15480 trainingloss 0.6931471805599453
iteration 4 batch 15490 trainingloss 0.6931471805599453
iteration 4 batch 15500 trainingloss 0.6931471805599453
iteration 4 batch 15510 trainingloss 0.6931471805599453
iteration 4 batch 15520 trainingloss 0.6931471805599453
iteration 4 batch 15530 trainingloss 0.6931471805599453
iteration 4 batch 15540 trainingloss 0.6916632528527511
iteration 4 batch 15550 trainingloss 0.6931471805599453
iteration 4 batch 15560 trainingloss 0.6931471805599453
iteration 4 batch 15570 trainingloss 0.6931471805599453
iteration 4 batch 15580 trainingloss 0.6931471805599453
iteration 4 batch 15590 trainingloss 0.6931471805599453
iteration 4 batch 15600 trainingloss 0.6931471805599453
iteration 4 batch 15610 trainingloss 0.6931471805599453
iteration 4 batch 15620 trainingloss 0.6931471805599453
iteration 4 batch 15630 trainingloss 0.6931471805599453
iteration 4 batch 15640 trainingloss 0.6931471805599453
iteration 4 batch 15650 trainingloss 0.6931471805599453
iteration 4 batch 15660 trainingloss 0.6931471805599453
iteration 4 batch 15670 trainingloss 0.6916632528527511
iteration 4 batch 15680 trainingloss 0.6931471805599453
iteration 4 batch 15690 trainingloss 0.6931471805599453
iteration 4 batch 15700 trainingloss 0.6931471805599453
iteration 4 batch 15710 trainingloss 0.6931471805599453
iteration 4 batch 15720 trainingloss 0.6931471805599453
iteration 4 batch 15730 trainingloss 0.6931471805599453
iteration 4 batch 15740 trainingloss 0.6931471805599453
iteration 4 batch 15750 trainingloss 0.6931471805599453
iteration 4 batch 15760 trainingloss 0.6931471805599453
iteration 4 batch 15770 trainingloss 0.6931471805599453
iteration 4 batch 15780 trainingloss 0.6931471805599453
iteration 4 batch 15790 trainingloss 0.6931471805599453
iteration 4 batch 15800 trainingloss 0.6931471805599453
iteration 4 batch 15810 trainingloss 0.6931471805599453
iteration 4 batch 15820 trainingloss 0.6931471805599453
iteration 4 batch 15830 trainingloss 0.6931471805599453
iteration 4 batch 15840 trainingloss 0.6931471805599453
iteration 4 batch 15850 trainingloss 0.6931471805599453
iteration 4 batch 15860 trainingloss 0.6931471805599453
iteration 4 batch 15870 trainingloss 0.6931471805599453
iteration 4 batch 15880 trainingloss 0.6931471805599453
iteration 4 batch 15890 trainingloss 0.6931471805599453
iteration 4 batch 15900 trainingloss 0.6931471805599453
iteration 4 batch 15910 trainingloss 0.6931471805599453
iteration 4 batch 15920 trainingloss 0.6931471805599453
iteration 4 batch 15930 trainingloss 0.6931471805599453
iteration 4 batch 15940 trainingloss 0.6931471805599453
iteration 4 batch 15950 trainingloss 0.6931471805599453
iteration 4 batch 15960 trainingloss 0.6931471805599453
iteration 4 batch 15970 trainingloss 0.6931471805599453
iteration 4 batch 15980 trainingloss 0.6931471805599453
iteration 4 batch 15990 trainingloss 0.6931471805599453
iteration 4 batch 16000 trainingloss 0.6931471805599453
iteration 4 batch 16010 trainingloss 0.6931471805599453
iteration 4 batch 16020 trainingloss 0.6931471805599453
iteration 4 batch 16030 trainingloss 0.6931471805599453
iteration 4 batch 16040 trainingloss 0.6931471805599453
iteration 4 batch 16050 trainingloss 0.6931471805599453
iteration 4 batch 16060 trainingloss 0.6931471805599453
iteration 4 batch 16070 trainingloss 0.6931471805599453
iteration 4 batch 16080 trainingloss 0.6931471805599453
iteration 4 batch 16090 trainingloss 0.6931471805599453
iteration 4 batch 16100 trainingloss 0.6931471805599453
iteration 4 batch 16110 trainingloss 0.6916632528527511
iteration 4 batch 16120 trainingloss 0.6916632528527511
iteration 4 batch 16130 trainingloss 0.6931471805599453
iteration 4 batch 16140 trainingloss 0.6931471805599453
iteration 4 batch 16150 trainingloss 0.6931471805599453
iteration 4 batch 16160 trainingloss 0.6931471805599453
iteration 4 batch 16170 trainingloss 0.6931471805599453
iteration 4 batch 16180 trainingloss 0.6931471805599453
iteration 4 batch 16190 trainingloss 0.6931471805599453
iteration 4 batch 16200 trainingloss 0.6931471805599453
iteration 4 batch 16210 trainingloss 0.6916632528527511
iteration 4 batch 16220 trainingloss 0.6931471805599453
iteration 4 batch 16230 trainingloss 0.6931471805599453
iteration 4 batch 16240 trainingloss 0.6931471805599453
iteration 4 batch 16250 trainingloss 0.6931471805599453
iteration 4 batch 16260 trainingloss 0.6931471805599453
iteration 4 batch 16270 trainingloss 0.6931471805599453
iteration 4 batch 16280 trainingloss 0.6931471805599453
iteration 4 batch 16290 trainingloss 0.6931471805599453
iteration 4 batch 16300 trainingloss 0.6931471805599453
iteration 4 batch 16310 trainingloss 0.6931471805599453
iteration 4 batch 16320 trainingloss 0.6931471805599453
iteration 4 batch 16330 trainingloss 0.6931471805599453
iteration 4 batch 16340 trainingloss 0.6916632528527511
iteration 4 batch 16350 trainingloss 0.6931471805599453
iteration 4 batch 16360 trainingloss 0.6931471805599453
iteration 4 batch 16370 trainingloss 0.6931471805599453
iteration 4 batch 16380 trainingloss 0.6931471805599453
iteration 4 batch 16390 trainingloss 0.6916632528527511
iteration 4 batch 16400 trainingloss 0.6931471805599453
iteration 4 batch 16410 trainingloss 0.6931471805599453
iteration 4 batch 16420 trainingloss 0.6931471805599453
iteration 4 batch 16430 trainingloss 0.6931471805599453
iteration 4 batch 16440 trainingloss 0.6931471805599453
iteration 4 batch 16450 trainingloss 0.6931471805599453
iteration 4 batch 16460 trainingloss 0.6931471805599453
iteration 4 batch 16470 trainingloss 0.6931471805599453
iteration 4 batch 16480 trainingloss 0.6931471805599453
iteration 4 batch 16490 trainingloss 0.6931471805599453
iteration 4 batch 16500 trainingloss 0.6931471805599453
iteration 4 batch 16510 trainingloss 0.6931471805599453
iteration 4 batch 16520 trainingloss 0.6931471805599453
iteration 4 batch 16530 trainingloss 0.6931471805599453
iteration 4 batch 16540 trainingloss 0.6931471805599453
iteration 4 batch 16550 trainingloss 0.6931471805599453
iteration 4 batch 16560 trainingloss 0.6916632528527511
iteration 4 batch 16570 trainingloss 0.6931471805599453
iteration 4 batch 16580 trainingloss 0.6931471805599453
iteration 4 batch 16590 trainingloss 0.6931471805599453
iteration 4 batch 16600 trainingloss 0.6931471805599453
iteration 4 batch 16610 trainingloss 0.6931471805599453
iteration 4 batch 16620 trainingloss 0.6931471805599453
iteration 4 batch 16630 trainingloss 0.6931471805599453
iteration 4 batch 16640 trainingloss 0.6916632528527511
iteration 4 batch 16650 trainingloss 0.6931471805599453
iteration 4 batch 16660 trainingloss 0.6931471805599453
iteration 4 batch 16670 trainingloss 0.6931471805599453
iteration 4 batch 16680 trainingloss 0.6931471805599453
iteration 4 batch 16690 trainingloss 0.6931471805599453
iteration 4 batch 16700 trainingloss 0.6931471805599453
iteration 4 batch 16710 trainingloss 0.6931471805599453
iteration 4 batch 16720 trainingloss 0.6931471805599453
iteration 4 batch 16730 trainingloss 0.6931471805599453
iteration 4 batch 16740 trainingloss 0.6931471805599453
iteration 4 batch 16750 trainingloss 0.6931471805599453
iteration 4 batch 16760 trainingloss 0.6931471805599453
iteration 4 batch 16770 trainingloss 0.6931471805599453
iteration 4 batch 16780 trainingloss 0.6931471805599453
iteration 4 batch 16790 trainingloss 0.6931471805599453
iteration 4 batch 16800 trainingloss 0.6931471805599453
iteration 4 batch 16810 trainingloss 0.6931471805599453
iteration 4 batch 16820 trainingloss 0.6931471805599453
iteration 4 batch 16830 trainingloss 0.6931471805599453
iteration 4 batch 16840 trainingloss 0.6931471805599453
iteration 4 batch 16850 trainingloss 0.6931471805599453
iteration 4 batch 16860 trainingloss 0.6931471805599453
iteration 4 batch 16870 trainingloss 0.6931471805599453
iteration 4 batch 16880 trainingloss 0.6931471805599453
iteration 4 batch 16890 trainingloss 0.6931471805599453
iteration 4 batch 16900 trainingloss 0.6916632528527511
iteration 4 batch 16910 trainingloss 0.6931471805599453
iteration 4 batch 16920 trainingloss 0.6931471805599453
iteration 4 batch 16930 trainingloss 0.6931471805599453
iteration 4 batch 16940 trainingloss 0.6931471805599453
iteration 4 batch 16950 trainingloss 0.6931471805599453
iteration 4 batch 16960 trainingloss 0.6931471805599453
iteration 4 batch 16970 trainingloss 0.6931471805599453
iteration 4 batch 16980 trainingloss 0.6931471805599453
iteration 4 batch 16990 trainingloss 0.6916632528527511
iteration 4 batch 17000 trainingloss 0.6931471805599453
iteration 4 batch 17010 trainingloss 0.6931471805599453
iteration 4 batch 17020 trainingloss 0.6931471805599453
iteration 4 batch 17030 trainingloss 0.6931471805599453
iteration 4 batch 17040 trainingloss 0.6931471805599453
iteration 4 batch 17050 trainingloss 0.6916632528527511
iteration 4 batch 17060 trainingloss 0.6931471805599453
iteration 4 batch 17070 trainingloss 0.6931471805599453
iteration 4 batch 17080 trainingloss 0.6931471805599453
iteration 4 batch 17090 trainingloss 0.6931471805599453
iteration 4 batch 17100 trainingloss 0.6931471805599453
iteration 4 batch 17110 trainingloss 0.6931471805599453
iteration 4 batch 17120 trainingloss 0.6931471805599453
iteration 4 batch 17130 trainingloss 0.6931471805599453
iteration 4 batch 17140 trainingloss 0.6931471805599453
iteration 4 batch 17150 trainingloss 0.6931471805599453
iteration 4 batch 17160 trainingloss 0.6931471805599453
iteration 4 batch 17170 trainingloss 0.6931471805599453
iteration 4 batch 17180 trainingloss 0.6931471805599453
iteration 4 batch 17190 trainingloss 0.6931471805599453
iteration 4 batch 17200 trainingloss 0.6931471805599453
iteration 4 batch 17210 trainingloss 0.6931471805599453
iteration 4 batch 17220 trainingloss 0.6931471805599453
iteration 4 batch 17230 trainingloss 0.6931471805599453
iteration 4 batch 17240 trainingloss 0.6931471805599453
iteration 4 batch 17250 trainingloss 0.6916632528527511
iteration 4 batch 17260 trainingloss 0.6931471805599453
iteration 4 batch 17270 trainingloss 0.6931471805599453
iteration 4 batch 17280 trainingloss 0.6931471805599453
iteration 4 batch 17290 trainingloss 0.6931471805599453
iteration 4 batch 17300 trainingloss 0.6931471805599453
iteration 4 batch 17310 trainingloss 0.6931471805599453
iteration 4 batch 17320 trainingloss 0.6931471805599453
iteration 4 batch 17330 trainingloss 0.6916632528527511
iteration 4 batch 17340 trainingloss 0.6931471805599453
iteration 4 batch 17350 trainingloss 0.6931471805599453
iteration 4 batch 17360 trainingloss 0.6931471805599453
iteration 4 batch 17370 trainingloss 0.6931471805599453
iteration 4 batch 17380 trainingloss 0.6931471805599453
iteration 4 batch 17390 trainingloss 0.6931471805599453
iteration 4 batch 17400 trainingloss 0.6931471805599453
iteration 4 batch 17410 trainingloss 0.6931471805599453
iteration 4 batch 17420 trainingloss 0.6931471805599453
iteration 4 batch 17430 trainingloss 0.6931471805599453
iteration 4 batch 17440 trainingloss 0.6931471805599453
iteration 4 batch 17450 trainingloss 0.6916632528527511
iteration 4 batch 17460 trainingloss 0.6931471805599453
iteration 4 batch 17470 trainingloss 0.6931471805599453
iteration 4 batch 17480 trainingloss 0.6931471805599453
iteration 4 batch 17490 trainingloss 0.6931471805599453
iteration 4 batch 17500 trainingloss 0.6916632528527511
iteration 4 batch 17510 trainingloss 0.6931471805599453
iteration 4 batch 17520 trainingloss 0.6931471805599453
iteration 4 batch 17530 trainingloss 0.6931471805599453
iteration 4 batch 17540 trainingloss 0.6931471805599453
iteration 4 batch 17550 trainingloss 0.6931471805599453
iteration 4 batch 17560 trainingloss 0.6931471805599453
iteration 4 batch 17570 trainingloss 0.6931471805599453
iteration 4 batch 17580 trainingloss 0.6931471805599453
iteration 4 batch 17590 trainingloss 0.6931471805599453
iteration 4 batch 17600 trainingloss 0.6931471805599453
iteration 4 batch 17610 trainingloss 0.6931471805599453
iteration 4 batch 17620 trainingloss 0.6931471805599453
iteration 4 batch 17630 trainingloss 0.6931471805599453
iteration 4 batch 17640 trainingloss 0.6931471805599453
iteration 4 batch 17650 trainingloss 0.6931471805599453
iteration 4 batch 17660 trainingloss 0.6931471805599453
iteration 4 batch 17670 trainingloss 0.6931471805599453
iteration 4 batch 17680 trainingloss 0.6931471805599453
iteration 4 batch 17690 trainingloss 0.6931471805599453
iteration 4 batch 17700 trainingloss 0.6931471805599453
iteration 4 batch 17710 trainingloss 0.6931471805599453
iteration 4 batch 17720 trainingloss 0.6931471805599453
iteration 4 batch 17730 trainingloss 0.6916632528527511
iteration 4 batch 17740 trainingloss 0.6931471805599453
iteration 4 batch 17750 trainingloss 0.6931471805599453
iteration 4 batch 17760 trainingloss 0.6931471805599453
iteration 4 batch 17770 trainingloss 0.6931471805599453
iteration 4 batch 17780 trainingloss 0.6931471805599453
iteration 4 batch 17790 trainingloss 0.6931471805599453
iteration 4 batch 17800 trainingloss 0.6931471805599453
iteration 4 batch 17810 trainingloss 0.6931471805599453
iteration 4 batch 17820 trainingloss 0.6931471805599453
iteration 4 batch 17830 trainingloss 0.6916632528527511
iteration 4 batch 17840 trainingloss 0.6931471805599453
iteration 4 batch 17850 trainingloss 0.6931471805599453
iteration 4 batch 17860 trainingloss 0.6931471805599453
iteration 4 batch 17870 trainingloss 0.6931471805599453
iteration 4 batch 17880 trainingloss 0.6931471805599453
iteration 4 batch 17890 trainingloss 0.6931471805599453
iteration 4 batch 17900 trainingloss 0.6931471805599453
iteration 4 batch 17910 trainingloss 0.6931471805599453
iteration 4 batch 17920 trainingloss 0.6931471805599453
iteration 4 batch 17930 trainingloss 0.6931471805599453
iteration 4 batch 17940 trainingloss 0.6931471805599453
iteration 4 batch 17950 trainingloss 0.6931471805599453
iteration 4 batch 17960 trainingloss 0.6931471805599453
iteration 4 batch 17970 trainingloss 0.6931471805599453
iteration 4 batch 17980 trainingloss 0.6931471805599453
iteration 4 batch 17990 trainingloss 0.6931471805599453
iteration 4 batch 18000 trainingloss 0.6931471805599453
iteration 4 batch 18010 trainingloss 0.6931471805599453
iteration 4 batch 18020 trainingloss 0.6931471805599453
iteration 4 batch 18030 trainingloss 0.6931471805599453
iteration 4 batch 18040 trainingloss 0.6931471805599453
iteration 4 batch 18050 trainingloss 0.6931471805599453
iteration 4 batch 18060 trainingloss 0.6931471805599453
iteration 4 batch 18070 trainingloss 0.6931471805599453
iteration 4 batch 18080 trainingloss 0.6931471805599453
iteration 4 batch 18090 trainingloss 0.6931471805599453
iteration 4 batch 18100 trainingloss 0.6931471805599453
iteration 4 batch 18110 trainingloss 0.6931471805599453
iteration 4 batch 18120 trainingloss 0.6931471805599453
iteration 4 batch 18130 trainingloss 0.6931471805599453
iteration 4 batch 18140 trainingloss 0.6931471805599453
iteration 4 batch 18150 trainingloss 0.6916632528527511
iteration 4 batch 18160 trainingloss 0.6931471805599453
iteration 4 batch 18170 trainingloss 0.6931471805599453
iteration 4 batch 18180 trainingloss 0.6916632528527511
iteration 4 batch 18190 trainingloss 0.6931471805599453
iteration 4 batch 18200 trainingloss 0.6931471805599453
iteration 4 batch 18210 trainingloss 0.6931471805599453
iteration 4 batch 18220 trainingloss 0.6931471805599453
iteration 4 batch 18230 trainingloss 0.6931471805599453
iteration 4 batch 18240 trainingloss 0.6931471805599453
iteration 4 batch 18250 trainingloss 0.6931471805599453
iteration 4 batch 18260 trainingloss 0.6931471805599453
iteration 4 batch 18270 trainingloss 0.6931471805599453
iteration 4 batch 18280 trainingloss 0.6931471805599453
iteration 4 batch 18290 trainingloss 0.6931471805599453
iteration 4 batch 18300 trainingloss 0.6931471805599453
iteration 4 batch 18310 trainingloss 0.6931471805599453
iteration 4 batch 18320 trainingloss 0.6931471805599453
iteration 4 batch 18330 trainingloss 0.6931471805599453
iteration 4 batch 18340 trainingloss 0.6916632528527511
iteration 4 batch 18350 trainingloss 0.6931471805599453
iteration 4 batch 18360 trainingloss 0.6931471805599453
iteration 4 batch 18370 trainingloss 0.6931471805599453
iteration 4 batch 18380 trainingloss 0.6931471805599453
iteration 4 batch 18390 trainingloss 0.6901793251455568
iteration 4 batch 18400 trainingloss 0.6931471805599453
iteration 4 batch 18410 trainingloss 0.6931471805599453
iteration 4 batch 18420 trainingloss 0.6931471805599453
iteration 4 batch 18430 trainingloss 0.6931471805599453
iteration 4 batch 18440 trainingloss 0.6931471805599453
iteration 4 batch 18450 trainingloss 0.6931471805599453
iteration 4 batch 18460 trainingloss 0.6931471805599453
iteration 4 batch 18470 trainingloss 0.6931471805599453
iteration 4 batch 18480 trainingloss 0.6931471805599453
iteration 4 batch 18490 trainingloss 0.6931471805599453
iteration 4 batch 18500 trainingloss 0.6931471805599453
iteration 4 batch 18510 trainingloss 0.6931471805599453
iteration 4 batch 18520 trainingloss 0.6931471805599453
iteration 4 batch 18530 trainingloss 0.6931471805599453
iteration 4 batch 18540 trainingloss 0.6931471805599453
iteration 4 batch 18550 trainingloss 0.6931471805599453
iteration 4 batch 18560 trainingloss 0.6931471805599453
iteration 4 batch 18570 trainingloss 0.6931471805599453
iteration 4 batch 18580 trainingloss 0.6931471805599453
iteration 4 batch 18590 trainingloss 0.6916632528527511
iteration 4 batch 18600 trainingloss 0.6931471805599453
iteration 4 batch 18610 trainingloss 0.6931471805599453
iteration 5 batch 0 trainingloss 0.6931471805599453
iteration 5 batch 10 trainingloss 0.6931471805599453
iteration 5 batch 20 trainingloss 0.6931471805599453
iteration 5 batch 30 trainingloss 0.6931471805599453
iteration 5 batch 40 trainingloss 0.6931471805599453
iteration 5 batch 50 trainingloss 0.6916632528527511
iteration 5 batch 60 trainingloss 0.6931471805599453
iteration 5 batch 70 trainingloss 0.6931471805599453
iteration 5 batch 80 trainingloss 0.6931471805599453
iteration 5 batch 90 trainingloss 0.6931471805599453
iteration 5 batch 100 trainingloss 0.6931471805599453
iteration 5 batch 110 trainingloss 0.6931471805599453
iteration 5 batch 120 trainingloss 0.6931471805599453
iteration 5 batch 130 trainingloss 0.6931471805599453
iteration 5 batch 140 trainingloss 0.6931471805599453
iteration 5 batch 150 trainingloss 0.6931471805599453
iteration 5 batch 160 trainingloss 0.6931471805599453
iteration 5 batch 170 trainingloss 0.6931471805599453
iteration 5 batch 180 trainingloss 0.6931471805599453
iteration 5 batch 190 trainingloss 0.6931471805599453
iteration 5 batch 200 trainingloss 0.6931471805599453
iteration 5 batch 210 trainingloss 0.6931471805599453
iteration 5 batch 220 trainingloss 0.6931471805599453
iteration 5 batch 230 trainingloss 0.6931471805599453
iteration 5 batch 240 trainingloss 0.6931471805599453
iteration 5 batch 250 trainingloss 0.6931471805599453
iteration 5 batch 260 trainingloss 0.6931471805599453
iteration 5 batch 270 trainingloss 0.6931471805599453
iteration 5 batch 280 trainingloss 0.6931471805599453
iteration 5 batch 290 trainingloss 0.6931471805599453
iteration 5 batch 300 trainingloss 0.6931471805599453
iteration 5 batch 310 trainingloss 0.6931471805599453
iteration 5 batch 320 trainingloss 0.6931471805599453
iteration 5 batch 330 trainingloss 0.6931471805599453
iteration 5 batch 340 trainingloss 0.6931471805599453
iteration 5 batch 350 trainingloss 0.6931471805599453
iteration 5 batch 360 trainingloss 0.6931471805599453
iteration 5 batch 370 trainingloss 0.6931471805599453
iteration 5 batch 380 trainingloss 0.6931471805599453
iteration 5 batch 390 trainingloss 0.6931471805599453
iteration 5 batch 400 trainingloss 0.6916632528527511
iteration 5 batch 410 trainingloss 0.6931471805599453
iteration 5 batch 420 trainingloss 0.6931471805599453
iteration 5 batch 430 trainingloss 0.6916632528527511
iteration 5 batch 440 trainingloss 0.6931471805599453
iteration 5 batch 450 trainingloss 0.6931471805599453
iteration 5 batch 460 trainingloss 0.6931471805599453
iteration 5 batch 470 trainingloss 0.6931471805599453
iteration 5 batch 480 trainingloss 0.6931471805599453
iteration 5 batch 490 trainingloss 0.6931471805599453
iteration 5 batch 500 trainingloss 0.6931471805599453
iteration 5 batch 510 trainingloss 0.6931471805599453
iteration 5 batch 520 trainingloss 0.6931471805599453
iteration 5 batch 530 trainingloss 0.6931471805599453
iteration 5 batch 540 trainingloss 0.6931471805599453
iteration 5 batch 550 trainingloss 0.6931471805599453
iteration 5 batch 560 trainingloss 0.6931471805599453
iteration 5 batch 570 trainingloss 0.6931471805599453
iteration 5 batch 580 trainingloss 0.6931471805599453
iteration 5 batch 590 trainingloss 0.6931471805599453
iteration 5 batch 600 trainingloss 0.6931471805599453
iteration 5 batch 610 trainingloss 0.6931471805599453
iteration 5 batch 620 trainingloss 0.6931471805599453
iteration 5 batch 630 trainingloss 0.6931471805599453
iteration 5 batch 640 trainingloss 0.6931471805599453
iteration 5 batch 650 trainingloss 0.6931471805599453
iteration 5 batch 660 trainingloss 0.6931471805599453
iteration 5 batch 670 trainingloss 0.6931471805599453
iteration 5 batch 680 trainingloss 0.6931471805599453
iteration 5 batch 690 trainingloss 0.6916632528527511
iteration 5 batch 700 trainingloss 0.6931471805599453
iteration 5 batch 710 trainingloss 0.6931471805599453
iteration 5 batch 720 trainingloss 0.6931471805599453
iteration 5 batch 730 trainingloss 0.6931471805599453
iteration 5 batch 740 trainingloss 0.6931471805599453
iteration 5 batch 750 trainingloss 0.6931471805599453
iteration 5 batch 760 trainingloss 0.6931471805599453
iteration 5 batch 770 trainingloss 0.6931471805599453
iteration 5 batch 780 trainingloss 0.6931471805599453
iteration 5 batch 790 trainingloss 0.6931471805599453
iteration 5 batch 800 trainingloss 0.6931471805599453
iteration 5 batch 810 trainingloss 0.6931471805599453
iteration 5 batch 820 trainingloss 0.6916632528527511
iteration 5 batch 830 trainingloss 0.6916632528527511
iteration 5 batch 840 trainingloss 0.6931471805599453
iteration 5 batch 850 trainingloss 0.6931471805599453
iteration 5 batch 860 trainingloss 0.6931471805599453
iteration 5 batch 870 trainingloss 0.6931471805599453
iteration 5 batch 880 trainingloss 0.6916632528527511
iteration 5 batch 890 trainingloss 0.6931471805599453
iteration 5 batch 900 trainingloss 0.6931471805599453
iteration 5 batch 910 trainingloss 0.6931471805599453
iteration 5 batch 920 trainingloss 0.6931471805599453
iteration 5 batch 930 trainingloss 0.6931471805599453
iteration 5 batch 940 trainingloss 0.6931471805599453
iteration 5 batch 950 trainingloss 0.6931471805599453
iteration 5 batch 960 trainingloss 0.6931471805599453
iteration 5 batch 970 trainingloss 0.6931471805599453
iteration 5 batch 980 trainingloss 0.6916632528527511
iteration 5 batch 990 trainingloss 0.6931471805599453
iteration 5 batch 1000 trainingloss 0.6931471805599453
iteration 5 batch 1010 trainingloss 0.6931471805599453
iteration 5 batch 1020 trainingloss 0.6931471805599453
iteration 5 batch 1030 trainingloss 0.6931471805599453
iteration 5 batch 1040 trainingloss 0.6931471805599453
iteration 5 batch 1050 trainingloss 0.6931471805599453
iteration 5 batch 1060 trainingloss 0.6931471805599453
iteration 5 batch 1070 trainingloss 0.6931471805599453
iteration 5 batch 1080 trainingloss 0.6931471805599453
iteration 5 batch 1090 trainingloss 0.6931471805599453
iteration 5 batch 1100 trainingloss 0.6931471805599453
iteration 5 batch 1110 trainingloss 0.6931471805599453
iteration 5 batch 1120 trainingloss 0.6931471805599453
iteration 5 batch 1130 trainingloss 0.6931471805599453
iteration 5 batch 1140 trainingloss 0.6931471805599453
iteration 5 batch 1150 trainingloss 0.6931471805599453
iteration 5 batch 1160 trainingloss 0.6931471805599453
iteration 5 batch 1170 trainingloss 0.6931471805599453
iteration 5 batch 1180 trainingloss 0.6931471805599453
iteration 5 batch 1190 trainingloss 0.6931471805599453
iteration 5 batch 1200 trainingloss 0.6931471805599453
iteration 5 batch 1210 trainingloss 0.6931471805599453
iteration 5 batch 1220 trainingloss 0.6931471805599453
iteration 5 batch 1230 trainingloss 0.6916632528527511
iteration 5 batch 1240 trainingloss 0.6931471805599453
iteration 5 batch 1250 trainingloss 0.6931471805599453
iteration 5 batch 1260 trainingloss 0.6931471805599453
iteration 5 batch 1270 trainingloss 0.6931471805599453
iteration 5 batch 1280 trainingloss 0.6931471805599453
iteration 5 batch 1290 trainingloss 0.6931471805599453
iteration 5 batch 1300 trainingloss 0.6931471805599453
iteration 5 batch 1310 trainingloss 0.6931471805599453
iteration 5 batch 1320 trainingloss 0.6931471805599453
iteration 5 batch 1330 trainingloss 0.6931471805599453
iteration 5 batch 1340 trainingloss 0.6931471805599453
iteration 5 batch 1350 trainingloss 0.6931471805599453
iteration 5 batch 1360 trainingloss 0.6931471805599453
iteration 5 batch 1370 trainingloss 0.6931471805599453
iteration 5 batch 1380 trainingloss 0.6931471805599453
iteration 5 batch 1390 trainingloss 0.6931471805599453
iteration 5 batch 1400 trainingloss 0.6931471805599453
iteration 5 batch 1410 trainingloss 0.6931471805599453
iteration 5 batch 1420 trainingloss 0.6931471805599453
iteration 5 batch 1430 trainingloss 0.6931471805599453
iteration 5 batch 1440 trainingloss 0.6931471805599453
iteration 5 batch 1450 trainingloss 0.6931471805599453
iteration 5 batch 1460 trainingloss 0.6931471805599453
iteration 5 batch 1470 trainingloss 0.6931471805599453
iteration 5 batch 1480 trainingloss 0.6931471805599453
iteration 5 batch 1490 trainingloss 0.6931471805599453
iteration 5 batch 1500 trainingloss 0.6931471805599453
iteration 5 batch 1510 trainingloss 0.6931471805599453
iteration 5 batch 1520 trainingloss 0.6931471805599453
iteration 5 batch 1530 trainingloss 0.6931471805599453
iteration 5 batch 1540 trainingloss 0.6931471805599453
iteration 5 batch 1550 trainingloss 0.6931471805599453
iteration 5 batch 1560 trainingloss 0.6931471805599453
iteration 5 batch 1570 trainingloss 0.6931471805599453
iteration 5 batch 1580 trainingloss 0.6931471805599453
iteration 5 batch 1590 trainingloss 0.6931471805599453
iteration 5 batch 1600 trainingloss 0.6931471805599453
iteration 5 batch 1610 trainingloss 0.6931471805599453
iteration 5 batch 1620 trainingloss 0.6931471805599453
iteration 5 batch 1630 trainingloss 0.6931471805599453
iteration 5 batch 1640 trainingloss 0.6931471805599453
iteration 5 batch 1650 trainingloss 0.6931471805599453
iteration 5 batch 1660 trainingloss 0.6931471805599453
iteration 5 batch 1670 trainingloss 0.6916632528527511
iteration 5 batch 1680 trainingloss 0.6931471805599453
iteration 5 batch 1690 trainingloss 0.6931471805599453
iteration 5 batch 1700 trainingloss 0.6931471805599453
iteration 5 batch 1710 trainingloss 0.6931471805599453
iteration 5 batch 1720 trainingloss 0.6931471805599453
iteration 5 batch 1730 trainingloss 0.6931471805599453
iteration 5 batch 1740 trainingloss 0.6916632528527511
iteration 5 batch 1750 trainingloss 0.6931471805599453
iteration 5 batch 1760 trainingloss 0.6931471805599453
iteration 5 batch 1770 trainingloss 0.6931471805599453
iteration 5 batch 1780 trainingloss 0.6931471805599453
iteration 5 batch 1790 trainingloss 0.6931471805599453
iteration 5 batch 1800 trainingloss 0.6931471805599453
iteration 5 batch 1810 trainingloss 0.6931471805599453
iteration 5 batch 1820 trainingloss 0.6931471805599453
iteration 5 batch 1830 trainingloss 0.6931471805599453
iteration 5 batch 1840 trainingloss 0.6931471805599453
iteration 5 batch 1850 trainingloss 0.6916632528527511
iteration 5 batch 1860 trainingloss 0.6931471805599453
iteration 5 batch 1870 trainingloss 0.6931471805599453
iteration 5 batch 1880 trainingloss 0.6931471805599453
iteration 5 batch 1890 trainingloss 0.6931471805599453
iteration 5 batch 1900 trainingloss 0.6931471805599453
iteration 5 batch 1910 trainingloss 0.6931471805599453
iteration 5 batch 1920 trainingloss 0.6931471805599453
iteration 5 batch 1930 trainingloss 0.6931471805599453
iteration 5 batch 1940 trainingloss 0.6931471805599453
iteration 5 batch 1950 trainingloss 0.6931471805599453
iteration 5 batch 1960 trainingloss 0.6931471805599453
iteration 5 batch 1970 trainingloss 0.6931471805599453
iteration 5 batch 1980 trainingloss 0.6931471805599453
iteration 5 batch 1990 trainingloss 0.6931471805599453
iteration 5 batch 2000 trainingloss 0.6931471805599453
iteration 5 batch 2010 trainingloss 0.6931471805599453
iteration 5 batch 2020 trainingloss 0.6931471805599453
iteration 5 batch 2030 trainingloss 0.6931471805599453
iteration 5 batch 2040 trainingloss 0.6931471805599453
iteration 5 batch 2050 trainingloss 0.6931471805599453
iteration 5 batch 2060 trainingloss 0.6931471805599453
iteration 5 batch 2070 trainingloss 0.6931471805599453
iteration 5 batch 2080 trainingloss 0.6931471805599453
iteration 5 batch 2090 trainingloss 0.6931471805599453
iteration 5 batch 2100 trainingloss 0.6931471805599453
iteration 5 batch 2110 trainingloss 0.6931471805599453
iteration 5 batch 2120 trainingloss 0.6931471805599453
iteration 5 batch 2130 trainingloss 0.6931471805599453
iteration 5 batch 2140 trainingloss 0.6931471805599453
iteration 5 batch 2150 trainingloss 0.6931471805599453
iteration 5 batch 2160 trainingloss 0.6931471805599453
iteration 5 batch 2170 trainingloss 0.6931471805599453
iteration 5 batch 2180 trainingloss 0.6931471805599453
iteration 5 batch 2190 trainingloss 0.6931471805599453
iteration 5 batch 2200 trainingloss 0.6931471805599453
iteration 5 batch 2210 trainingloss 0.6931471805599453
iteration 5 batch 2220 trainingloss 0.6931471805599453
iteration 5 batch 2230 trainingloss 0.6931471805599453
iteration 5 batch 2240 trainingloss 0.6931471805599453
iteration 5 batch 2250 trainingloss 0.6931471805599453
iteration 5 batch 2260 trainingloss 0.6931471805599453
iteration 5 batch 2270 trainingloss 0.6931471805599453
iteration 5 batch 2280 trainingloss 0.6931471805599453
iteration 5 batch 2290 trainingloss 0.6931471805599453
iteration 5 batch 2300 trainingloss 0.6931471805599453
iteration 5 batch 2310 trainingloss 0.6931471805599453
iteration 5 batch 2320 trainingloss 0.6931471805599453
iteration 5 batch 2330 trainingloss 0.6931471805599453
iteration 5 batch 2340 trainingloss 0.6931471805599453
iteration 5 batch 2350 trainingloss 0.6931471805599453
iteration 5 batch 2360 trainingloss 0.6931471805599453
iteration 5 batch 2370 trainingloss 0.6931471805599453
iteration 5 batch 2380 trainingloss 0.6931471805599453
iteration 5 batch 2390 trainingloss 0.6931471805599453
iteration 5 batch 2400 trainingloss 0.6931471805599453
iteration 5 batch 2410 trainingloss 0.6931471805599453
iteration 5 batch 2420 trainingloss 0.6931471805599453
iteration 5 batch 2430 trainingloss 0.6931471805599453
iteration 5 batch 2440 trainingloss 0.6931471805599453
iteration 5 batch 2450 trainingloss 0.6931471805599453
iteration 5 batch 2460 trainingloss 0.6931471805599453
iteration 5 batch 2470 trainingloss 0.6931471805599453
iteration 5 batch 2480 trainingloss 0.6931471805599453
iteration 5 batch 2490 trainingloss 0.6931471805599453
iteration 5 batch 2500 trainingloss 0.6931471805599453
iteration 5 batch 2510 trainingloss 0.6931471805599453
iteration 5 batch 2520 trainingloss 0.6931471805599453
iteration 5 batch 2530 trainingloss 0.6931471805599453
iteration 5 batch 2540 trainingloss 0.6931471805599453
iteration 5 batch 2550 trainingloss 0.6931471805599453
iteration 5 batch 2560 trainingloss 0.6931471805599453
iteration 5 batch 2570 trainingloss 0.6931471805599453
iteration 5 batch 2580 trainingloss 0.6931471805599453
iteration 5 batch 2590 trainingloss 0.6931471805599453
iteration 5 batch 2600 trainingloss 0.6931471805599453
iteration 5 batch 2610 trainingloss 0.6931471805599453
iteration 5 batch 2620 trainingloss 0.6931471805599453
iteration 5 batch 2630 trainingloss 0.6931471805599453
iteration 5 batch 2640 trainingloss 0.6931471805599453
iteration 5 batch 2650 trainingloss 0.6931471805599453
iteration 5 batch 2660 trainingloss 0.6931471805599453
iteration 5 batch 2670 trainingloss 0.6931471805599453
iteration 5 batch 2680 trainingloss 0.6931471805599453
iteration 5 batch 2690 trainingloss 0.6931471805599453
iteration 5 batch 2700 trainingloss 0.6931471805599453
iteration 5 batch 2710 trainingloss 0.6931471805599453
iteration 5 batch 2720 trainingloss 0.6931471805599453
iteration 5 batch 2730 trainingloss 0.6916632528527511
iteration 5 batch 2740 trainingloss 0.6931471805599453
iteration 5 batch 2750 trainingloss 0.6931471805599453
iteration 5 batch 2760 trainingloss 0.6931471805599453
iteration 5 batch 2770 trainingloss 0.6931471805599453
iteration 5 batch 2780 trainingloss 0.6931471805599453
iteration 5 batch 2790 trainingloss 0.6916632528527511
iteration 5 batch 2800 trainingloss 0.6931471805599453
iteration 5 batch 2810 trainingloss 0.6931471805599453
iteration 5 batch 2820 trainingloss 0.6931471805599453
iteration 5 batch 2830 trainingloss 0.6931471805599453
iteration 5 batch 2840 trainingloss 0.6931471805599453
iteration 5 batch 2850 trainingloss 0.6931471805599453
iteration 5 batch 2860 trainingloss 0.6931471805599453
iteration 5 batch 2870 trainingloss 0.6931471805599453
iteration 5 batch 2880 trainingloss 0.6931471805599453
iteration 5 batch 2890 trainingloss 0.6931471805599453
iteration 5 batch 2900 trainingloss 0.6931471805599453
iteration 5 batch 2910 trainingloss 0.6931471805599453
iteration 5 batch 2920 trainingloss 0.6916632528527511
iteration 5 batch 2930 trainingloss 0.6931471805599453
iteration 5 batch 2940 trainingloss 0.6931471805599453
iteration 5 batch 2950 trainingloss 0.6931471805599453
iteration 5 batch 2960 trainingloss 0.6931471805599453
iteration 5 batch 2970 trainingloss 0.6931471805599453
iteration 5 batch 2980 trainingloss 0.6931471805599453
iteration 5 batch 2990 trainingloss 0.6931471805599453
iteration 5 batch 3000 trainingloss 0.6931471805599453
iteration 5 batch 3010 trainingloss 0.6931471805599453
iteration 5 batch 3020 trainingloss 0.6931471805599453
iteration 5 batch 3030 trainingloss 0.6931471805599453
iteration 5 batch 3040 trainingloss 0.6931471805599453
iteration 5 batch 3050 trainingloss 0.6931471805599453
iteration 5 batch 3060 trainingloss 0.6931471805599453
iteration 5 batch 3070 trainingloss 0.6931471805599453
iteration 5 batch 3080 trainingloss 0.6931471805599453
iteration 5 batch 3090 trainingloss 0.6931471805599453
iteration 5 batch 3100 trainingloss 0.6931471805599453
iteration 5 batch 3110 trainingloss 0.6931471805599453
iteration 5 batch 3120 trainingloss 0.6931471805599453
iteration 5 batch 3130 trainingloss 0.6931471805599453
iteration 5 batch 3140 trainingloss 0.6931471805599453
iteration 5 batch 3150 trainingloss 0.6931471805599453
iteration 5 batch 3160 trainingloss 0.6931471805599453
iteration 5 batch 3170 trainingloss 0.6931471805599453
iteration 5 batch 3180 trainingloss 0.6931471805599453
iteration 5 batch 3190 trainingloss 0.6931471805599453
iteration 5 batch 3200 trainingloss 0.6931471805599453
iteration 5 batch 3210 trainingloss 0.6931471805599453
iteration 5 batch 3220 trainingloss 0.6931471805599453
iteration 5 batch 3230 trainingloss 0.6931471805599453
iteration 5 batch 3240 trainingloss 0.6916632528527511
iteration 5 batch 3250 trainingloss 0.6931471805599453
iteration 5 batch 3260 trainingloss 0.6931471805599453
iteration 5 batch 3270 trainingloss 0.6931471805599453
iteration 5 batch 3280 trainingloss 0.6931471805599453
iteration 5 batch 3290 trainingloss 0.6931471805599453
iteration 5 batch 3300 trainingloss 0.6931471805599453
iteration 5 batch 3310 trainingloss 0.6931471805599453
iteration 5 batch 3320 trainingloss 0.6931471805599453
iteration 5 batch 3330 trainingloss 0.6931471805599453
iteration 5 batch 3340 trainingloss 0.6931471805599453
iteration 5 batch 3350 trainingloss 0.6931471805599453
iteration 5 batch 3360 trainingloss 0.6916632528527511
iteration 5 batch 3370 trainingloss 0.6931471805599453
iteration 5 batch 3380 trainingloss 0.6931471805599453
iteration 5 batch 3390 trainingloss 0.6931471805599453
iteration 5 batch 3400 trainingloss 0.6931471805599453
iteration 5 batch 3410 trainingloss 0.6931471805599453
iteration 5 batch 3420 trainingloss 0.6931471805599453
iteration 5 batch 3430 trainingloss 0.6931471805599453
iteration 5 batch 3440 trainingloss 0.6931471805599453
iteration 5 batch 3450 trainingloss 0.6931471805599453
iteration 5 batch 3460 trainingloss 0.6931471805599453
iteration 5 batch 3470 trainingloss 0.6931471805599453
iteration 5 batch 3480 trainingloss 0.6931471805599453
iteration 5 batch 3490 trainingloss 0.6931471805599453
iteration 5 batch 3500 trainingloss 0.6931471805599453
iteration 5 batch 3510 trainingloss 0.6931471805599453
iteration 5 batch 3520 trainingloss 0.6931471805599453
iteration 5 batch 3530 trainingloss 0.6931471805599453
iteration 5 batch 3540 trainingloss 0.6931471805599453
iteration 5 batch 3550 trainingloss 0.6931471805599453
iteration 5 batch 3560 trainingloss 0.6931471805599453
iteration 5 batch 3570 trainingloss 0.6931471805599453
iteration 5 batch 3580 trainingloss 0.6931471805599453
iteration 5 batch 3590 trainingloss 0.6916632528527511
iteration 5 batch 3600 trainingloss 0.6931471805599453
iteration 5 batch 3610 trainingloss 0.6931471805599453
iteration 5 batch 3620 trainingloss 0.6931471805599453
iteration 5 batch 3630 trainingloss 0.6916632528527511
iteration 5 batch 3640 trainingloss 0.6931471805599453
iteration 5 batch 3650 trainingloss 0.6931471805599453
iteration 5 batch 3660 trainingloss 0.6931471805599453
iteration 5 batch 3670 trainingloss 0.6931471805599453
iteration 5 batch 3680 trainingloss 0.6931471805599453
iteration 5 batch 3690 trainingloss 0.6931471805599453
iteration 5 batch 3700 trainingloss 0.6931471805599453
iteration 5 batch 3710 trainingloss 0.6916632528527511
iteration 5 batch 3720 trainingloss 0.6931471805599453
iteration 5 batch 3730 trainingloss 0.6931471805599453
iteration 5 batch 3740 trainingloss 0.6931471805599453
iteration 5 batch 3750 trainingloss 0.6931471805599453
iteration 5 batch 3760 trainingloss 0.6916632528527511
iteration 5 batch 3770 trainingloss 0.6931471805599453
iteration 5 batch 3780 trainingloss 0.6931471805599453
iteration 5 batch 3790 trainingloss 0.6916632528527511
iteration 5 batch 3800 trainingloss 0.6931471805599453
iteration 5 batch 3810 trainingloss 0.6931471805599453
iteration 5 batch 3820 trainingloss 0.6931471805599453
iteration 5 batch 3830 trainingloss 0.6931471805599453
iteration 5 batch 3840 trainingloss 0.6931471805599453
iteration 5 batch 3850 trainingloss 0.6931471805599453
iteration 5 batch 3860 trainingloss 0.6931471805599453
iteration 5 batch 3870 trainingloss 0.6931471805599453
iteration 5 batch 3880 trainingloss 0.6931471805599453
iteration 5 batch 3890 trainingloss 0.6931471805599453
iteration 5 batch 3900 trainingloss 0.6931471805599453
iteration 5 batch 3910 trainingloss 0.6931471805599453
iteration 5 batch 3920 trainingloss 0.6931471805599453
iteration 5 batch 3930 trainingloss 0.6931471805599453
iteration 5 batch 3940 trainingloss 0.6931471805599453
iteration 5 batch 3950 trainingloss 0.6931471805599453
iteration 5 batch 3960 trainingloss 0.6931471805599453
iteration 5 batch 3970 trainingloss 0.6931471805599453
iteration 5 batch 3980 trainingloss 0.6931471805599453
iteration 5 batch 3990 trainingloss 0.6931471805599453
iteration 5 batch 4000 trainingloss 0.6931471805599453
iteration 5 batch 4010 trainingloss 0.6931471805599453
iteration 5 batch 4020 trainingloss 0.6931471805599453
iteration 5 batch 4030 trainingloss 0.6931471805599453
iteration 5 batch 4040 trainingloss 0.6931471805599453
iteration 5 batch 4050 trainingloss 0.6931471805599453
iteration 5 batch 4060 trainingloss 0.6931471805599453
iteration 5 batch 4070 trainingloss 0.6931471805599453
iteration 5 batch 4080 trainingloss 0.6931471805599453
iteration 5 batch 4090 trainingloss 0.6931471805599453
iteration 5 batch 4100 trainingloss 0.6931471805599453
iteration 5 batch 4110 trainingloss 0.6931471805599453
iteration 5 batch 4120 trainingloss 0.6931471805599453
iteration 5 batch 4130 trainingloss 0.6931471805599453
iteration 5 batch 4140 trainingloss 0.6931471805599453
iteration 5 batch 4150 trainingloss 0.6931471805599453
iteration 5 batch 4160 trainingloss 0.6916632528527511
iteration 5 batch 4170 trainingloss 0.6931471805599453
iteration 5 batch 4180 trainingloss 0.6931471805599453
iteration 5 batch 4190 trainingloss 0.6931471805599453
iteration 5 batch 4200 trainingloss 0.6931471805599453
iteration 5 batch 4210 trainingloss 0.6931471805599453
iteration 5 batch 4220 trainingloss 0.6931471805599453
iteration 5 batch 4230 trainingloss 0.6931471805599453
iteration 5 batch 4240 trainingloss 0.6931471805599453
iteration 5 batch 4250 trainingloss 0.6931471805599453
iteration 5 batch 4260 trainingloss 0.6931471805599453
iteration 5 batch 4270 trainingloss 0.6931471805599453
iteration 5 batch 4280 trainingloss 0.6931471805599453
iteration 5 batch 4290 trainingloss 0.6916632528527511
iteration 5 batch 4300 trainingloss 0.6931471805599453
iteration 5 batch 4310 trainingloss 0.6931471805599453
iteration 5 batch 4320 trainingloss 0.6931471805599453
iteration 5 batch 4330 trainingloss 0.6931471805599453
iteration 5 batch 4340 trainingloss 0.6931471805599453
iteration 5 batch 4350 trainingloss 0.6931471805599453
iteration 5 batch 4360 trainingloss 0.6931471805599453
iteration 5 batch 4370 trainingloss 0.6931471805599453
iteration 5 batch 4380 trainingloss 0.6931471805599453
iteration 5 batch 4390 trainingloss 0.6931471805599453
iteration 5 batch 4400 trainingloss 0.6931471805599453
iteration 5 batch 4410 trainingloss 0.6931471805599453
iteration 5 batch 4420 trainingloss 0.6931471805599453
iteration 5 batch 4430 trainingloss 0.6931471805599453
iteration 5 batch 4440 trainingloss 0.6931471805599453
iteration 5 batch 4450 trainingloss 0.6931471805599453
iteration 5 batch 4460 trainingloss 0.6931471805599453
iteration 5 batch 4470 trainingloss 0.6931471805599453
iteration 5 batch 4480 trainingloss 0.6931471805599453
iteration 5 batch 4490 trainingloss 0.6931471805599453
iteration 5 batch 4500 trainingloss 0.6931471805599453
iteration 5 batch 4510 trainingloss 0.6931471805599453
iteration 5 batch 4520 trainingloss 0.6931471805599453
iteration 5 batch 4530 trainingloss 0.6931471805599453
iteration 5 batch 4540 trainingloss 0.6931471805599453
iteration 5 batch 4550 trainingloss 0.6916632528527511
iteration 5 batch 4560 trainingloss 0.6931471805599453
iteration 5 batch 4570 trainingloss 0.6931471805599453
iteration 5 batch 4580 trainingloss 0.6931471805599453
iteration 5 batch 4590 trainingloss 0.6931471805599453
iteration 5 batch 4600 trainingloss 0.6931471805599453
iteration 5 batch 4610 trainingloss 0.6916632528527511
iteration 5 batch 4620 trainingloss 0.6931471805599453
iteration 5 batch 4630 trainingloss 0.6931471805599453
iteration 5 batch 4640 trainingloss 0.6931471805599453
iteration 5 batch 4650 trainingloss 0.6931471805599453
iteration 5 batch 4660 trainingloss 0.6931471805599453
iteration 5 batch 4670 trainingloss 0.6931471805599453
iteration 5 batch 4680 trainingloss 0.6931471805599453
iteration 5 batch 4690 trainingloss 0.6931471805599453
iteration 5 batch 4700 trainingloss 0.6931471805599453
iteration 5 batch 4710 trainingloss 0.6931471805599453
iteration 5 batch 4720 trainingloss 0.6931471805599453
iteration 5 batch 4730 trainingloss 0.6931471805599453
iteration 5 batch 4740 trainingloss 0.6931471805599453
iteration 5 batch 4750 trainingloss 0.6931471805599453
iteration 5 batch 4760 trainingloss 0.6931471805599453
iteration 5 batch 4770 trainingloss 0.6931471805599453
iteration 5 batch 4780 trainingloss 0.6931471805599453
iteration 5 batch 4790 trainingloss 0.6916632528527511
iteration 5 batch 4800 trainingloss 0.6931471805599453
iteration 5 batch 4810 trainingloss 0.6931471805599453
iteration 5 batch 4820 trainingloss 0.6931471805599453
iteration 5 batch 4830 trainingloss 0.6931471805599453
iteration 5 batch 4840 trainingloss 0.6931471805599453
iteration 5 batch 4850 trainingloss 0.6931471805599453
iteration 5 batch 4860 trainingloss 0.6931471805599453
iteration 5 batch 4870 trainingloss 0.6931471805599453
iteration 5 batch 4880 trainingloss 0.6931471805599453
iteration 5 batch 4890 trainingloss 0.6931471805599453
iteration 5 batch 4900 trainingloss 0.6931471805599453
iteration 5 batch 4910 trainingloss 0.6931471805599453
iteration 5 batch 4920 trainingloss 0.6931471805599453
iteration 5 batch 4930 trainingloss 0.6931471805599453
iteration 5 batch 4940 trainingloss 0.6931471805599453
iteration 5 batch 4950 trainingloss 0.6931471805599453
iteration 5 batch 4960 trainingloss 0.6931471805599453
iteration 5 batch 4970 trainingloss 0.6931471805599453
iteration 5 batch 4980 trainingloss 0.6931471805599453
iteration 5 batch 4990 trainingloss 0.6916632528527511
iteration 5 batch 5000 trainingloss 0.6931471805599453
iteration 5 batch 5010 trainingloss 0.6931471805599453
iteration 5 batch 5020 trainingloss 0.6931471805599453
iteration 5 batch 5030 trainingloss 0.6931471805599453
iteration 5 batch 5040 trainingloss 0.6931471805599453
iteration 5 batch 5050 trainingloss 0.6916632528527511
iteration 5 batch 5060 trainingloss 0.6931471805599453
iteration 5 batch 5070 trainingloss 0.6931471805599453
iteration 5 batch 5080 trainingloss 0.6931471805599453
iteration 5 batch 5090 trainingloss 0.6916632528527511
iteration 5 batch 5100 trainingloss 0.6931471805599453
iteration 5 batch 5110 trainingloss 0.6931471805599453
iteration 5 batch 5120 trainingloss 0.6931471805599453
iteration 5 batch 5130 trainingloss 0.6931471805599453
iteration 5 batch 5140 trainingloss 0.6931471805599453
iteration 5 batch 5150 trainingloss 0.6931471805599453
iteration 5 batch 5160 trainingloss 0.6931471805599453
iteration 5 batch 5170 trainingloss 0.6931471805599453
iteration 5 batch 5180 trainingloss 0.6931471805599453
iteration 5 batch 5190 trainingloss 0.6931471805599453
iteration 5 batch 5200 trainingloss 0.6931471805599453
iteration 5 batch 5210 trainingloss 0.6916632528527511
iteration 5 batch 5220 trainingloss 0.6931471805599453
iteration 5 batch 5230 trainingloss 0.6931471805599453
iteration 5 batch 5240 trainingloss 0.6931471805599453
iteration 5 batch 5250 trainingloss 0.6931471805599453
iteration 5 batch 5260 trainingloss 0.6916632528527511
iteration 5 batch 5270 trainingloss 0.6931471805599453
iteration 5 batch 5280 trainingloss 0.6931471805599453
iteration 5 batch 5290 trainingloss 0.6931471805599453
iteration 5 batch 5300 trainingloss 0.6931471805599453
iteration 5 batch 5310 trainingloss 0.6931471805599453
iteration 5 batch 5320 trainingloss 0.6931471805599453
iteration 5 batch 5330 trainingloss 0.6931471805599453
iteration 5 batch 5340 trainingloss 0.6931471805599453
iteration 5 batch 5350 trainingloss 0.6931471805599453
iteration 5 batch 5360 trainingloss 0.6931471805599453
iteration 5 batch 5370 trainingloss 0.6931471805599453
iteration 5 batch 5380 trainingloss 0.6931471805599453
iteration 5 batch 5390 trainingloss 0.6931471805599453
iteration 5 batch 5400 trainingloss 0.6931471805599453
iteration 5 batch 5410 trainingloss 0.6931471805599453
iteration 5 batch 5420 trainingloss 0.6931471805599453
iteration 5 batch 5430 trainingloss 0.6931471805599453
iteration 5 batch 5440 trainingloss 0.6931471805599453
iteration 5 batch 5450 trainingloss 0.6931471805599453
iteration 5 batch 5460 trainingloss 0.6931471805599453
iteration 5 batch 5470 trainingloss 0.6931471805599453
iteration 5 batch 5480 trainingloss 0.6931471805599453
iteration 5 batch 5490 trainingloss 0.6916632528527511
iteration 5 batch 5500 trainingloss 0.6931471805599453
iteration 5 batch 5510 trainingloss 0.6931471805599453
iteration 5 batch 5520 trainingloss 0.6931471805599453
iteration 5 batch 5530 trainingloss 0.6931471805599453
iteration 5 batch 5540 trainingloss 0.6931471805599453
iteration 5 batch 5550 trainingloss 0.6931471805599453
iteration 5 batch 5560 trainingloss 0.6916632528527511
iteration 5 batch 5570 trainingloss 0.6931471805599453
iteration 5 batch 5580 trainingloss 0.6916632528527511
iteration 5 batch 5590 trainingloss 0.6931471805599453
iteration 5 batch 5600 trainingloss 0.6931471805599453
iteration 5 batch 5610 trainingloss 0.6931471805599453
iteration 5 batch 5620 trainingloss 0.6931471805599453
iteration 5 batch 5630 trainingloss 0.6931471805599453
iteration 5 batch 5640 trainingloss 0.6931471805599453
iteration 5 batch 5650 trainingloss 0.6931471805599453
iteration 5 batch 5660 trainingloss 0.6931471805599453
iteration 5 batch 5670 trainingloss 0.6931471805599453
iteration 5 batch 5680 trainingloss 0.6931471805599453
iteration 5 batch 5690 trainingloss 0.6931471805599453
iteration 5 batch 5700 trainingloss 0.6931471805599453
iteration 5 batch 5710 trainingloss 0.6931471805599453
iteration 5 batch 5720 trainingloss 0.6931471805599453
iteration 5 batch 5730 trainingloss 0.6931471805599453
iteration 5 batch 5740 trainingloss 0.6931471805599453
iteration 5 batch 5750 trainingloss 0.6931471805599453
iteration 5 batch 5760 trainingloss 0.6931471805599453
iteration 5 batch 5770 trainingloss 0.6931471805599453
iteration 5 batch 5780 trainingloss 0.6916632528527511
iteration 5 batch 5790 trainingloss 0.6931471805599453
iteration 5 batch 5800 trainingloss 0.6931471805599453
iteration 5 batch 5810 trainingloss 0.6916632528527511
iteration 5 batch 5820 trainingloss 0.6931471805599453
iteration 5 batch 5830 trainingloss 0.6931471805599453
iteration 5 batch 5840 trainingloss 0.6931471805599453
iteration 5 batch 5850 trainingloss 0.6931471805599453
iteration 5 batch 5860 trainingloss 0.6931471805599453
iteration 5 batch 5870 trainingloss 0.6931471805599453
iteration 5 batch 5880 trainingloss 0.6931471805599453
iteration 5 batch 5890 trainingloss 0.6931471805599453
iteration 5 batch 5900 trainingloss 0.6931471805599453
iteration 5 batch 5910 trainingloss 0.6931471805599453
iteration 5 batch 5920 trainingloss 0.6931471805599453
iteration 5 batch 5930 trainingloss 0.6931471805599453
iteration 5 batch 5940 trainingloss 0.6931471805599453
iteration 5 batch 5950 trainingloss 0.6931471805599453
iteration 5 batch 5960 trainingloss 0.6931471805599453
iteration 5 batch 5970 trainingloss 0.6931471805599453
iteration 5 batch 5980 trainingloss 0.6931471805599453
iteration 5 batch 5990 trainingloss 0.6931471805599453
iteration 5 batch 6000 trainingloss 0.6931471805599453
iteration 5 batch 6010 trainingloss 0.6931471805599453
iteration 5 batch 6020 trainingloss 0.6931471805599453
iteration 5 batch 6030 trainingloss 0.6931471805599453
iteration 5 batch 6040 trainingloss 0.6931471805599453
iteration 5 batch 6050 trainingloss 0.6931471805599453
iteration 5 batch 6060 trainingloss 0.6931471805599453
iteration 5 batch 6070 trainingloss 0.6931471805599453
iteration 5 batch 6080 trainingloss 0.6931471805599453
iteration 5 batch 6090 trainingloss 0.6931471805599453
iteration 5 batch 6100 trainingloss 0.6931471805599453
iteration 5 batch 6110 trainingloss 0.6931471805599453
iteration 5 batch 6120 trainingloss 0.6931471805599453
iteration 5 batch 6130 trainingloss 0.6931471805599453
iteration 5 batch 6140 trainingloss 0.6931471805599453
iteration 5 batch 6150 trainingloss 0.6931471805599453
iteration 5 batch 6160 trainingloss 0.6931471805599453
iteration 5 batch 6170 trainingloss 0.6916632528527511
iteration 5 batch 6180 trainingloss 0.6916632528527511
iteration 5 batch 6190 trainingloss 0.6931471805599453
iteration 5 batch 6200 trainingloss 0.6931471805599453
iteration 5 batch 6210 trainingloss 0.6931471805599453
iteration 5 batch 6220 trainingloss 0.6931471805599453
iteration 5 batch 6230 trainingloss 0.6916632528527511
iteration 5 batch 6240 trainingloss 0.6916632528527511
iteration 5 batch 6250 trainingloss 0.6931471805599453
iteration 5 batch 6260 trainingloss 0.6931471805599453
iteration 5 batch 6270 trainingloss 0.6931471805599453
iteration 5 batch 6280 trainingloss 0.6931471805599453
iteration 5 batch 6290 trainingloss 0.6901793251455568
iteration 5 batch 6300 trainingloss 0.6931471805599453
iteration 5 batch 6310 trainingloss 0.6931471805599453
iteration 5 batch 6320 trainingloss 0.6931471805599453
iteration 5 batch 6330 trainingloss 0.6931471805599453
iteration 5 batch 6340 trainingloss 0.6931471805599453
iteration 5 batch 6350 trainingloss 0.6931471805599453
iteration 5 batch 6360 trainingloss 0.6931471805599453
iteration 5 batch 6370 trainingloss 0.6916632528527511
iteration 5 batch 6380 trainingloss 0.6931471805599453
iteration 5 batch 6390 trainingloss 0.6931471805599453
iteration 5 batch 6400 trainingloss 0.6931471805599453
iteration 5 batch 6410 trainingloss 0.6916632528527511
iteration 5 batch 6420 trainingloss 0.6931471805599453
iteration 5 batch 6430 trainingloss 0.6931471805599453
iteration 5 batch 6440 trainingloss 0.6931471805599453
iteration 5 batch 6450 trainingloss 0.6931471805599453
iteration 5 batch 6460 trainingloss 0.6931471805599453
iteration 5 batch 6470 trainingloss 0.6931471805599453
iteration 5 batch 6480 trainingloss 0.6931471805599453
iteration 5 batch 6490 trainingloss 0.6931471805599453
iteration 5 batch 6500 trainingloss 0.6916632528527511
iteration 5 batch 6510 trainingloss 0.6931471805599453
iteration 5 batch 6520 trainingloss 0.6931471805599453
iteration 5 batch 6530 trainingloss 0.6931471805599453
iteration 5 batch 6540 trainingloss 0.6931471805599453
iteration 5 batch 6550 trainingloss 0.6931471805599453
iteration 5 batch 6560 trainingloss 0.6931471805599453
iteration 5 batch 6570 trainingloss 0.6931471805599453
iteration 5 batch 6580 trainingloss 0.6931471805599453
iteration 5 batch 6590 trainingloss 0.6931471805599453
iteration 5 batch 6600 trainingloss 0.6931471805599453
iteration 5 batch 6610 trainingloss 0.6931471805599453
iteration 5 batch 6620 trainingloss 0.6931471805599453
iteration 5 batch 6630 trainingloss 0.6931471805599453
iteration 5 batch 6640 trainingloss 0.6931471805599453
iteration 5 batch 6650 trainingloss 0.6931471805599453
iteration 5 batch 6660 trainingloss 0.6931471805599453
iteration 5 batch 6670 trainingloss 0.6931471805599453
iteration 5 batch 6680 trainingloss 0.6931471805599453
iteration 5 batch 6690 trainingloss 0.6931471805599453
iteration 5 batch 6700 trainingloss 0.6931471805599453
iteration 5 batch 6710 trainingloss 0.6931471805599453
iteration 5 batch 6720 trainingloss 0.6931471805599453
iteration 5 batch 6730 trainingloss 0.6931471805599453
iteration 5 batch 6740 trainingloss 0.6931471805599453
iteration 5 batch 6750 trainingloss 0.6931471805599453
iteration 5 batch 6760 trainingloss 0.6931471805599453
iteration 5 batch 6770 trainingloss 0.6931471805599453
iteration 5 batch 6780 trainingloss 0.6931471805599453
iteration 5 batch 6790 trainingloss 0.6931471805599453
iteration 5 batch 6800 trainingloss 0.6931471805599453
iteration 5 batch 6810 trainingloss 0.6931471805599453
iteration 5 batch 6820 trainingloss 0.6931471805599453
iteration 5 batch 6830 trainingloss 0.6931471805599453
iteration 5 batch 6840 trainingloss 0.6931471805599453
iteration 5 batch 6850 trainingloss 0.6931471805599453
iteration 5 batch 6860 trainingloss 0.6931471805599453
iteration 5 batch 6870 trainingloss 0.6931471805599453
iteration 5 batch 6880 trainingloss 0.6931471805599453
iteration 5 batch 6890 trainingloss 0.6931471805599453
iteration 5 batch 6900 trainingloss 0.6931471805599453
iteration 5 batch 6910 trainingloss 0.6931471805599453
iteration 5 batch 6920 trainingloss 0.6931471805599453
iteration 5 batch 6930 trainingloss 0.6931471805599453
iteration 5 batch 6940 trainingloss 0.6931471805599453
iteration 5 batch 6950 trainingloss 0.6931471805599453
iteration 5 batch 6960 trainingloss 0.6931471805599453
iteration 5 batch 6970 trainingloss 0.6931471805599453
iteration 5 batch 6980 trainingloss 0.6931471805599453
iteration 5 batch 6990 trainingloss 0.6931471805599453
iteration 5 batch 7000 trainingloss 0.6931471805599453
iteration 5 batch 7010 trainingloss 0.6931471805599453
iteration 5 batch 7020 trainingloss 0.6931471805599453
iteration 5 batch 7030 trainingloss 0.6931471805599453
iteration 5 batch 7040 trainingloss 0.6931471805599453
iteration 5 batch 7050 trainingloss 0.6931471805599453
iteration 5 batch 7060 trainingloss 0.6931471805599453
iteration 5 batch 7070 trainingloss 0.6931471805599453
iteration 5 batch 7080 trainingloss 0.6931471805599453
iteration 5 batch 7090 trainingloss 0.6931471805599453
iteration 5 batch 7100 trainingloss 0.6931471805599453
iteration 5 batch 7110 trainingloss 0.6931471805599453
iteration 5 batch 7120 trainingloss 0.6931471805599453
iteration 5 batch 7130 trainingloss 0.6931471805599453
iteration 5 batch 7140 trainingloss 0.6931471805599453
iteration 5 batch 7150 trainingloss 0.6931471805599453
iteration 5 batch 7160 trainingloss 0.6931471805599453
iteration 5 batch 7170 trainingloss 0.6931471805599453
iteration 5 batch 7180 trainingloss 0.6931471805599453
iteration 5 batch 7190 trainingloss 0.6931471805599453
iteration 5 batch 7200 trainingloss 0.6931471805599453
iteration 5 batch 7210 trainingloss 0.6931471805599453
iteration 5 batch 7220 trainingloss 0.6931471805599453
iteration 5 batch 7230 trainingloss 0.6931471805599453
iteration 5 batch 7240 trainingloss 0.6931471805599453
iteration 5 batch 7250 trainingloss 0.6931471805599453
iteration 5 batch 7260 trainingloss 0.6931471805599453
iteration 5 batch 7270 trainingloss 0.6931471805599453
iteration 5 batch 7280 trainingloss 0.6931471805599453
iteration 5 batch 7290 trainingloss 0.6931471805599453
iteration 5 batch 7300 trainingloss 0.6931471805599453
iteration 5 batch 7310 trainingloss 0.6931471805599453
iteration 5 batch 7320 trainingloss 0.6931471805599453
iteration 5 batch 7330 trainingloss 0.6931471805599453
iteration 5 batch 7340 trainingloss 0.6931471805599453
iteration 5 batch 7350 trainingloss 0.6931471805599453
iteration 5 batch 7360 trainingloss 0.6931471805599453
iteration 5 batch 7370 trainingloss 0.6901793251455568
iteration 5 batch 7380 trainingloss 0.6931471805599453
iteration 5 batch 7390 trainingloss 0.6931471805599453
iteration 5 batch 7400 trainingloss 0.6931471805599453
iteration 5 batch 7410 trainingloss 0.6931471805599453
iteration 5 batch 7420 trainingloss 0.6931471805599453
iteration 5 batch 7430 trainingloss 0.6931471805599453
iteration 5 batch 7440 trainingloss 0.6931471805599453
iteration 5 batch 7450 trainingloss 0.6931471805599453
iteration 5 batch 7460 trainingloss 0.6916632528527511
iteration 5 batch 7470 trainingloss 0.6931471805599453
iteration 5 batch 7480 trainingloss 0.6931471805599453
iteration 5 batch 7490 trainingloss 0.6931471805599453
iteration 5 batch 7500 trainingloss 0.6931471805599453
iteration 5 batch 7510 trainingloss 0.6931471805599453
iteration 5 batch 7520 trainingloss 0.6931471805599453
iteration 5 batch 7530 trainingloss 0.6931471805599453
iteration 5 batch 7540 trainingloss 0.6931471805599453
iteration 5 batch 7550 trainingloss 0.6931471805599453
iteration 5 batch 7560 trainingloss 0.6931471805599453
iteration 5 batch 7570 trainingloss 0.6931471805599453
iteration 5 batch 7580 trainingloss 0.6931471805599453
iteration 5 batch 7590 trainingloss 0.6931471805599453
iteration 5 batch 7600 trainingloss 0.6931471805599453
iteration 5 batch 7610 trainingloss 0.6931471805599453
iteration 5 batch 7620 trainingloss 0.6931471805599453
iteration 5 batch 7630 trainingloss 0.6931471805599453
iteration 5 batch 7640 trainingloss 0.6931471805599453
iteration 5 batch 7650 trainingloss 0.6931471805599453
iteration 5 batch 7660 trainingloss 0.6931471805599453
iteration 5 batch 7670 trainingloss 0.6931471805599453
iteration 5 batch 7680 trainingloss 0.6931471805599453
iteration 5 batch 7690 trainingloss 0.6916632528527511
iteration 5 batch 7700 trainingloss 0.6931471805599453
iteration 5 batch 7710 trainingloss 0.6931471805599453
iteration 5 batch 7720 trainingloss 0.6916632528527511
iteration 5 batch 7730 trainingloss 0.6931471805599453
iteration 5 batch 7740 trainingloss 0.6931471805599453
iteration 5 batch 7750 trainingloss 0.6931471805599453
iteration 5 batch 7760 trainingloss 0.6916632528527511
iteration 5 batch 7770 trainingloss 0.6931471805599453
iteration 5 batch 7780 trainingloss 0.6931471805599453
iteration 5 batch 7790 trainingloss 0.6931471805599453
iteration 5 batch 7800 trainingloss 0.6931471805599453
iteration 5 batch 7810 trainingloss 0.6931471805599453
iteration 5 batch 7820 trainingloss 0.6931471805599453
iteration 5 batch 7830 trainingloss 0.6931471805599453
iteration 5 batch 7840 trainingloss 0.6931471805599453
iteration 5 batch 7850 trainingloss 0.6931471805599453
iteration 5 batch 7860 trainingloss 0.6931471805599453
iteration 5 batch 7870 trainingloss 0.6931471805599453
iteration 5 batch 7880 trainingloss 0.6931471805599453
iteration 5 batch 7890 trainingloss 0.6931471805599453
iteration 5 batch 7900 trainingloss 0.6931471805599453
iteration 5 batch 7910 trainingloss 0.6931471805599453
iteration 5 batch 7920 trainingloss 0.6931471805599453
iteration 5 batch 7930 trainingloss 0.6931471805599453
iteration 5 batch 7940 trainingloss 0.6931471805599453
iteration 5 batch 7950 trainingloss 0.6931471805599453
iteration 5 batch 7960 trainingloss 0.6931471805599453
iteration 5 batch 7970 trainingloss 0.6931471805599453
iteration 5 batch 7980 trainingloss 0.6931471805599453
iteration 5 batch 7990 trainingloss 0.6931471805599453
iteration 5 batch 8000 trainingloss 0.6931471805599453
iteration 5 batch 8010 trainingloss 0.6931471805599453
iteration 5 batch 8020 trainingloss 0.6931471805599453
iteration 5 batch 8030 trainingloss 0.6931471805599453
iteration 5 batch 8040 trainingloss 0.6931471805599453
iteration 5 batch 8050 trainingloss 0.6931471805599453
iteration 5 batch 8060 trainingloss 0.6931471805599453
iteration 5 batch 8070 trainingloss 0.6931471805599453
iteration 5 batch 8080 trainingloss 0.6916632528527511
iteration 5 batch 8090 trainingloss 0.6931471805599453
iteration 5 batch 8100 trainingloss 0.6931471805599453
iteration 5 batch 8110 trainingloss 0.6931471805599453
iteration 5 batch 8120 trainingloss 0.6931471805599453
iteration 5 batch 8130 trainingloss 0.6931471805599453
iteration 5 batch 8140 trainingloss 0.6931471805599453
iteration 5 batch 8150 trainingloss 0.6931471805599453
iteration 5 batch 8160 trainingloss 0.6931471805599453
iteration 5 batch 8170 trainingloss 0.6931471805599453
iteration 5 batch 8180 trainingloss 0.6931471805599453
iteration 5 batch 8190 trainingloss 0.6931471805599453
iteration 5 batch 8200 trainingloss 0.6931471805599453
iteration 5 batch 8210 trainingloss 0.6931471805599453
iteration 5 batch 8220 trainingloss 0.6931471805599453
iteration 5 batch 8230 trainingloss 0.6931471805599453
iteration 5 batch 8240 trainingloss 0.6931471805599453
iteration 5 batch 8250 trainingloss 0.6931471805599453
iteration 5 batch 8260 trainingloss 0.6931471805599453
iteration 5 batch 8270 trainingloss 0.6931471805599453
iteration 5 batch 8280 trainingloss 0.6931471805599453
iteration 5 batch 8290 trainingloss 0.6916632528527511
iteration 5 batch 8300 trainingloss 0.6931471805599453
iteration 5 batch 8310 trainingloss 0.6931471805599453
iteration 5 batch 8320 trainingloss 0.6931471805599453
iteration 5 batch 8330 trainingloss 0.6931471805599453
iteration 5 batch 8340 trainingloss 0.6931471805599453
iteration 5 batch 8350 trainingloss 0.6931471805599453
iteration 5 batch 8360 trainingloss 0.6931471805599453
iteration 5 batch 8370 trainingloss 0.6931471805599453
iteration 5 batch 8380 trainingloss 0.6931471805599453
iteration 5 batch 8390 trainingloss 0.6931471805599453
iteration 5 batch 8400 trainingloss 0.6931471805599453
iteration 5 batch 8410 trainingloss 0.6931471805599453
iteration 5 batch 8420 trainingloss 0.6931471805599453
iteration 5 batch 8430 trainingloss 0.6931471805599453
iteration 5 batch 8440 trainingloss 0.6931471805599453
iteration 5 batch 8450 trainingloss 0.6931471805599453
iteration 5 batch 8460 trainingloss 0.6931471805599453
iteration 5 batch 8470 trainingloss 0.6931471805599453
iteration 5 batch 8480 trainingloss 0.6931471805599453
iteration 5 batch 8490 trainingloss 0.6931471805599453
iteration 5 batch 8500 trainingloss 0.6931471805599453
iteration 5 batch 8510 trainingloss 0.6931471805599453
iteration 5 batch 8520 trainingloss 0.6916632528527511
iteration 5 batch 8530 trainingloss 0.6931471805599453
iteration 5 batch 8540 trainingloss 0.6931471805599453
iteration 5 batch 8550 trainingloss 0.6931471805599453
iteration 5 batch 8560 trainingloss 0.6931471805599453
iteration 5 batch 8570 trainingloss 0.6931471805599453
iteration 5 batch 8580 trainingloss 0.6931471805599453
iteration 5 batch 8590 trainingloss 0.6931471805599453
iteration 5 batch 8600 trainingloss 0.6931471805599453
iteration 5 batch 8610 trainingloss 0.6931471805599453
iteration 5 batch 8620 trainingloss 0.6931471805599453
iteration 5 batch 8630 trainingloss 0.6931471805599453
iteration 5 batch 8640 trainingloss 0.6931471805599453
iteration 5 batch 8650 trainingloss 0.6931471805599453
iteration 5 batch 8660 trainingloss 0.6931471805599453
iteration 5 batch 8670 trainingloss 0.6931471805599453
iteration 5 batch 8680 trainingloss 0.6931471805599453
iteration 5 batch 8690 trainingloss 0.6931471805599453
iteration 5 batch 8700 trainingloss 0.6931471805599453
iteration 5 batch 8710 trainingloss 0.6931471805599453
iteration 5 batch 8720 trainingloss 0.6931471805599453
iteration 5 batch 8730 trainingloss 0.6931471805599453
iteration 5 batch 8740 trainingloss 0.6931471805599453
iteration 5 batch 8750 trainingloss 0.6931471805599453
iteration 5 batch 8760 trainingloss 0.6931471805599453
iteration 5 batch 8770 trainingloss 0.6931471805599453
iteration 5 batch 8780 trainingloss 0.6931471805599453
iteration 5 batch 8790 trainingloss 0.6931471805599453
iteration 5 batch 8800 trainingloss 0.6931471805599453
iteration 5 batch 8810 trainingloss 0.6931471805599453
iteration 5 batch 8820 trainingloss 0.6931471805599453
iteration 5 batch 8830 trainingloss 0.6931471805599453
iteration 5 batch 8840 trainingloss 0.6931471805599453
iteration 5 batch 8850 trainingloss 0.6931471805599453
iteration 5 batch 8860 trainingloss 0.6931471805599453
iteration 5 batch 8870 trainingloss 0.6931471805599453
iteration 5 batch 8880 trainingloss 0.6931471805599453
iteration 5 batch 8890 trainingloss 0.6931471805599453
iteration 5 batch 8900 trainingloss 0.6931471805599453
iteration 5 batch 8910 trainingloss 0.6931471805599453
iteration 5 batch 8920 trainingloss 0.6931471805599453
iteration 5 batch 8930 trainingloss 0.6931471805599453
iteration 5 batch 8940 trainingloss 0.6931471805599453
iteration 5 batch 8950 trainingloss 0.6931471805599453
iteration 5 batch 8960 trainingloss 0.6931471805599453
iteration 5 batch 8970 trainingloss 0.6931471805599453
iteration 5 batch 8980 trainingloss 0.6931471805599453
iteration 5 batch 8990 trainingloss 0.6931471805599453
iteration 5 batch 9000 trainingloss 0.6931471805599453
iteration 5 batch 9010 trainingloss 0.6931471805599453
iteration 5 batch 9020 trainingloss 0.6931471805599453
iteration 5 batch 9030 trainingloss 0.6931471805599453
iteration 5 batch 9040 trainingloss 0.6931471805599453
iteration 5 batch 9050 trainingloss 0.6931471805599453
iteration 5 batch 9060 trainingloss 0.6931471805599453
iteration 5 batch 9070 trainingloss 0.6931471805599453
iteration 5 batch 9080 trainingloss 0.6931471805599453
iteration 5 batch 9090 trainingloss 0.6931471805599453
iteration 5 batch 9100 trainingloss 0.6931471805599453
iteration 5 batch 9110 trainingloss 0.6931471805599453
iteration 5 batch 9120 trainingloss 0.6931471805599453
iteration 5 batch 9130 trainingloss 0.6931471805599453
iteration 5 batch 9140 trainingloss 0.6931471805599453
iteration 5 batch 9150 trainingloss 0.6931471805599453
iteration 5 batch 9160 trainingloss 0.6931471805599453
iteration 5 batch 9170 trainingloss 0.6931471805599453
iteration 5 batch 9180 trainingloss 0.6931471805599453
iteration 5 batch 9190 trainingloss 0.6916632528527511
iteration 5 batch 9200 trainingloss 0.6931471805599453
iteration 5 batch 9210 trainingloss 0.6931471805599453
iteration 5 batch 9220 trainingloss 0.6916632528527511
iteration 5 batch 9230 trainingloss 0.6916632528527511
iteration 5 batch 9240 trainingloss 0.6931471805599453
iteration 5 batch 9250 trainingloss 0.6931471805599453
iteration 5 batch 9260 trainingloss 0.6931471805599453
iteration 5 batch 9270 trainingloss 0.6931471805599453
iteration 5 batch 9280 trainingloss 0.6931471805599453
iteration 5 batch 9290 trainingloss 0.6931471805599453
iteration 5 batch 9300 trainingloss 0.6931471805599453
iteration 5 batch 9310 trainingloss 0.6931471805599453
iteration 5 batch 9320 trainingloss 0.6931471805599453
iteration 5 batch 9330 trainingloss 0.6931471805599453
iteration 5 batch 9340 trainingloss 0.6931471805599453
iteration 5 batch 9350 trainingloss 0.6931471805599453
iteration 5 batch 9360 trainingloss 0.6931471805599453
iteration 5 batch 9370 trainingloss 0.6916632528527511
iteration 5 batch 9380 trainingloss 0.6931471805599453
iteration 5 batch 9390 trainingloss 0.6931471805599453
iteration 5 batch 9400 trainingloss 0.6931471805599453
iteration 5 batch 9410 trainingloss 0.6931471805599453
iteration 5 batch 9420 trainingloss 0.6931471805599453
iteration 5 batch 9430 trainingloss 0.6931471805599453
iteration 5 batch 9440 trainingloss 0.6916632528527511
iteration 5 batch 9450 trainingloss 0.6931471805599453
iteration 5 batch 9460 trainingloss 0.6931471805599453
iteration 5 batch 9470 trainingloss 0.6931471805599453
iteration 5 batch 9480 trainingloss 0.6931471805599453
iteration 5 batch 9490 trainingloss 0.6931471805599453
iteration 5 batch 9500 trainingloss 0.6931471805599453
iteration 5 batch 9510 trainingloss 0.6931471805599453
iteration 5 batch 9520 trainingloss 0.6931471805599453
iteration 5 batch 9530 trainingloss 0.6931471805599453
iteration 5 batch 9540 trainingloss 0.6931471805599453
iteration 5 batch 9550 trainingloss 0.6916632528527511
iteration 5 batch 9560 trainingloss 0.6931471805599453
iteration 5 batch 9570 trainingloss 0.6916632528527511
iteration 5 batch 9580 trainingloss 0.6931471805599453
iteration 5 batch 9590 trainingloss 0.6931471805599453
iteration 5 batch 9600 trainingloss 0.6931471805599453
iteration 5 batch 9610 trainingloss 0.6931471805599453
iteration 5 batch 9620 trainingloss 0.6931471805599453
iteration 5 batch 9630 trainingloss 0.6931471805599453
iteration 5 batch 9640 trainingloss 0.6931471805599453
iteration 5 batch 9650 trainingloss 0.6931471805599453
iteration 5 batch 9660 trainingloss 0.6931471805599453
iteration 5 batch 9670 trainingloss 0.6931471805599453
iteration 5 batch 9680 trainingloss 0.6931471805599453
iteration 5 batch 9690 trainingloss 0.6916632528527511
iteration 5 batch 9700 trainingloss 0.6931471805599453
iteration 5 batch 9710 trainingloss 0.6931471805599453
iteration 5 batch 9720 trainingloss 0.6931471805599453
iteration 5 batch 9730 trainingloss 0.6931471805599453
iteration 5 batch 9740 trainingloss 0.6931471805599453
iteration 5 batch 9750 trainingloss 0.6931471805599453
iteration 5 batch 9760 trainingloss 0.6931471805599453
iteration 5 batch 9770 trainingloss 0.6931471805599453
iteration 5 batch 9780 trainingloss 0.6931471805599453
iteration 5 batch 9790 trainingloss 0.6931471805599453
iteration 5 batch 9800 trainingloss 0.6931471805599453
iteration 5 batch 9810 trainingloss 0.6931471805599453
iteration 5 batch 9820 trainingloss 0.6931471805599453
iteration 5 batch 9830 trainingloss 0.6931471805599453
iteration 5 batch 9840 trainingloss 0.6931471805599453
iteration 5 batch 9850 trainingloss 0.6931471805599453
iteration 5 batch 9860 trainingloss 0.6931471805599453
iteration 5 batch 9870 trainingloss 0.6931471805599453
iteration 5 batch 9880 trainingloss 0.6916632528527511
iteration 5 batch 9890 trainingloss 0.6931471805599453
iteration 5 batch 9900 trainingloss 0.6931471805599453
iteration 5 batch 9910 trainingloss 0.6931471805599453
iteration 5 batch 9920 trainingloss 0.6931471805599453
iteration 5 batch 9930 trainingloss 0.6931471805599453
iteration 5 batch 9940 trainingloss 0.6931471805599453
iteration 5 batch 9950 trainingloss 0.6931471805599453
iteration 5 batch 9960 trainingloss 0.6931471805599453
iteration 5 batch 9970 trainingloss 0.6916632528527511
iteration 5 batch 9980 trainingloss 0.6931471805599453
iteration 5 batch 9990 trainingloss 0.6931471805599453
iteration 5 batch 10000 trainingloss 0.6931471805599453
iteration 5 batch 10010 trainingloss 0.6931471805599453
iteration 5 batch 10020 trainingloss 0.6931471805599453
iteration 5 batch 10030 trainingloss 0.6931471805599453
iteration 5 batch 10040 trainingloss 0.6931471805599453
iteration 5 batch 10050 trainingloss 0.6931471805599453
iteration 5 batch 10060 trainingloss 0.6931471805599453
iteration 5 batch 10070 trainingloss 0.6931471805599453
iteration 5 batch 10080 trainingloss 0.6931471805599453
iteration 5 batch 10090 trainingloss 0.6931471805599453
iteration 5 batch 10100 trainingloss 0.6931471805599453
iteration 5 batch 10110 trainingloss 0.6931471805599453
iteration 5 batch 10120 trainingloss 0.6931471805599453
iteration 5 batch 10130 trainingloss 0.6931471805599453
iteration 5 batch 10140 trainingloss 0.6931471805599453
iteration 5 batch 10150 trainingloss 0.6931471805599453
iteration 5 batch 10160 trainingloss 0.6931471805599453
iteration 5 batch 10170 trainingloss 0.6931471805599453
iteration 5 batch 10180 trainingloss 0.6931471805599453
iteration 5 batch 10190 trainingloss 0.6931471805599453
iteration 5 batch 10200 trainingloss 0.6931471805599453
iteration 5 batch 10210 trainingloss 0.6931471805599453
iteration 5 batch 10220 trainingloss 0.6916632528527511
iteration 5 batch 10230 trainingloss 0.6916632528527511
iteration 5 batch 10240 trainingloss 0.6931471805599453
iteration 5 batch 10250 trainingloss 0.6931471805599453
iteration 5 batch 10260 trainingloss 0.6931471805599453
iteration 5 batch 10270 trainingloss 0.6931471805599453
iteration 5 batch 10280 trainingloss 0.6931471805599453
iteration 5 batch 10290 trainingloss 0.6931471805599453
iteration 5 batch 10300 trainingloss 0.6931471805599453
iteration 5 batch 10310 trainingloss 0.6931471805599453
iteration 5 batch 10320 trainingloss 0.6931471805599453
iteration 5 batch 10330 trainingloss 0.6931471805599453
iteration 5 batch 10340 trainingloss 0.6931471805599453
iteration 5 batch 10350 trainingloss 0.6916632528527511
iteration 5 batch 10360 trainingloss 0.6931471805599453
iteration 5 batch 10370 trainingloss 0.6931471805599453
iteration 5 batch 10380 trainingloss 0.6931471805599453
iteration 5 batch 10390 trainingloss 0.6931471805599453
iteration 5 batch 10400 trainingloss 0.6931471805599453
iteration 5 batch 10410 trainingloss 0.6931471805599453
iteration 5 batch 10420 trainingloss 0.6931471805599453
iteration 5 batch 10430 trainingloss 0.6931471805599453
iteration 5 batch 10440 trainingloss 0.6931471805599453
iteration 5 batch 10450 trainingloss 0.6931471805599453
iteration 5 batch 10460 trainingloss 0.6931471805599453
iteration 5 batch 10470 trainingloss 0.6931471805599453
iteration 5 batch 10480 trainingloss 0.6931471805599453
iteration 5 batch 10490 trainingloss 0.6931471805599453
iteration 5 batch 10500 trainingloss 0.6931471805599453
iteration 5 batch 10510 trainingloss 0.6931471805599453
iteration 5 batch 10520 trainingloss 0.6931471805599453
iteration 5 batch 10530 trainingloss 0.6931471805599453
iteration 5 batch 10540 trainingloss 0.6931471805599453
iteration 5 batch 10550 trainingloss 0.6931471805599453
iteration 5 batch 10560 trainingloss 0.6931471805599453
iteration 5 batch 10570 trainingloss 0.6931471805599453
iteration 5 batch 10580 trainingloss 0.6931471805599453
iteration 5 batch 10590 trainingloss 0.6931471805599453
iteration 5 batch 10600 trainingloss 0.6931471805599453
iteration 5 batch 10610 trainingloss 0.6931471805599453
iteration 5 batch 10620 trainingloss 0.6931471805599453
iteration 5 batch 10630 trainingloss 0.6931471805599453
iteration 5 batch 10640 trainingloss 0.6931471805599453
iteration 5 batch 10650 trainingloss 0.6931471805599453
iteration 5 batch 10660 trainingloss 0.6916632528527511
iteration 5 batch 10670 trainingloss 0.6931471805599453
iteration 5 batch 10680 trainingloss 0.6931471805599453
iteration 5 batch 10690 trainingloss 0.6931471805599453
iteration 5 batch 10700 trainingloss 0.6931471805599453
iteration 5 batch 10710 trainingloss 0.6931471805599453
iteration 5 batch 10720 trainingloss 0.6916632528527511
iteration 5 batch 10730 trainingloss 0.6931471805599453
iteration 5 batch 10740 trainingloss 0.6931471805599453
iteration 5 batch 10750 trainingloss 0.6931471805599453
iteration 5 batch 10760 trainingloss 0.6931471805599453
iteration 5 batch 10770 trainingloss 0.6931471805599453
iteration 5 batch 10780 trainingloss 0.6931471805599453
iteration 5 batch 10790 trainingloss 0.6931471805599453
iteration 5 batch 10800 trainingloss 0.6931471805599453
iteration 5 batch 10810 trainingloss 0.6931471805599453
iteration 5 batch 10820 trainingloss 0.6931471805599453
iteration 5 batch 10830 trainingloss 0.6931471805599453
iteration 5 batch 10840 trainingloss 0.6931471805599453
iteration 5 batch 10850 trainingloss 0.6931471805599453
iteration 5 batch 10860 trainingloss 0.6916632528527511
iteration 5 batch 10870 trainingloss 0.6931471805599453
iteration 5 batch 10880 trainingloss 0.6931471805599453
iteration 5 batch 10890 trainingloss 0.6931471805599453
iteration 5 batch 10900 trainingloss 0.6931471805599453
iteration 5 batch 10910 trainingloss 0.6931471805599453
iteration 5 batch 10920 trainingloss 0.6931471805599453
iteration 5 batch 10930 trainingloss 0.6931471805599453
iteration 5 batch 10940 trainingloss 0.6931471805599453
iteration 5 batch 10950 trainingloss 0.6931471805599453
iteration 5 batch 10960 trainingloss 0.6931471805599453
iteration 5 batch 10970 trainingloss 0.6931471805599453
iteration 5 batch 10980 trainingloss 0.6916632528527511
iteration 5 batch 10990 trainingloss 0.6931471805599453
iteration 5 batch 11000 trainingloss 0.6931471805599453
iteration 5 batch 11010 trainingloss 0.6931471805599453
iteration 5 batch 11020 trainingloss 0.6931471805599453
iteration 5 batch 11030 trainingloss 0.6931471805599453
iteration 5 batch 11040 trainingloss 0.6931471805599453
iteration 5 batch 11050 trainingloss 0.6931471805599453
iteration 5 batch 11060 trainingloss 0.6931471805599453
iteration 5 batch 11070 trainingloss 0.6931471805599453
iteration 5 batch 11080 trainingloss 0.6916632528527511
iteration 5 batch 11090 trainingloss 0.6931471805599453
iteration 5 batch 11100 trainingloss 0.6931471805599453
iteration 5 batch 11110 trainingloss 0.6931471805599453
iteration 5 batch 11120 trainingloss 0.6931471805599453
iteration 5 batch 11130 trainingloss 0.6931471805599453
iteration 5 batch 11140 trainingloss 0.6931471805599453
iteration 5 batch 11150 trainingloss 0.6931471805599453
iteration 5 batch 11160 trainingloss 0.6931471805599453
iteration 5 batch 11170 trainingloss 0.6931471805599453
iteration 5 batch 11180 trainingloss 0.6916632528527511
iteration 5 batch 11190 trainingloss 0.6931471805599453
iteration 5 batch 11200 trainingloss 0.6931471805599453
iteration 5 batch 11210 trainingloss 0.6931471805599453
iteration 5 batch 11220 trainingloss 0.6931471805599453
iteration 5 batch 11230 trainingloss 0.6931471805599453
iteration 5 batch 11240 trainingloss 0.6931471805599453
iteration 5 batch 11250 trainingloss 0.6931471805599453
iteration 5 batch 11260 trainingloss 0.6931471805599453
iteration 5 batch 11270 trainingloss 0.6931471805599453
iteration 5 batch 11280 trainingloss 0.6931471805599453
iteration 5 batch 11290 trainingloss 0.6931471805599453
iteration 5 batch 11300 trainingloss 0.6931471805599453
iteration 5 batch 11310 trainingloss 0.6931471805599453
iteration 5 batch 11320 trainingloss 0.6931471805599453
iteration 5 batch 11330 trainingloss 0.6931471805599453
iteration 5 batch 11340 trainingloss 0.6931471805599453
iteration 5 batch 11350 trainingloss 0.6931471805599453
iteration 5 batch 11360 trainingloss 0.6931471805599453
iteration 5 batch 11370 trainingloss 0.6931471805599453
iteration 5 batch 11380 trainingloss 0.6931471805599453
iteration 5 batch 11390 trainingloss 0.6916632528527511
iteration 5 batch 11400 trainingloss 0.6931471805599453
iteration 5 batch 11410 trainingloss 0.6931471805599453
iteration 5 batch 11420 trainingloss 0.6931471805599453
iteration 5 batch 11430 trainingloss 0.6931471805599453
iteration 5 batch 11440 trainingloss 0.6931471805599453
iteration 5 batch 11450 trainingloss 0.6931471805599453
iteration 5 batch 11460 trainingloss 0.6931471805599453
iteration 5 batch 11470 trainingloss 0.6931471805599453
iteration 5 batch 11480 trainingloss 0.6931471805599453
iteration 5 batch 11490 trainingloss 0.6931471805599453
iteration 5 batch 11500 trainingloss 0.6931471805599453
iteration 5 batch 11510 trainingloss 0.6931471805599453
iteration 5 batch 11520 trainingloss 0.6931471805599453
iteration 5 batch 11530 trainingloss 0.6931471805599453
iteration 5 batch 11540 trainingloss 0.6931471805599453
iteration 5 batch 11550 trainingloss 0.6931471805599453
iteration 5 batch 11560 trainingloss 0.6931471805599453
iteration 5 batch 11570 trainingloss 0.6931471805599453
iteration 5 batch 11580 trainingloss 0.6931471805599453
iteration 5 batch 11590 trainingloss 0.6931471805599453
iteration 5 batch 11600 trainingloss 0.6931471805599453
iteration 5 batch 11610 trainingloss 0.6931471805599453
iteration 5 batch 11620 trainingloss 0.6931471805599453
iteration 5 batch 11630 trainingloss 0.6931471805599453
iteration 5 batch 11640 trainingloss 0.6931471805599453
iteration 5 batch 11650 trainingloss 0.6916632528527511
iteration 5 batch 11660 trainingloss 0.6931471805599453
iteration 5 batch 11670 trainingloss 0.6931471805599453
iteration 5 batch 11680 trainingloss 0.6931471805599453
iteration 5 batch 11690 trainingloss 0.6931471805599453
iteration 5 batch 11700 trainingloss 0.6931471805599453
iteration 5 batch 11710 trainingloss 0.6931471805599453
iteration 5 batch 11720 trainingloss 0.6931471805599453
iteration 5 batch 11730 trainingloss 0.6916632528527511
iteration 5 batch 11740 trainingloss 0.6931471805599453
iteration 5 batch 11750 trainingloss 0.6916632528527511
iteration 5 batch 11760 trainingloss 0.6931471805599453
iteration 5 batch 11770 trainingloss 0.6931471805599453
iteration 5 batch 11780 trainingloss 0.6916632528527511
iteration 5 batch 11790 trainingloss 0.6931471805599453
iteration 5 batch 11800 trainingloss 0.6931471805599453
iteration 5 batch 11810 trainingloss 0.6931471805599453
iteration 5 batch 11820 trainingloss 0.6931471805599453
iteration 5 batch 11830 trainingloss 0.6931471805599453
iteration 5 batch 11840 trainingloss 0.6931471805599453
iteration 5 batch 11850 trainingloss 0.6931471805599453
iteration 5 batch 11860 trainingloss 0.6931471805599453
iteration 5 batch 11870 trainingloss 0.6931471805599453
iteration 5 batch 11880 trainingloss 0.6931471805599453
iteration 5 batch 11890 trainingloss 0.6931471805599453
iteration 5 batch 11900 trainingloss 0.6931471805599453
iteration 5 batch 11910 trainingloss 0.6931471805599453
iteration 5 batch 11920 trainingloss 0.6931471805599453
iteration 5 batch 11930 trainingloss 0.6931471805599453
iteration 5 batch 11940 trainingloss 0.6931471805599453
iteration 5 batch 11950 trainingloss 0.6931471805599453
iteration 5 batch 11960 trainingloss 0.6931471805599453
iteration 5 batch 11970 trainingloss 0.6916632528527511
iteration 5 batch 11980 trainingloss 0.6931471805599453
iteration 5 batch 11990 trainingloss 0.6931471805599453
iteration 5 batch 12000 trainingloss 0.6931471805599453
iteration 5 batch 12010 trainingloss 0.6916632528527511
iteration 5 batch 12020 trainingloss 0.6931471805599453
iteration 5 batch 12030 trainingloss 0.6931471805599453
iteration 5 batch 12040 trainingloss 0.6931471805599453
iteration 5 batch 12050 trainingloss 0.6931471805599453
iteration 5 batch 12060 trainingloss 0.6931471805599453
iteration 5 batch 12070 trainingloss 0.6931471805599453
iteration 5 batch 12080 trainingloss 0.6931471805599453
iteration 5 batch 12090 trainingloss 0.6931471805599453
iteration 5 batch 12100 trainingloss 0.6931471805599453
iteration 5 batch 12110 trainingloss 0.6931471805599453
iteration 5 batch 12120 trainingloss 0.6931471805599453
iteration 5 batch 12130 trainingloss 0.6931471805599453
iteration 5 batch 12140 trainingloss 0.6931471805599453
iteration 5 batch 12150 trainingloss 0.6931471805599453
iteration 5 batch 12160 trainingloss 0.6931471805599453
iteration 5 batch 12170 trainingloss 0.6931471805599453
iteration 5 batch 12180 trainingloss 0.6931471805599453
iteration 5 batch 12190 trainingloss 0.6931471805599453
iteration 5 batch 12200 trainingloss 0.6931471805599453
iteration 5 batch 12210 trainingloss 0.6931471805599453
iteration 5 batch 12220 trainingloss 0.6931471805599453
iteration 5 batch 12230 trainingloss 0.6931471805599453
iteration 5 batch 12240 trainingloss 0.6931471805599453
iteration 5 batch 12250 trainingloss 0.6931471805599453
iteration 5 batch 12260 trainingloss 0.6931471805599453
iteration 5 batch 12270 trainingloss 0.6931471805599453
iteration 5 batch 12280 trainingloss 0.6931471805599453
iteration 5 batch 12290 trainingloss 0.6931471805599453
iteration 5 batch 12300 trainingloss 0.6931471805599453
iteration 5 batch 12310 trainingloss 0.6931471805599453
iteration 5 batch 12320 trainingloss 0.6931471805599453
iteration 5 batch 12330 trainingloss 0.6931471805599453
iteration 5 batch 12340 trainingloss 0.6931471805599453
iteration 5 batch 12350 trainingloss 0.6931471805599453
iteration 5 batch 12360 trainingloss 0.6931471805599453
iteration 5 batch 12370 trainingloss 0.6931471805599453
iteration 5 batch 12380 trainingloss 0.6931471805599453
iteration 5 batch 12390 trainingloss 0.6931471805599453
iteration 5 batch 12400 trainingloss 0.6931471805599453
iteration 5 batch 12410 trainingloss 0.6916632528527511
iteration 5 batch 12420 trainingloss 0.6931471805599453
iteration 5 batch 12430 trainingloss 0.6931471805599453
iteration 5 batch 12440 trainingloss 0.6931471805599453
iteration 5 batch 12450 trainingloss 0.6931471805599453
iteration 5 batch 12460 trainingloss 0.6931471805599453
iteration 5 batch 12470 trainingloss 0.6931471805599453
iteration 5 batch 12480 trainingloss 0.6931471805599453
iteration 5 batch 12490 trainingloss 0.6916632528527511
iteration 5 batch 12500 trainingloss 0.6931471805599453
iteration 5 batch 12510 trainingloss 0.6931471805599453
iteration 5 batch 12520 trainingloss 0.6931471805599453
iteration 5 batch 12530 trainingloss 0.6931471805599453
iteration 5 batch 12540 trainingloss 0.6931471805599453
iteration 5 batch 12550 trainingloss 0.6931471805599453
iteration 5 batch 12560 trainingloss 0.6931471805599453
iteration 5 batch 12570 trainingloss 0.6931471805599453
iteration 5 batch 12580 trainingloss 0.6931471805599453
iteration 5 batch 12590 trainingloss 0.6931471805599453
iteration 5 batch 12600 trainingloss 0.6931471805599453
iteration 5 batch 12610 trainingloss 0.6931471805599453
iteration 5 batch 12620 trainingloss 0.6931471805599453
iteration 5 batch 12630 trainingloss 0.6931471805599453
iteration 5 batch 12640 trainingloss 0.6931471805599453
iteration 5 batch 12650 trainingloss 0.6931471805599453
iteration 5 batch 12660 trainingloss 0.6931471805599453
iteration 5 batch 12670 trainingloss 0.6931471805599453
iteration 5 batch 12680 trainingloss 0.6916632528527511
iteration 5 batch 12690 trainingloss 0.6931471805599453
iteration 5 batch 12700 trainingloss 0.6931471805599453
iteration 5 batch 12710 trainingloss 0.6931471805599453
iteration 5 batch 12720 trainingloss 0.6931471805599453
iteration 5 batch 12730 trainingloss 0.6931471805599453
iteration 5 batch 12740 trainingloss 0.6931471805599453
iteration 5 batch 12750 trainingloss 0.6931471805599453
iteration 5 batch 12760 trainingloss 0.6931471805599453
iteration 5 batch 12770 trainingloss 0.6931471805599453
iteration 5 batch 12780 trainingloss 0.6931471805599453
iteration 5 batch 12790 trainingloss 0.6931471805599453
iteration 5 batch 12800 trainingloss 0.6931471805599453
iteration 5 batch 12810 trainingloss 0.6931471805599453
iteration 5 batch 12820 trainingloss 0.6931471805599453
iteration 5 batch 12830 trainingloss 0.6931471805599453
iteration 5 batch 12840 trainingloss 0.6931471805599453
iteration 5 batch 12850 trainingloss 0.6931471805599453
iteration 5 batch 12860 trainingloss 0.6931471805599453
iteration 5 batch 12870 trainingloss 0.6931471805599453
iteration 5 batch 12880 trainingloss 0.6931471805599453
iteration 5 batch 12890 trainingloss 0.6931471805599453
iteration 5 batch 12900 trainingloss 0.6931471805599453
iteration 5 batch 12910 trainingloss 0.6931471805599453
iteration 5 batch 12920 trainingloss 0.6931471805599453
iteration 5 batch 12930 trainingloss 0.6931471805599453
iteration 5 batch 12940 trainingloss 0.6931471805599453
iteration 5 batch 12950 trainingloss 0.6916632528527511
iteration 5 batch 12960 trainingloss 0.6931471805599453
iteration 5 batch 12970 trainingloss 0.6931471805599453
iteration 5 batch 12980 trainingloss 0.6931471805599453
iteration 5 batch 12990 trainingloss 0.6931471805599453
iteration 5 batch 13000 trainingloss 0.6931471805599453
iteration 5 batch 13010 trainingloss 0.6931471805599453
iteration 5 batch 13020 trainingloss 0.6931471805599453
iteration 5 batch 13030 trainingloss 0.6931471805599453
iteration 5 batch 13040 trainingloss 0.6931471805599453
iteration 5 batch 13050 trainingloss 0.6931471805599453
iteration 5 batch 13060 trainingloss 0.6931471805599453
iteration 5 batch 13070 trainingloss 0.6931471805599453
iteration 5 batch 13080 trainingloss 0.6931471805599453
iteration 5 batch 13090 trainingloss 0.6931471805599453
iteration 5 batch 13100 trainingloss 0.6931471805599453
iteration 5 batch 13110 trainingloss 0.6931471805599453
iteration 5 batch 13120 trainingloss 0.6931471805599453
iteration 5 batch 13130 trainingloss 0.6931471805599453
iteration 5 batch 13140 trainingloss 0.6931471805599453
iteration 5 batch 13150 trainingloss 0.6931471805599453
iteration 5 batch 13160 trainingloss 0.6931471805599453
iteration 5 batch 13170 trainingloss 0.6931471805599453
iteration 5 batch 13180 trainingloss 0.6931471805599453
iteration 5 batch 13190 trainingloss 0.6931471805599453
iteration 5 batch 13200 trainingloss 0.6931471805599453
iteration 5 batch 13210 trainingloss 0.6901793251455568
iteration 5 batch 13220 trainingloss 0.6931471805599453
iteration 5 batch 13230 trainingloss 0.6931471805599453
iteration 5 batch 13240 trainingloss 0.6931471805599453
iteration 5 batch 13250 trainingloss 0.6931471805599453
iteration 5 batch 13260 trainingloss 0.6931471805599453
iteration 5 batch 13270 trainingloss 0.6931471805599453
iteration 5 batch 13280 trainingloss 0.6931471805599453
iteration 5 batch 13290 trainingloss 0.6931471805599453
iteration 5 batch 13300 trainingloss 0.6931471805599453
iteration 5 batch 13310 trainingloss 0.6931471805599453
iteration 5 batch 13320 trainingloss 0.6916632528527511
iteration 5 batch 13330 trainingloss 0.6931471805599453
iteration 5 batch 13340 trainingloss 0.6931471805599453
iteration 5 batch 13350 trainingloss 0.6931471805599453
iteration 5 batch 13360 trainingloss 0.6931471805599453
iteration 5 batch 13370 trainingloss 0.6931471805599453
iteration 5 batch 13380 trainingloss 0.6931471805599453
iteration 5 batch 13390 trainingloss 0.6931471805599453
iteration 5 batch 13400 trainingloss 0.6916632528527511
iteration 5 batch 13410 trainingloss 0.6931471805599453
iteration 5 batch 13420 trainingloss 0.6931471805599453
iteration 5 batch 13430 trainingloss 0.6931471805599453
iteration 5 batch 13440 trainingloss 0.6916632528527511
iteration 5 batch 13450 trainingloss 0.6931471805599453
iteration 5 batch 13460 trainingloss 0.6931471805599453
iteration 5 batch 13470 trainingloss 0.6931471805599453
iteration 5 batch 13480 trainingloss 0.6931471805599453
iteration 5 batch 13490 trainingloss 0.6931471805599453
iteration 5 batch 13500 trainingloss 0.6931471805599453
iteration 5 batch 13510 trainingloss 0.6931471805599453
iteration 5 batch 13520 trainingloss 0.6931471805599453
iteration 5 batch 13530 trainingloss 0.6931471805599453
iteration 5 batch 13540 trainingloss 0.6931471805599453
iteration 5 batch 13550 trainingloss 0.6931471805599453
iteration 5 batch 13560 trainingloss 0.6931471805599453
iteration 5 batch 13570 trainingloss 0.6931471805599453
iteration 5 batch 13580 trainingloss 0.6931471805599453
iteration 5 batch 13590 trainingloss 0.6931471805599453
iteration 5 batch 13600 trainingloss 0.6931471805599453
iteration 5 batch 13610 trainingloss 0.6916632528527511
iteration 5 batch 13620 trainingloss 0.6916632528527511
iteration 5 batch 13630 trainingloss 0.6931471805599453
iteration 5 batch 13640 trainingloss 0.6931471805599453
iteration 5 batch 13650 trainingloss 0.6931471805599453
iteration 5 batch 13660 trainingloss 0.6931471805599453
iteration 5 batch 13670 trainingloss 0.6931471805599453
iteration 5 batch 13680 trainingloss 0.6931471805599453
iteration 5 batch 13690 trainingloss 0.6931471805599453
iteration 5 batch 13700 trainingloss 0.6931471805599453
iteration 5 batch 13710 trainingloss 0.6931471805599453
iteration 5 batch 13720 trainingloss 0.6931471805599453
iteration 5 batch 13730 trainingloss 0.6931471805599453
iteration 5 batch 13740 trainingloss 0.6931471805599453
iteration 5 batch 13750 trainingloss 0.6916632528527511
iteration 5 batch 13760 trainingloss 0.6931471805599453
iteration 5 batch 13770 trainingloss 0.6931471805599453
iteration 5 batch 13780 trainingloss 0.6931471805599453
iteration 5 batch 13790 trainingloss 0.6931471805599453
iteration 5 batch 13800 trainingloss 0.6931471805599453
iteration 5 batch 13810 trainingloss 0.6931471805599453
iteration 5 batch 13820 trainingloss 0.6931471805599453
iteration 5 batch 13830 trainingloss 0.6916632528527511
iteration 5 batch 13840 trainingloss 0.6931471805599453
iteration 5 batch 13850 trainingloss 0.6916632528527511
iteration 5 batch 13860 trainingloss 0.6931471805599453
iteration 5 batch 13870 trainingloss 0.6916632528527511
iteration 5 batch 13880 trainingloss 0.6931471805599453
iteration 5 batch 13890 trainingloss 0.6931471805599453
iteration 5 batch 13900 trainingloss 0.6931471805599453
iteration 5 batch 13910 trainingloss 0.6931471805599453
iteration 5 batch 13920 trainingloss 0.6931471805599453
iteration 5 batch 13930 trainingloss 0.6916632528527511
iteration 5 batch 13940 trainingloss 0.6931471805599453
iteration 5 batch 13950 trainingloss 0.6931471805599453
iteration 5 batch 13960 trainingloss 0.6931471805599453
iteration 5 batch 13970 trainingloss 0.6931471805599453
iteration 5 batch 13980 trainingloss 0.6931471805599453
iteration 5 batch 13990 trainingloss 0.6931471805599453
iteration 5 batch 14000 trainingloss 0.6931471805599453
iteration 5 batch 14010 trainingloss 0.6931471805599453
iteration 5 batch 14020 trainingloss 0.6931471805599453
iteration 5 batch 14030 trainingloss 0.6931471805599453
iteration 5 batch 14040 trainingloss 0.6931471805599453
iteration 5 batch 14050 trainingloss 0.6916632528527511
iteration 5 batch 14060 trainingloss 0.6916632528527511
iteration 5 batch 14070 trainingloss 0.6931471805599453
iteration 5 batch 14080 trainingloss 0.6931471805599453
iteration 5 batch 14090 trainingloss 0.6931471805599453
iteration 5 batch 14100 trainingloss 0.6931471805599453
iteration 5 batch 14110 trainingloss 0.6931471805599453
iteration 5 batch 14120 trainingloss 0.6931471805599453
iteration 5 batch 14130 trainingloss 0.6931471805599453
iteration 5 batch 14140 trainingloss 0.6931471805599453
iteration 5 batch 14150 trainingloss 0.6931471805599453
iteration 5 batch 14160 trainingloss 0.6931471805599453
iteration 5 batch 14170 trainingloss 0.6931471805599453
iteration 5 batch 14180 trainingloss 0.6931471805599453
iteration 5 batch 14190 trainingloss 0.6931471805599453
iteration 5 batch 14200 trainingloss 0.6931471805599453
iteration 5 batch 14210 trainingloss 0.6931471805599453
iteration 5 batch 14220 trainingloss 0.6931471805599453
iteration 5 batch 14230 trainingloss 0.6931471805599453
iteration 5 batch 14240 trainingloss 0.6931471805599453
iteration 5 batch 14250 trainingloss 0.6931471805599453
iteration 5 batch 14260 trainingloss 0.6931471805599453
iteration 5 batch 14270 trainingloss 0.6931471805599453
iteration 5 batch 14280 trainingloss 0.6931471805599453
iteration 5 batch 14290 trainingloss 0.6931471805599453
iteration 5 batch 14300 trainingloss 0.6931471805599453
iteration 5 batch 14310 trainingloss 0.6931471805599453
iteration 5 batch 14320 trainingloss 0.6931471805599453
iteration 5 batch 14330 trainingloss 0.6931471805599453
iteration 5 batch 14340 trainingloss 0.6931471805599453
iteration 5 batch 14350 trainingloss 0.6931471805599453
iteration 5 batch 14360 trainingloss 0.6931471805599453
iteration 5 batch 14370 trainingloss 0.6931471805599453
iteration 5 batch 14380 trainingloss 0.6931471805599453
iteration 5 batch 14390 trainingloss 0.6931471805599453
iteration 5 batch 14400 trainingloss 0.6931471805599453
iteration 5 batch 14410 trainingloss 0.6931471805599453
iteration 5 batch 14420 trainingloss 0.6931471805599453
iteration 5 batch 14430 trainingloss 0.6931471805599453
iteration 5 batch 14440 trainingloss 0.6931471805599453
iteration 5 batch 14450 trainingloss 0.6931471805599453
iteration 5 batch 14460 trainingloss 0.6931471805599453
iteration 5 batch 14470 trainingloss 0.6931471805599453
iteration 5 batch 14480 trainingloss 0.6931471805599453
iteration 5 batch 14490 trainingloss 0.6931471805599453
iteration 5 batch 14500 trainingloss 0.6931471805599453
iteration 5 batch 14510 trainingloss 0.6931471805599453
iteration 5 batch 14520 trainingloss 0.6931471805599453
iteration 5 batch 14530 trainingloss 0.6931471805599453
iteration 5 batch 14540 trainingloss 0.6931471805599453
iteration 5 batch 14550 trainingloss 0.6931471805599453
iteration 5 batch 14560 trainingloss 0.6931471805599453
iteration 5 batch 14570 trainingloss 0.6931471805599453
iteration 5 batch 14580 trainingloss 0.6916632528527511
iteration 5 batch 14590 trainingloss 0.6931471805599453
iteration 5 batch 14600 trainingloss 0.6931471805599453
iteration 5 batch 14610 trainingloss 0.6931471805599453
iteration 5 batch 14620 trainingloss 0.6931471805599453
iteration 5 batch 14630 trainingloss 0.6931471805599453
iteration 5 batch 14640 trainingloss 0.6931471805599453
iteration 5 batch 14650 trainingloss 0.6931471805599453
iteration 5 batch 14660 trainingloss 0.6931471805599453
iteration 5 batch 14670 trainingloss 0.6931471805599453
iteration 5 batch 14680 trainingloss 0.6931471805599453
iteration 5 batch 14690 trainingloss 0.6931471805599453
iteration 5 batch 14700 trainingloss 0.6931471805599453
iteration 5 batch 14710 trainingloss 0.6931471805599453
iteration 5 batch 14720 trainingloss 0.6931471805599453
iteration 5 batch 14730 trainingloss 0.6931471805599453
iteration 5 batch 14740 trainingloss 0.6931471805599453
iteration 5 batch 14750 trainingloss 0.6931471805599453
iteration 5 batch 14760 trainingloss 0.6931471805599453
iteration 5 batch 14770 trainingloss 0.6931471805599453
iteration 5 batch 14780 trainingloss 0.6931471805599453
iteration 5 batch 14790 trainingloss 0.6931471805599453
iteration 5 batch 14800 trainingloss 0.6931471805599453
iteration 5 batch 14810 trainingloss 0.6931471805599453
iteration 5 batch 14820 trainingloss 0.6931471805599453
iteration 5 batch 14830 trainingloss 0.6931471805599453
iteration 5 batch 14840 trainingloss 0.6931471805599453
iteration 5 batch 14850 trainingloss 0.6931471805599453
iteration 5 batch 14860 trainingloss 0.6931471805599453
iteration 5 batch 14870 trainingloss 0.6931471805599453
iteration 5 batch 14880 trainingloss 0.6931471805599453
iteration 5 batch 14890 trainingloss 0.6931471805599453
iteration 5 batch 14900 trainingloss 0.6931471805599453
iteration 5 batch 14910 trainingloss 0.6931471805599453
iteration 5 batch 14920 trainingloss 0.6931471805599453
iteration 5 batch 14930 trainingloss 0.6916632528527511
iteration 5 batch 14940 trainingloss 0.6931471805599453
iteration 5 batch 14950 trainingloss 0.6931471805599453
iteration 5 batch 14960 trainingloss 0.6931471805599453
iteration 5 batch 14970 trainingloss 0.6931471805599453
iteration 5 batch 14980 trainingloss 0.6931471805599453
iteration 5 batch 14990 trainingloss 0.6931471805599453
iteration 5 batch 15000 trainingloss 0.6931471805599453
iteration 5 batch 15010 trainingloss 0.6931471805599453
iteration 5 batch 15020 trainingloss 0.6931471805599453
iteration 5 batch 15030 trainingloss 0.6931471805599453
iteration 5 batch 15040 trainingloss 0.6931471805599453
iteration 5 batch 15050 trainingloss 0.6931471805599453
iteration 5 batch 15060 trainingloss 0.6931471805599453
iteration 5 batch 15070 trainingloss 0.6931471805599453
iteration 5 batch 15080 trainingloss 0.6931471805599453
iteration 5 batch 15090 trainingloss 0.6931471805599453
iteration 5 batch 15100 trainingloss 0.6916632528527511
iteration 5 batch 15110 trainingloss 0.6931471805599453
iteration 5 batch 15120 trainingloss 0.6931471805599453
iteration 5 batch 15130 trainingloss 0.6931471805599453
iteration 5 batch 15140 trainingloss 0.6931471805599453
iteration 5 batch 15150 trainingloss 0.6916632528527511
iteration 5 batch 15160 trainingloss 0.6931471805599453
iteration 5 batch 15170 trainingloss 0.6931471805599453
iteration 5 batch 15180 trainingloss 0.6931471805599453
iteration 5 batch 15190 trainingloss 0.6931471805599453
iteration 5 batch 15200 trainingloss 0.6931471805599453
iteration 5 batch 15210 trainingloss 0.6931471805599453
iteration 5 batch 15220 trainingloss 0.6931471805599453
iteration 5 batch 15230 trainingloss 0.6931471805599453
iteration 5 batch 15240 trainingloss 0.6931471805599453
iteration 5 batch 15250 trainingloss 0.6931471805599453
iteration 5 batch 15260 trainingloss 0.6931471805599453
iteration 5 batch 15270 trainingloss 0.6931471805599453
iteration 5 batch 15280 trainingloss 0.6931471805599453
iteration 5 batch 15290 trainingloss 0.6931471805599453
iteration 5 batch 15300 trainingloss 0.6931471805599453
iteration 5 batch 15310 trainingloss 0.6931471805599453
iteration 5 batch 15320 trainingloss 0.6931471805599453
iteration 5 batch 15330 trainingloss 0.6931471805599453
iteration 5 batch 15340 trainingloss 0.6931471805599453
iteration 5 batch 15350 trainingloss 0.6931471805599453
iteration 5 batch 15360 trainingloss 0.6931471805599453
iteration 5 batch 15370 trainingloss 0.6916632528527511
iteration 5 batch 15380 trainingloss 0.6931471805599453
iteration 5 batch 15390 trainingloss 0.6931471805599453
iteration 5 batch 15400 trainingloss 0.6931471805599453
iteration 5 batch 15410 trainingloss 0.6931471805599453
iteration 5 batch 15420 trainingloss 0.6931471805599453
iteration 5 batch 15430 trainingloss 0.6931471805599453
iteration 5 batch 15440 trainingloss 0.6931471805599453
iteration 5 batch 15450 trainingloss 0.6931471805599453
iteration 5 batch 15460 trainingloss 0.6931471805599453
iteration 5 batch 15470 trainingloss 0.6931471805599453
iteration 5 batch 15480 trainingloss 0.6931471805599453
iteration 5 batch 15490 trainingloss 0.6931471805599453
iteration 5 batch 15500 trainingloss 0.6931471805599453
iteration 5 batch 15510 trainingloss 0.6931471805599453
iteration 5 batch 15520 trainingloss 0.6931471805599453
iteration 5 batch 15530 trainingloss 0.6931471805599453
iteration 5 batch 15540 trainingloss 0.6916632528527511
iteration 5 batch 15550 trainingloss 0.6931471805599453
iteration 5 batch 15560 trainingloss 0.6931471805599453
iteration 5 batch 15570 trainingloss 0.6931471805599453
iteration 5 batch 15580 trainingloss 0.6931471805599453
iteration 5 batch 15590 trainingloss 0.6931471805599453
iteration 5 batch 15600 trainingloss 0.6931471805599453
iteration 5 batch 15610 trainingloss 0.6931471805599453
iteration 5 batch 15620 trainingloss 0.6931471805599453
iteration 5 batch 15630 trainingloss 0.6931471805599453
iteration 5 batch 15640 trainingloss 0.6931471805599453
iteration 5 batch 15650 trainingloss 0.6931471805599453
iteration 5 batch 15660 trainingloss 0.6931471805599453
iteration 5 batch 15670 trainingloss 0.6916632528527511
iteration 5 batch 15680 trainingloss 0.6931471805599453
iteration 5 batch 15690 trainingloss 0.6931471805599453
iteration 5 batch 15700 trainingloss 0.6931471805599453
iteration 5 batch 15710 trainingloss 0.6931471805599453
iteration 5 batch 15720 trainingloss 0.6931471805599453
iteration 5 batch 15730 trainingloss 0.6931471805599453
iteration 5 batch 15740 trainingloss 0.6931471805599453
iteration 5 batch 15750 trainingloss 0.6931471805599453
iteration 5 batch 15760 trainingloss 0.6931471805599453
iteration 5 batch 15770 trainingloss 0.6931471805599453
iteration 5 batch 15780 trainingloss 0.6931471805599453
iteration 5 batch 15790 trainingloss 0.6931471805599453
iteration 5 batch 15800 trainingloss 0.6931471805599453
iteration 5 batch 15810 trainingloss 0.6931471805599453
iteration 5 batch 15820 trainingloss 0.6931471805599453
iteration 5 batch 15830 trainingloss 0.6931471805599453
iteration 5 batch 15840 trainingloss 0.6931471805599453
iteration 5 batch 15850 trainingloss 0.6931471805599453
iteration 5 batch 15860 trainingloss 0.6931471805599453
iteration 5 batch 15870 trainingloss 0.6931471805599453
iteration 5 batch 15880 trainingloss 0.6931471805599453
iteration 5 batch 15890 trainingloss 0.6931471805599453
iteration 5 batch 15900 trainingloss 0.6931471805599453
iteration 5 batch 15910 trainingloss 0.6931471805599453
iteration 5 batch 15920 trainingloss 0.6931471805599453
iteration 5 batch 15930 trainingloss 0.6931471805599453
iteration 5 batch 15940 trainingloss 0.6931471805599453
iteration 5 batch 15950 trainingloss 0.6931471805599453
iteration 5 batch 15960 trainingloss 0.6931471805599453
iteration 5 batch 15970 trainingloss 0.6931471805599453
iteration 5 batch 15980 trainingloss 0.6931471805599453
iteration 5 batch 15990 trainingloss 0.6931471805599453
iteration 5 batch 16000 trainingloss 0.6931471805599453
iteration 5 batch 16010 trainingloss 0.6931471805599453
iteration 5 batch 16020 trainingloss 0.6931471805599453
iteration 5 batch 16030 trainingloss 0.6931471805599453
iteration 5 batch 16040 trainingloss 0.6931471805599453
iteration 5 batch 16050 trainingloss 0.6931471805599453
iteration 5 batch 16060 trainingloss 0.6931471805599453
iteration 5 batch 16070 trainingloss 0.6931471805599453
iteration 5 batch 16080 trainingloss 0.6931471805599453
iteration 5 batch 16090 trainingloss 0.6931471805599453
iteration 5 batch 16100 trainingloss 0.6931471805599453
iteration 5 batch 16110 trainingloss 0.6916632528527511
iteration 5 batch 16120 trainingloss 0.6916632528527511
iteration 5 batch 16130 trainingloss 0.6931471805599453
iteration 5 batch 16140 trainingloss 0.6931471805599453
iteration 5 batch 16150 trainingloss 0.6931471805599453
iteration 5 batch 16160 trainingloss 0.6931471805599453
iteration 5 batch 16170 trainingloss 0.6931471805599453
iteration 5 batch 16180 trainingloss 0.6931471805599453
iteration 5 batch 16190 trainingloss 0.6931471805599453
iteration 5 batch 16200 trainingloss 0.6931471805599453
iteration 5 batch 16210 trainingloss 0.6916632528527511
iteration 5 batch 16220 trainingloss 0.6931471805599453
iteration 5 batch 16230 trainingloss 0.6931471805599453
iteration 5 batch 16240 trainingloss 0.6931471805599453
iteration 5 batch 16250 trainingloss 0.6931471805599453
iteration 5 batch 16260 trainingloss 0.6931471805599453
iteration 5 batch 16270 trainingloss 0.6931471805599453
iteration 5 batch 16280 trainingloss 0.6931471805599453
iteration 5 batch 16290 trainingloss 0.6931471805599453
iteration 5 batch 16300 trainingloss 0.6931471805599453
iteration 5 batch 16310 trainingloss 0.6931471805599453
iteration 5 batch 16320 trainingloss 0.6931471805599453
iteration 5 batch 16330 trainingloss 0.6931471805599453
iteration 5 batch 16340 trainingloss 0.6931471805599453
iteration 5 batch 16350 trainingloss 0.6931471805599453
iteration 5 batch 16360 trainingloss 0.6931471805599453
iteration 5 batch 16370 trainingloss 0.6931471805599453
iteration 5 batch 16380 trainingloss 0.6931471805599453
iteration 5 batch 16390 trainingloss 0.6916632528527511
iteration 5 batch 16400 trainingloss 0.6931471805599453
iteration 5 batch 16410 trainingloss 0.6931471805599453
iteration 5 batch 16420 trainingloss 0.6931471805599453
iteration 5 batch 16430 trainingloss 0.6931471805599453
iteration 5 batch 16440 trainingloss 0.6931471805599453
iteration 5 batch 16450 trainingloss 0.6931471805599453
iteration 5 batch 16460 trainingloss 0.6931471805599453
iteration 5 batch 16470 trainingloss 0.6931471805599453
iteration 5 batch 16480 trainingloss 0.6931471805599453
iteration 5 batch 16490 trainingloss 0.6931471805599453
iteration 5 batch 16500 trainingloss 0.6931471805599453
iteration 5 batch 16510 trainingloss 0.6931471805599453
iteration 5 batch 16520 trainingloss 0.6931471805599453
iteration 5 batch 16530 trainingloss 0.6931471805599453
iteration 5 batch 16540 trainingloss 0.6931471805599453
iteration 5 batch 16550 trainingloss 0.6931471805599453
iteration 5 batch 16560 trainingloss 0.6916632528527511
iteration 5 batch 16570 trainingloss 0.6931471805599453
iteration 5 batch 16580 trainingloss 0.6931471805599453
iteration 5 batch 16590 trainingloss 0.6931471805599453
iteration 5 batch 16600 trainingloss 0.6931471805599453
iteration 5 batch 16610 trainingloss 0.6931471805599453
iteration 5 batch 16620 trainingloss 0.6931471805599453
iteration 5 batch 16630 trainingloss 0.6931471805599453
iteration 5 batch 16640 trainingloss 0.6916632528527511
iteration 5 batch 16650 trainingloss 0.6931471805599453
iteration 5 batch 16660 trainingloss 0.6931471805599453
iteration 5 batch 16670 trainingloss 0.6931471805599453
iteration 5 batch 16680 trainingloss 0.6931471805599453
iteration 5 batch 16690 trainingloss 0.6931471805599453
iteration 5 batch 16700 trainingloss 0.6931471805599453
iteration 5 batch 16710 trainingloss 0.6931471805599453
iteration 5 batch 16720 trainingloss 0.6931471805599453
iteration 5 batch 16730 trainingloss 0.6931471805599453
iteration 5 batch 16740 trainingloss 0.6931471805599453
iteration 5 batch 16750 trainingloss 0.6931471805599453
iteration 5 batch 16760 trainingloss 0.6931471805599453
iteration 5 batch 16770 trainingloss 0.6931471805599453
iteration 5 batch 16780 trainingloss 0.6931471805599453
iteration 5 batch 16790 trainingloss 0.6931471805599453
iteration 5 batch 16800 trainingloss 0.6931471805599453
iteration 5 batch 16810 trainingloss 0.6931471805599453
iteration 5 batch 16820 trainingloss 0.6931471805599453
iteration 5 batch 16830 trainingloss 0.6931471805599453
iteration 5 batch 16840 trainingloss 0.6931471805599453
iteration 5 batch 16850 trainingloss 0.6931471805599453
iteration 5 batch 16860 trainingloss 0.6931471805599453
iteration 5 batch 16870 trainingloss 0.6931471805599453
iteration 5 batch 16880 trainingloss 0.6931471805599453
iteration 5 batch 16890 trainingloss 0.6931471805599453
iteration 5 batch 16900 trainingloss 0.6916632528527511
iteration 5 batch 16910 trainingloss 0.6931471805599453
iteration 5 batch 16920 trainingloss 0.6931471805599453
iteration 5 batch 16930 trainingloss 0.6931471805599453
iteration 5 batch 16940 trainingloss 0.6931471805599453
iteration 5 batch 16950 trainingloss 0.6931471805599453
iteration 5 batch 16960 trainingloss 0.6931471805599453
iteration 5 batch 16970 trainingloss 0.6931471805599453
iteration 5 batch 16980 trainingloss 0.6931471805599453
iteration 5 batch 16990 trainingloss 0.6916632528527511
iteration 5 batch 17000 trainingloss 0.6931471805599453
iteration 5 batch 17010 trainingloss 0.6931471805599453
iteration 5 batch 17020 trainingloss 0.6931471805599453
iteration 5 batch 17030 trainingloss 0.6931471805599453
iteration 5 batch 17040 trainingloss 0.6931471805599453
iteration 5 batch 17050 trainingloss 0.6916632528527511
iteration 5 batch 17060 trainingloss 0.6931471805599453
iteration 5 batch 17070 trainingloss 0.6931471805599453
iteration 5 batch 17080 trainingloss 0.6931471805599453
iteration 5 batch 17090 trainingloss 0.6931471805599453
iteration 5 batch 17100 trainingloss 0.6931471805599453
iteration 5 batch 17110 trainingloss 0.6931471805599453
iteration 5 batch 17120 trainingloss 0.6931471805599453
iteration 5 batch 17130 trainingloss 0.6931471805599453
iteration 5 batch 17140 trainingloss 0.6931471805599453
iteration 5 batch 17150 trainingloss 0.6931471805599453
iteration 5 batch 17160 trainingloss 0.6931471805599453
iteration 5 batch 17170 trainingloss 0.6931471805599453
iteration 5 batch 17180 trainingloss 0.6931471805599453
iteration 5 batch 17190 trainingloss 0.6931471805599453
iteration 5 batch 17200 trainingloss 0.6931471805599453
iteration 5 batch 17210 trainingloss 0.6931471805599453
iteration 5 batch 17220 trainingloss 0.6931471805599453
iteration 5 batch 17230 trainingloss 0.6931471805599453
iteration 5 batch 17240 trainingloss 0.6931471805599453
iteration 5 batch 17250 trainingloss 0.6916632528527511
iteration 5 batch 17260 trainingloss 0.6931471805599453
iteration 5 batch 17270 trainingloss 0.6931471805599453
iteration 5 batch 17280 trainingloss 0.6931471805599453
iteration 5 batch 17290 trainingloss 0.6931471805599453
iteration 5 batch 17300 trainingloss 0.6931471805599453
iteration 5 batch 17310 trainingloss 0.6931471805599453
iteration 5 batch 17320 trainingloss 0.6931471805599453
iteration 5 batch 17330 trainingloss 0.6916632528527511
iteration 5 batch 17340 trainingloss 0.6931471805599453
iteration 5 batch 17350 trainingloss 0.6931471805599453
iteration 5 batch 17360 trainingloss 0.6931471805599453
iteration 5 batch 17370 trainingloss 0.6931471805599453
iteration 5 batch 17380 trainingloss 0.6931471805599453
iteration 5 batch 17390 trainingloss 0.6931471805599453
iteration 5 batch 17400 trainingloss 0.6931471805599453
iteration 5 batch 17410 trainingloss 0.6931471805599453
iteration 5 batch 17420 trainingloss 0.6931471805599453
iteration 5 batch 17430 trainingloss 0.6931471805599453
iteration 5 batch 17440 trainingloss 0.6931471805599453
iteration 5 batch 17450 trainingloss 0.6916632528527511
iteration 5 batch 17460 trainingloss 0.6931471805599453
iteration 5 batch 17470 trainingloss 0.6931471805599453
iteration 5 batch 17480 trainingloss 0.6931471805599453
iteration 5 batch 17490 trainingloss 0.6931471805599453
iteration 5 batch 17500 trainingloss 0.6916632528527511
iteration 5 batch 17510 trainingloss 0.6931471805599453
iteration 5 batch 17520 trainingloss 0.6931471805599453
iteration 5 batch 17530 trainingloss 0.6931471805599453
iteration 5 batch 17540 trainingloss 0.6931471805599453
iteration 5 batch 17550 trainingloss 0.6931471805599453
iteration 5 batch 17560 trainingloss 0.6931471805599453
iteration 5 batch 17570 trainingloss 0.6931471805599453
iteration 5 batch 17580 trainingloss 0.6931471805599453
iteration 5 batch 17590 trainingloss 0.6931471805599453
iteration 5 batch 17600 trainingloss 0.6931471805599453
iteration 5 batch 17610 trainingloss 0.6931471805599453
iteration 5 batch 17620 trainingloss 0.6931471805599453
iteration 5 batch 17630 trainingloss 0.6931471805599453
iteration 5 batch 17640 trainingloss 0.6931471805599453
iteration 5 batch 17650 trainingloss 0.6931471805599453
iteration 5 batch 17660 trainingloss 0.6931471805599453
iteration 5 batch 17670 trainingloss 0.6931471805599453
iteration 5 batch 17680 trainingloss 0.6931471805599453
iteration 5 batch 17690 trainingloss 0.6931471805599453
iteration 5 batch 17700 trainingloss 0.6931471805599453
iteration 5 batch 17710 trainingloss 0.6931471805599453
iteration 5 batch 17720 trainingloss 0.6931471805599453
iteration 5 batch 17730 trainingloss 0.6916632528527511
iteration 5 batch 17740 trainingloss 0.6931471805599453
iteration 5 batch 17750 trainingloss 0.6931471805599453
iteration 5 batch 17760 trainingloss 0.6931471805599453
iteration 5 batch 17770 trainingloss 0.6931471805599453
iteration 5 batch 17780 trainingloss 0.6931471805599453
iteration 5 batch 17790 trainingloss 0.6931471805599453
iteration 5 batch 17800 trainingloss 0.6931471805599453
iteration 5 batch 17810 trainingloss 0.6931471805599453
iteration 5 batch 17820 trainingloss 0.6931471805599453
iteration 5 batch 17830 trainingloss 0.6916632528527511
iteration 5 batch 17840 trainingloss 0.6931471805599453
iteration 5 batch 17850 trainingloss 0.6931471805599453
iteration 5 batch 17860 trainingloss 0.6931471805599453
iteration 5 batch 17870 trainingloss 0.6931471805599453
iteration 5 batch 17880 trainingloss 0.6931471805599453
iteration 5 batch 17890 trainingloss 0.6931471805599453
iteration 5 batch 17900 trainingloss 0.6931471805599453
iteration 5 batch 17910 trainingloss 0.6931471805599453
iteration 5 batch 17920 trainingloss 0.6931471805599453
iteration 5 batch 17930 trainingloss 0.6931471805599453
iteration 5 batch 17940 trainingloss 0.6931471805599453
iteration 5 batch 17950 trainingloss 0.6931471805599453
iteration 5 batch 17960 trainingloss 0.6931471805599453
iteration 5 batch 17970 trainingloss 0.6931471805599453
iteration 5 batch 17980 trainingloss 0.6931471805599453
iteration 5 batch 17990 trainingloss 0.6931471805599453
iteration 5 batch 18000 trainingloss 0.6931471805599453
iteration 5 batch 18010 trainingloss 0.6931471805599453
iteration 5 batch 18020 trainingloss 0.6931471805599453
iteration 5 batch 18030 trainingloss 0.6931471805599453
iteration 5 batch 18040 trainingloss 0.6931471805599453
iteration 5 batch 18050 trainingloss 0.6931471805599453
iteration 5 batch 18060 trainingloss 0.6931471805599453
iteration 5 batch 18070 trainingloss 0.6931471805599453
iteration 5 batch 18080 trainingloss 0.6931471805599453
iteration 5 batch 18090 trainingloss 0.6931471805599453
iteration 5 batch 18100 trainingloss 0.6931471805599453
iteration 5 batch 18110 trainingloss 0.6931471805599453
iteration 5 batch 18120 trainingloss 0.6931471805599453
iteration 5 batch 18130 trainingloss 0.6931471805599453
iteration 5 batch 18140 trainingloss 0.6931471805599453
iteration 5 batch 18150 trainingloss 0.6931471805599453
iteration 5 batch 18160 trainingloss 0.6931471805599453
iteration 5 batch 18170 trainingloss 0.6931471805599453
iteration 5 batch 18180 trainingloss 0.6916632528527511
iteration 5 batch 18190 trainingloss 0.6931471805599453
iteration 5 batch 18200 trainingloss 0.6931471805599453
iteration 5 batch 18210 trainingloss 0.6931471805599453
iteration 5 batch 18220 trainingloss 0.6931471805599453
iteration 5 batch 18230 trainingloss 0.6931471805599453
iteration 5 batch 18240 trainingloss 0.6931471805599453
iteration 5 batch 18250 trainingloss 0.6931471805599453
iteration 5 batch 18260 trainingloss 0.6931471805599453
iteration 5 batch 18270 trainingloss 0.6931471805599453
iteration 5 batch 18280 trainingloss 0.6931471805599453
iteration 5 batch 18290 trainingloss 0.6931471805599453
iteration 5 batch 18300 trainingloss 0.6931471805599453
iteration 5 batch 18310 trainingloss 0.6931471805599453
iteration 5 batch 18320 trainingloss 0.6931471805599453
iteration 5 batch 18330 trainingloss 0.6931471805599453
iteration 5 batch 18340 trainingloss 0.6916632528527511
iteration 5 batch 18350 trainingloss 0.6931471805599453
iteration 5 batch 18360 trainingloss 0.6931471805599453
iteration 5 batch 18370 trainingloss 0.6931471805599453
iteration 5 batch 18380 trainingloss 0.6931471805599453
iteration 5 batch 18390 trainingloss 0.6901793251455568
iteration 5 batch 18400 trainingloss 0.6931471805599453
iteration 5 batch 18410 trainingloss 0.6931471805599453
iteration 5 batch 18420 trainingloss 0.6931471805599453
iteration 5 batch 18430 trainingloss 0.6931471805599453
iteration 5 batch 18440 trainingloss 0.6931471805599453
iteration 5 batch 18450 trainingloss 0.6931471805599453
iteration 5 batch 18460 trainingloss 0.6931471805599453
iteration 5 batch 18470 trainingloss 0.6931471805599453
iteration 5 batch 18480 trainingloss 0.6931471805599453
iteration 5 batch 18490 trainingloss 0.6931471805599453
iteration 5 batch 18500 trainingloss 0.6931471805599453
iteration 5 batch 18510 trainingloss 0.6931471805599453
iteration 5 batch 18520 trainingloss 0.6931471805599453
iteration 5 batch 18530 trainingloss 0.6931471805599453
iteration 5 batch 18540 trainingloss 0.6931471805599453
iteration 5 batch 18550 trainingloss 0.6931471805599453
iteration 5 batch 18560 trainingloss 0.6931471805599453
iteration 5 batch 18570 trainingloss 0.6931471805599453
iteration 5 batch 18580 trainingloss 0.6931471805599453
iteration 5 batch 18590 trainingloss 0.6916632528527511
iteration 5 batch 18600 trainingloss 0.6931471805599453
iteration 5 batch 18610 trainingloss 0.6931471805599453
iteration 6 batch 0 trainingloss 0.6931471805599453
iteration 6 batch 10 trainingloss 0.6931471805599453
iteration 6 batch 20 trainingloss 0.6931471805599453
iteration 6 batch 30 trainingloss 0.6931471805599453
iteration 6 batch 40 trainingloss 0.6931471805599453
iteration 6 batch 50 trainingloss 0.6916632528527511
iteration 6 batch 60 trainingloss 0.6931471805599453
iteration 6 batch 70 trainingloss 0.6931471805599453
iteration 6 batch 80 trainingloss 0.6931471805599453
iteration 6 batch 90 trainingloss 0.6931471805599453
iteration 6 batch 100 trainingloss 0.6931471805599453
iteration 6 batch 110 trainingloss 0.6931471805599453
iteration 6 batch 120 trainingloss 0.6931471805599453
iteration 6 batch 130 trainingloss 0.6931471805599453
iteration 6 batch 140 trainingloss 0.6931471805599453
iteration 6 batch 150 trainingloss 0.6931471805599453
iteration 6 batch 160 trainingloss 0.6931471805599453
iteration 6 batch 170 trainingloss 0.6931471805599453
iteration 6 batch 180 trainingloss 0.6931471805599453
iteration 6 batch 190 trainingloss 0.6931471805599453
iteration 6 batch 200 trainingloss 0.6931471805599453
iteration 6 batch 210 trainingloss 0.6931471805599453
iteration 6 batch 220 trainingloss 0.6931471805599453
iteration 6 batch 230 trainingloss 0.6931471805599453
iteration 6 batch 240 trainingloss 0.6931471805599453
iteration 6 batch 250 trainingloss 0.6931471805599453
iteration 6 batch 260 trainingloss 0.6931471805599453
iteration 6 batch 270 trainingloss 0.6931471805599453
iteration 6 batch 280 trainingloss 0.6931471805599453
iteration 6 batch 290 trainingloss 0.6931471805599453
iteration 6 batch 300 trainingloss 0.6931471805599453
iteration 6 batch 310 trainingloss 0.6931471805599453
iteration 6 batch 320 trainingloss 0.6931471805599453
iteration 6 batch 330 trainingloss 0.6931471805599453
iteration 6 batch 340 trainingloss 0.6931471805599453
iteration 6 batch 350 trainingloss 0.6931471805599453
iteration 6 batch 360 trainingloss 0.6931471805599453
iteration 6 batch 370 trainingloss 0.6931471805599453
iteration 6 batch 380 trainingloss 0.6931471805599453
iteration 6 batch 390 trainingloss 0.6931471805599453
iteration 6 batch 400 trainingloss 0.6916632528527511
iteration 6 batch 410 trainingloss 0.6931471805599453
iteration 6 batch 420 trainingloss 0.6931471805599453
iteration 6 batch 430 trainingloss 0.6916632528527511
iteration 6 batch 440 trainingloss 0.6931471805599453
iteration 6 batch 450 trainingloss 0.6931471805599453
iteration 6 batch 460 trainingloss 0.6931471805599453
iteration 6 batch 470 trainingloss 0.6931471805599453
iteration 6 batch 480 trainingloss 0.6931471805599453
iteration 6 batch 490 trainingloss 0.6931471805599453
iteration 6 batch 500 trainingloss 0.6931471805599453
iteration 6 batch 510 trainingloss 0.6931471805599453
iteration 6 batch 520 trainingloss 0.6931471805599453
iteration 6 batch 530 trainingloss 0.6931471805599453
iteration 6 batch 540 trainingloss 0.6931471805599453
iteration 6 batch 550 trainingloss 0.6931471805599453
iteration 6 batch 560 trainingloss 0.6931471805599453
iteration 6 batch 570 trainingloss 0.6931471805599453
iteration 6 batch 580 trainingloss 0.6931471805599453
iteration 6 batch 590 trainingloss 0.6931471805599453
iteration 6 batch 600 trainingloss 0.6931471805599453
iteration 6 batch 610 trainingloss 0.6931471805599453
iteration 6 batch 620 trainingloss 0.6931471805599453
iteration 6 batch 630 trainingloss 0.6931471805599453
iteration 6 batch 640 trainingloss 0.6931471805599453
iteration 6 batch 650 trainingloss 0.6931471805599453
iteration 6 batch 660 trainingloss 0.6931471805599453
iteration 6 batch 670 trainingloss 0.6931471805599453
iteration 6 batch 680 trainingloss 0.6931471805599453
iteration 6 batch 690 trainingloss 0.6916632528527511
iteration 6 batch 700 trainingloss 0.6931471805599453
iteration 6 batch 710 trainingloss 0.6931471805599453
iteration 6 batch 720 trainingloss 0.6931471805599453
iteration 6 batch 730 trainingloss 0.6931471805599453
iteration 6 batch 740 trainingloss 0.6931471805599453
iteration 6 batch 750 trainingloss 0.6931471805599453
iteration 6 batch 760 trainingloss 0.6931471805599453
iteration 6 batch 770 trainingloss 0.6931471805599453
iteration 6 batch 780 trainingloss 0.6931471805599453
iteration 6 batch 790 trainingloss 0.6931471805599453
iteration 6 batch 800 trainingloss 0.6931471805599453
iteration 6 batch 810 trainingloss 0.6931471805599453
iteration 6 batch 820 trainingloss 0.6916632528527511
iteration 6 batch 830 trainingloss 0.6916632528527511
iteration 6 batch 840 trainingloss 0.6931471805599453
iteration 6 batch 850 trainingloss 0.6931471805599453
iteration 6 batch 860 trainingloss 0.6931471805599453
iteration 6 batch 870 trainingloss 0.6931471805599453
iteration 6 batch 880 trainingloss 0.6916632528527511
iteration 6 batch 890 trainingloss 0.6931471805599453
iteration 6 batch 900 trainingloss 0.6931471805599453
iteration 6 batch 910 trainingloss 0.6931471805599453
iteration 6 batch 920 trainingloss 0.6931471805599453
iteration 6 batch 930 trainingloss 0.6931471805599453
iteration 6 batch 940 trainingloss 0.6931471805599453
iteration 6 batch 950 trainingloss 0.6931471805599453
iteration 6 batch 960 trainingloss 0.6931471805599453
iteration 6 batch 970 trainingloss 0.6931471805599453
iteration 6 batch 980 trainingloss 0.6916632528527511
iteration 6 batch 990 trainingloss 0.6931471805599453
iteration 6 batch 1000 trainingloss 0.6931471805599453
iteration 6 batch 1010 trainingloss 0.6931471805599453
iteration 6 batch 1020 trainingloss 0.6931471805599453
iteration 6 batch 1030 trainingloss 0.6931471805599453
iteration 6 batch 1040 trainingloss 0.6931471805599453
iteration 6 batch 1050 trainingloss 0.6931471805599453
iteration 6 batch 1060 trainingloss 0.6931471805599453
iteration 6 batch 1070 trainingloss 0.6931471805599453
iteration 6 batch 1080 trainingloss 0.6931471805599453
iteration 6 batch 1090 trainingloss 0.6931471805599453
iteration 6 batch 1100 trainingloss 0.6931471805599453
iteration 6 batch 1110 trainingloss 0.6931471805599453
iteration 6 batch 1120 trainingloss 0.6931471805599453
iteration 6 batch 1130 trainingloss 0.6931471805599453
iteration 6 batch 1140 trainingloss 0.6931471805599453
iteration 6 batch 1150 trainingloss 0.6931471805599453
iteration 6 batch 1160 trainingloss 0.6931471805599453
iteration 6 batch 1170 trainingloss 0.6931471805599453
iteration 6 batch 1180 trainingloss 0.6931471805599453
iteration 6 batch 1190 trainingloss 0.6931471805599453
iteration 6 batch 1200 trainingloss 0.6931471805599453
iteration 6 batch 1210 trainingloss 0.6931471805599453
iteration 6 batch 1220 trainingloss 0.6931471805599453
iteration 6 batch 1230 trainingloss 0.6916632528527511
iteration 6 batch 1240 trainingloss 0.6931471805599453
iteration 6 batch 1250 trainingloss 0.6931471805599453
iteration 6 batch 1260 trainingloss 0.6931471805599453
iteration 6 batch 1270 trainingloss 0.6931471805599453
iteration 6 batch 1280 trainingloss 0.6931471805599453
iteration 6 batch 1290 trainingloss 0.6931471805599453
iteration 6 batch 1300 trainingloss 0.6931471805599453
iteration 6 batch 1310 trainingloss 0.6931471805599453
iteration 6 batch 1320 trainingloss 0.6931471805599453
iteration 6 batch 1330 trainingloss 0.6931471805599453
iteration 6 batch 1340 trainingloss 0.6931471805599453
iteration 6 batch 1350 trainingloss 0.6931471805599453
iteration 6 batch 1360 trainingloss 0.6931471805599453
iteration 6 batch 1370 trainingloss 0.6931471805599453
iteration 6 batch 1380 trainingloss 0.6931471805599453
iteration 6 batch 1390 trainingloss 0.6931471805599453
iteration 6 batch 1400 trainingloss 0.6931471805599453
iteration 6 batch 1410 trainingloss 0.6931471805599453
iteration 6 batch 1420 trainingloss 0.6931471805599453
iteration 6 batch 1430 trainingloss 0.6931471805599453
iteration 6 batch 1440 trainingloss 0.6931471805599453
iteration 6 batch 1450 trainingloss 0.6931471805599453
iteration 6 batch 1460 trainingloss 0.6931471805599453
iteration 6 batch 1470 trainingloss 0.6931471805599453
iteration 6 batch 1480 trainingloss 0.6931471805599453
iteration 6 batch 1490 trainingloss 0.6931471805599453
iteration 6 batch 1500 trainingloss 0.6931471805599453
iteration 6 batch 1510 trainingloss 0.6931471805599453
iteration 6 batch 1520 trainingloss 0.6931471805599453
iteration 6 batch 1530 trainingloss 0.6931471805599453
iteration 6 batch 1540 trainingloss 0.6931471805599453
iteration 6 batch 1550 trainingloss 0.6931471805599453
iteration 6 batch 1560 trainingloss 0.6931471805599453
iteration 6 batch 1570 trainingloss 0.6931471805599453
iteration 6 batch 1580 trainingloss 0.6931471805599453
iteration 6 batch 1590 trainingloss 0.6931471805599453
iteration 6 batch 1600 trainingloss 0.6931471805599453
iteration 6 batch 1610 trainingloss 0.6931471805599453
iteration 6 batch 1620 trainingloss 0.6931471805599453
iteration 6 batch 1630 trainingloss 0.6931471805599453
iteration 6 batch 1640 trainingloss 0.6931471805599453
iteration 6 batch 1650 trainingloss 0.6931471805599453
iteration 6 batch 1660 trainingloss 0.6931471805599453
iteration 6 batch 1670 trainingloss 0.6916632528527511
iteration 6 batch 1680 trainingloss 0.6931471805599453
iteration 6 batch 1690 trainingloss 0.6931471805599453
iteration 6 batch 1700 trainingloss 0.6931471805599453
iteration 6 batch 1710 trainingloss 0.6931471805599453
iteration 6 batch 1720 trainingloss 0.6931471805599453
iteration 6 batch 1730 trainingloss 0.6931471805599453
iteration 6 batch 1740 trainingloss 0.6916632528527511
iteration 6 batch 1750 trainingloss 0.6931471805599453
iteration 6 batch 1760 trainingloss 0.6931471805599453
iteration 6 batch 1770 trainingloss 0.6931471805599453
iteration 6 batch 1780 trainingloss 0.6931471805599453
iteration 6 batch 1790 trainingloss 0.6931471805599453
iteration 6 batch 1800 trainingloss 0.6931471805599453
iteration 6 batch 1810 trainingloss 0.6931471805599453
iteration 6 batch 1820 trainingloss 0.6931471805599453
iteration 6 batch 1830 trainingloss 0.6931471805599453
iteration 6 batch 1840 trainingloss 0.6931471805599453
iteration 6 batch 1850 trainingloss 0.6916632528527511
iteration 6 batch 1860 trainingloss 0.6931471805599453
iteration 6 batch 1870 trainingloss 0.6931471805599453
iteration 6 batch 1880 trainingloss 0.6931471805599453
iteration 6 batch 1890 trainingloss 0.6931471805599453
iteration 6 batch 1900 trainingloss 0.6931471805599453
iteration 6 batch 1910 trainingloss 0.6931471805599453
iteration 6 batch 1920 trainingloss 0.6931471805599453
iteration 6 batch 1930 trainingloss 0.6931471805599453
iteration 6 batch 1940 trainingloss 0.6931471805599453
iteration 6 batch 1950 trainingloss 0.6931471805599453
iteration 6 batch 1960 trainingloss 0.6931471805599453
iteration 6 batch 1970 trainingloss 0.6931471805599453
iteration 6 batch 1980 trainingloss 0.6931471805599453
iteration 6 batch 1990 trainingloss 0.6931471805599453
iteration 6 batch 2000 trainingloss 0.6931471805599453
iteration 6 batch 2010 trainingloss 0.6931471805599453
iteration 6 batch 2020 trainingloss 0.6931471805599453
iteration 6 batch 2030 trainingloss 0.6931471805599453
iteration 6 batch 2040 trainingloss 0.6931471805599453
iteration 6 batch 2050 trainingloss 0.6931471805599453
iteration 6 batch 2060 trainingloss 0.6931471805599453
iteration 6 batch 2070 trainingloss 0.6931471805599453
iteration 6 batch 2080 trainingloss 0.6931471805599453
iteration 6 batch 2090 trainingloss 0.6931471805599453
iteration 6 batch 2100 trainingloss 0.6931471805599453
iteration 6 batch 2110 trainingloss 0.6931471805599453
iteration 6 batch 2120 trainingloss 0.6931471805599453
iteration 6 batch 2130 trainingloss 0.6931471805599453
iteration 6 batch 2140 trainingloss 0.6931471805599453
iteration 6 batch 2150 trainingloss 0.6931471805599453
iteration 6 batch 2160 trainingloss 0.6931471805599453
iteration 6 batch 2170 trainingloss 0.6931471805599453
iteration 6 batch 2180 trainingloss 0.6931471805599453
iteration 6 batch 2190 trainingloss 0.6931471805599453
iteration 6 batch 2200 trainingloss 0.6931471805599453
iteration 6 batch 2210 trainingloss 0.6931471805599453
iteration 6 batch 2220 trainingloss 0.6931471805599453
iteration 6 batch 2230 trainingloss 0.6931471805599453
iteration 6 batch 2240 trainingloss 0.6931471805599453
iteration 6 batch 2250 trainingloss 0.6931471805599453
iteration 6 batch 2260 trainingloss 0.6931471805599453
iteration 6 batch 2270 trainingloss 0.6931471805599453
iteration 6 batch 2280 trainingloss 0.6931471805599453
iteration 6 batch 2290 trainingloss 0.6931471805599453
iteration 6 batch 2300 trainingloss 0.6931471805599453
iteration 6 batch 2310 trainingloss 0.6931471805599453
iteration 6 batch 2320 trainingloss 0.6931471805599453
iteration 6 batch 2330 trainingloss 0.6931471805599453
iteration 6 batch 2340 trainingloss 0.6931471805599453
iteration 6 batch 2350 trainingloss 0.6931471805599453
iteration 6 batch 2360 trainingloss 0.6931471805599453
iteration 6 batch 2370 trainingloss 0.6931471805599453
iteration 6 batch 2380 trainingloss 0.6931471805599453
iteration 6 batch 2390 trainingloss 0.6931471805599453
iteration 6 batch 2400 trainingloss 0.6931471805599453
iteration 6 batch 2410 trainingloss 0.6931471805599453
iteration 6 batch 2420 trainingloss 0.6931471805599453
iteration 6 batch 2430 trainingloss 0.6931471805599453
iteration 6 batch 2440 trainingloss 0.6931471805599453
iteration 6 batch 2450 trainingloss 0.6931471805599453
iteration 6 batch 2460 trainingloss 0.6931471805599453
iteration 6 batch 2470 trainingloss 0.6931471805599453
iteration 6 batch 2480 trainingloss 0.6931471805599453
iteration 6 batch 2490 trainingloss 0.6931471805599453
iteration 6 batch 2500 trainingloss 0.6931471805599453
iteration 6 batch 2510 trainingloss 0.6931471805599453
iteration 6 batch 2520 trainingloss 0.6931471805599453
iteration 6 batch 2530 trainingloss 0.6931471805599453
iteration 6 batch 2540 trainingloss 0.6931471805599453
iteration 6 batch 2550 trainingloss 0.6931471805599453
iteration 6 batch 2560 trainingloss 0.6931471805599453
iteration 6 batch 2570 trainingloss 0.6931471805599453
iteration 6 batch 2580 trainingloss 0.6931471805599453
iteration 6 batch 2590 trainingloss 0.6931471805599453
iteration 6 batch 2600 trainingloss 0.6931471805599453
iteration 6 batch 2610 trainingloss 0.6931471805599453
iteration 6 batch 2620 trainingloss 0.6931471805599453
iteration 6 batch 2630 trainingloss 0.6931471805599453
iteration 6 batch 2640 trainingloss 0.6931471805599453
iteration 6 batch 2650 trainingloss 0.6931471805599453
iteration 6 batch 2660 trainingloss 0.6931471805599453
iteration 6 batch 2670 trainingloss 0.6931471805599453
iteration 6 batch 2680 trainingloss 0.6931471805599453
iteration 6 batch 2690 trainingloss 0.6931471805599453
iteration 6 batch 2700 trainingloss 0.6931471805599453
iteration 6 batch 2710 trainingloss 0.6931471805599453
iteration 6 batch 2720 trainingloss 0.6931471805599453
iteration 6 batch 2730 trainingloss 0.6916632528527511
iteration 6 batch 2740 trainingloss 0.6931471805599453
iteration 6 batch 2750 trainingloss 0.6931471805599453
iteration 6 batch 2760 trainingloss 0.6931471805599453
iteration 6 batch 2770 trainingloss 0.6931471805599453
iteration 6 batch 2780 trainingloss 0.6931471805599453
iteration 6 batch 2790 trainingloss 0.6916632528527511
iteration 6 batch 2800 trainingloss 0.6931471805599453
iteration 6 batch 2810 trainingloss 0.6931471805599453
iteration 6 batch 2820 trainingloss 0.6931471805599453
iteration 6 batch 2830 trainingloss 0.6931471805599453
iteration 6 batch 2840 trainingloss 0.6931471805599453
iteration 6 batch 2850 trainingloss 0.6931471805599453
iteration 6 batch 2860 trainingloss 0.6931471805599453
iteration 6 batch 2870 trainingloss 0.6931471805599453
iteration 6 batch 2880 trainingloss 0.6931471805599453
iteration 6 batch 2890 trainingloss 0.6931471805599453
iteration 6 batch 2900 trainingloss 0.6931471805599453
iteration 6 batch 2910 trainingloss 0.6931471805599453
iteration 6 batch 2920 trainingloss 0.6916632528527511
iteration 6 batch 2930 trainingloss 0.6931471805599453
iteration 6 batch 2940 trainingloss 0.6931471805599453
iteration 6 batch 2950 trainingloss 0.6931471805599453
iteration 6 batch 2960 trainingloss 0.6931471805599453
iteration 6 batch 2970 trainingloss 0.6931471805599453
iteration 6 batch 2980 trainingloss 0.6931471805599453
iteration 6 batch 2990 trainingloss 0.6931471805599453
iteration 6 batch 3000 trainingloss 0.6931471805599453
iteration 6 batch 3010 trainingloss 0.6931471805599453
iteration 6 batch 3020 trainingloss 0.6931471805599453
iteration 6 batch 3030 trainingloss 0.6931471805599453
iteration 6 batch 3040 trainingloss 0.6931471805599453
iteration 6 batch 3050 trainingloss 0.6931471805599453
iteration 6 batch 3060 trainingloss 0.6931471805599453
iteration 6 batch 3070 trainingloss 0.6931471805599453
iteration 6 batch 3080 trainingloss 0.6931471805599453
iteration 6 batch 3090 trainingloss 0.6931471805599453
iteration 6 batch 3100 trainingloss 0.6931471805599453
iteration 6 batch 3110 trainingloss 0.6931471805599453
iteration 6 batch 3120 trainingloss 0.6931471805599453
iteration 6 batch 3130 trainingloss 0.6931471805599453
iteration 6 batch 3140 trainingloss 0.6931471805599453
iteration 6 batch 3150 trainingloss 0.6931471805599453
iteration 6 batch 3160 trainingloss 0.6931471805599453
iteration 6 batch 3170 trainingloss 0.6931471805599453
iteration 6 batch 3180 trainingloss 0.6931471805599453
iteration 6 batch 3190 trainingloss 0.6931471805599453
iteration 6 batch 3200 trainingloss 0.6931471805599453
iteration 6 batch 3210 trainingloss 0.6931471805599453
iteration 6 batch 3220 trainingloss 0.6931471805599453
iteration 6 batch 3230 trainingloss 0.6931471805599453
iteration 6 batch 3240 trainingloss 0.6916632528527511
iteration 6 batch 3250 trainingloss 0.6931471805599453
iteration 6 batch 3260 trainingloss 0.6931471805599453
iteration 6 batch 3270 trainingloss 0.6931471805599453
iteration 6 batch 3280 trainingloss 0.6931471805599453
iteration 6 batch 3290 trainingloss 0.6931471805599453
iteration 6 batch 3300 trainingloss 0.6931471805599453
iteration 6 batch 3310 trainingloss 0.6931471805599453
iteration 6 batch 3320 trainingloss 0.6931471805599453
iteration 6 batch 3330 trainingloss 0.6931471805599453
iteration 6 batch 3340 trainingloss 0.6931471805599453
iteration 6 batch 3350 trainingloss 0.6931471805599453
iteration 6 batch 3360 trainingloss 0.6916632528527511
iteration 6 batch 3370 trainingloss 0.6931471805599453
iteration 6 batch 3380 trainingloss 0.6931471805599453
iteration 6 batch 3390 trainingloss 0.6931471805599453
iteration 6 batch 3400 trainingloss 0.6931471805599453
iteration 6 batch 3410 trainingloss 0.6931471805599453
iteration 6 batch 3420 trainingloss 0.6931471805599453
iteration 6 batch 3430 trainingloss 0.6931471805599453
iteration 6 batch 3440 trainingloss 0.6931471805599453
iteration 6 batch 3450 trainingloss 0.6931471805599453
iteration 6 batch 3460 trainingloss 0.6931471805599453
iteration 6 batch 3470 trainingloss 0.6931471805599453
iteration 6 batch 3480 trainingloss 0.6931471805599453
iteration 6 batch 3490 trainingloss 0.6931471805599453
iteration 6 batch 3500 trainingloss 0.6931471805599453
iteration 6 batch 3510 trainingloss 0.6931471805599453
iteration 6 batch 3520 trainingloss 0.6931471805599453
iteration 6 batch 3530 trainingloss 0.6931471805599453
iteration 6 batch 3540 trainingloss 0.6931471805599453
iteration 6 batch 3550 trainingloss 0.6931471805599453
iteration 6 batch 3560 trainingloss 0.6931471805599453
iteration 6 batch 3570 trainingloss 0.6931471805599453
iteration 6 batch 3580 trainingloss 0.6931471805599453
iteration 6 batch 3590 trainingloss 0.6916632528527511
iteration 6 batch 3600 trainingloss 0.6931471805599453
iteration 6 batch 3610 trainingloss 0.6931471805599453
iteration 6 batch 3620 trainingloss 0.6931471805599453
iteration 6 batch 3630 trainingloss 0.6916632528527511
iteration 6 batch 3640 trainingloss 0.6931471805599453
iteration 6 batch 3650 trainingloss 0.6931471805599453
iteration 6 batch 3660 trainingloss 0.6931471805599453
iteration 6 batch 3670 trainingloss 0.6931471805599453
iteration 6 batch 3680 trainingloss 0.6931471805599453
iteration 6 batch 3690 trainingloss 0.6931471805599453
iteration 6 batch 3700 trainingloss 0.6931471805599453
iteration 6 batch 3710 trainingloss 0.6916632528527511
iteration 6 batch 3720 trainingloss 0.6931471805599453
iteration 6 batch 3730 trainingloss 0.6931471805599453
iteration 6 batch 3740 trainingloss 0.6931471805599453
iteration 6 batch 3750 trainingloss 0.6931471805599453
iteration 6 batch 3760 trainingloss 0.6916632528527511
iteration 6 batch 3770 trainingloss 0.6931471805599453
iteration 6 batch 3780 trainingloss 0.6931471805599453
iteration 6 batch 3790 trainingloss 0.6916632528527511
iteration 6 batch 3800 trainingloss 0.6931471805599453
iteration 6 batch 3810 trainingloss 0.6931471805599453
iteration 6 batch 3820 trainingloss 0.6931471805599453
iteration 6 batch 3830 trainingloss 0.6931471805599453
iteration 6 batch 3840 trainingloss 0.6931471805599453
iteration 6 batch 3850 trainingloss 0.6931471805599453
iteration 6 batch 3860 trainingloss 0.6931471805599453
iteration 6 batch 3870 trainingloss 0.6931471805599453
iteration 6 batch 3880 trainingloss 0.6931471805599453
iteration 6 batch 3890 trainingloss 0.6931471805599453
iteration 6 batch 3900 trainingloss 0.6931471805599453
iteration 6 batch 3910 trainingloss 0.6931471805599453
iteration 6 batch 3920 trainingloss 0.6931471805599453
iteration 6 batch 3930 trainingloss 0.6931471805599453
iteration 6 batch 3940 trainingloss 0.6931471805599453
iteration 6 batch 3950 trainingloss 0.6931471805599453
iteration 6 batch 3960 trainingloss 0.6931471805599453
iteration 6 batch 3970 trainingloss 0.6931471805599453
iteration 6 batch 3980 trainingloss 0.6931471805599453
iteration 6 batch 3990 trainingloss 0.6931471805599453
iteration 6 batch 4000 trainingloss 0.6931471805599453
iteration 6 batch 4010 trainingloss 0.6931471805599453
iteration 6 batch 4020 trainingloss 0.6931471805599453
iteration 6 batch 4030 trainingloss 0.6931471805599453
iteration 6 batch 4040 trainingloss 0.6931471805599453
iteration 6 batch 4050 trainingloss 0.6931471805599453
iteration 6 batch 4060 trainingloss 0.6931471805599453
iteration 6 batch 4070 trainingloss 0.6931471805599453
iteration 6 batch 4080 trainingloss 0.6931471805599453
iteration 6 batch 4090 trainingloss 0.6931471805599453
iteration 6 batch 4100 trainingloss 0.6931471805599453
iteration 6 batch 4110 trainingloss 0.6931471805599453
iteration 6 batch 4120 trainingloss 0.6931471805599453
iteration 6 batch 4130 trainingloss 0.6931471805599453
iteration 6 batch 4140 trainingloss 0.6931471805599453
iteration 6 batch 4150 trainingloss 0.6931471805599453
iteration 6 batch 4160 trainingloss 0.6916632528527511
iteration 6 batch 4170 trainingloss 0.6931471805599453
iteration 6 batch 4180 trainingloss 0.6931471805599453
iteration 6 batch 4190 trainingloss 0.6931471805599453
iteration 6 batch 4200 trainingloss 0.6931471805599453
iteration 6 batch 4210 trainingloss 0.6931471805599453
iteration 6 batch 4220 trainingloss 0.6931471805599453
iteration 6 batch 4230 trainingloss 0.6931471805599453
iteration 6 batch 4240 trainingloss 0.6931471805599453
iteration 6 batch 4250 trainingloss 0.6931471805599453
iteration 6 batch 4260 trainingloss 0.6931471805599453
iteration 6 batch 4270 trainingloss 0.6931471805599453
iteration 6 batch 4280 trainingloss 0.6931471805599453
iteration 6 batch 4290 trainingloss 0.6916632528527511
iteration 6 batch 4300 trainingloss 0.6931471805599453
iteration 6 batch 4310 trainingloss 0.6931471805599453
iteration 6 batch 4320 trainingloss 0.6931471805599453
iteration 6 batch 4330 trainingloss 0.6931471805599453
iteration 6 batch 4340 trainingloss 0.6931471805599453
iteration 6 batch 4350 trainingloss 0.6931471805599453
iteration 6 batch 4360 trainingloss 0.6931471805599453
iteration 6 batch 4370 trainingloss 0.6931471805599453
iteration 6 batch 4380 trainingloss 0.6931471805599453
iteration 6 batch 4390 trainingloss 0.6931471805599453
iteration 6 batch 4400 trainingloss 0.6931471805599453
iteration 6 batch 4410 trainingloss 0.6931471805599453
iteration 6 batch 4420 trainingloss 0.6931471805599453
iteration 6 batch 4430 trainingloss 0.6931471805599453
iteration 6 batch 4440 trainingloss 0.6931471805599453
iteration 6 batch 4450 trainingloss 0.6931471805599453
iteration 6 batch 4460 trainingloss 0.6931471805599453
iteration 6 batch 4470 trainingloss 0.6931471805599453
iteration 6 batch 4480 trainingloss 0.6931471805599453
iteration 6 batch 4490 trainingloss 0.6931471805599453
iteration 6 batch 4500 trainingloss 0.6931471805599453
iteration 6 batch 4510 trainingloss 0.6931471805599453
iteration 6 batch 4520 trainingloss 0.6931471805599453
iteration 6 batch 4530 trainingloss 0.6931471805599453
iteration 6 batch 4540 trainingloss 0.6931471805599453
iteration 6 batch 4550 trainingloss 0.6916632528527511
iteration 6 batch 4560 trainingloss 0.6931471805599453
iteration 6 batch 4570 trainingloss 0.6931471805599453
iteration 6 batch 4580 trainingloss 0.6931471805599453
iteration 6 batch 4590 trainingloss 0.6931471805599453
iteration 6 batch 4600 trainingloss 0.6931471805599453
iteration 6 batch 4610 trainingloss 0.6916632528527511
iteration 6 batch 4620 trainingloss 0.6931471805599453
iteration 6 batch 4630 trainingloss 0.6931471805599453
iteration 6 batch 4640 trainingloss 0.6931471805599453
iteration 6 batch 4650 trainingloss 0.6931471805599453
iteration 6 batch 4660 trainingloss 0.6931471805599453
iteration 6 batch 4670 trainingloss 0.6931471805599453
iteration 6 batch 4680 trainingloss 0.6931471805599453
iteration 6 batch 4690 trainingloss 0.6931471805599453
iteration 6 batch 4700 trainingloss 0.6931471805599453
iteration 6 batch 4710 trainingloss 0.6931471805599453
iteration 6 batch 4720 trainingloss 0.6931471805599453
iteration 6 batch 4730 trainingloss 0.6931471805599453
iteration 6 batch 4740 trainingloss 0.6931471805599453
iteration 6 batch 4750 trainingloss 0.6931471805599453
iteration 6 batch 4760 trainingloss 0.6931471805599453
iteration 6 batch 4770 trainingloss 0.6931471805599453
iteration 6 batch 4780 trainingloss 0.6931471805599453
iteration 6 batch 4790 trainingloss 0.6916632528527511
iteration 6 batch 4800 trainingloss 0.6931471805599453
iteration 6 batch 4810 trainingloss 0.6931471805599453
iteration 6 batch 4820 trainingloss 0.6931471805599453
iteration 6 batch 4830 trainingloss 0.6931471805599453
iteration 6 batch 4840 trainingloss 0.6931471805599453
iteration 6 batch 4850 trainingloss 0.6931471805599453
iteration 6 batch 4860 trainingloss 0.6931471805599453
iteration 6 batch 4870 trainingloss 0.6931471805599453
iteration 6 batch 4880 trainingloss 0.6931471805599453
iteration 6 batch 4890 trainingloss 0.6931471805599453
iteration 6 batch 4900 trainingloss 0.6931471805599453
iteration 6 batch 4910 trainingloss 0.6931471805599453
iteration 6 batch 4920 trainingloss 0.6931471805599453
iteration 6 batch 4930 trainingloss 0.6931471805599453
iteration 6 batch 4940 trainingloss 0.6931471805599453
iteration 6 batch 4950 trainingloss 0.6931471805599453
iteration 6 batch 4960 trainingloss 0.6931471805599453
iteration 6 batch 4970 trainingloss 0.6931471805599453
iteration 6 batch 4980 trainingloss 0.6931471805599453
iteration 6 batch 4990 trainingloss 0.6931471805599453
iteration 6 batch 5000 trainingloss 0.6931471805599453
iteration 6 batch 5010 trainingloss 0.6931471805599453
iteration 6 batch 5020 trainingloss 0.6931471805599453
iteration 6 batch 5030 trainingloss 0.6931471805599453
iteration 6 batch 5040 trainingloss 0.6931471805599453
iteration 6 batch 5050 trainingloss 0.6916632528527511
iteration 6 batch 5060 trainingloss 0.6931471805599453
iteration 6 batch 5070 trainingloss 0.6931471805599453
iteration 6 batch 5080 trainingloss 0.6931471805599453
iteration 6 batch 5090 trainingloss 0.6916632528527511
iteration 6 batch 5100 trainingloss 0.6931471805599453
iteration 6 batch 5110 trainingloss 0.6931471805599453
iteration 6 batch 5120 trainingloss 0.6931471805599453
iteration 6 batch 5130 trainingloss 0.6931471805599453
iteration 6 batch 5140 trainingloss 0.6931471805599453
iteration 6 batch 5150 trainingloss 0.6931471805599453
iteration 6 batch 5160 trainingloss 0.6931471805599453
iteration 6 batch 5170 trainingloss 0.6931471805599453
iteration 6 batch 5180 trainingloss 0.6931471805599453
iteration 6 batch 5190 trainingloss 0.6931471805599453
iteration 6 batch 5200 trainingloss 0.6931471805599453
iteration 6 batch 5210 trainingloss 0.6916632528527511
iteration 6 batch 5220 trainingloss 0.6931471805599453
iteration 6 batch 5230 trainingloss 0.6931471805599453
iteration 6 batch 5240 trainingloss 0.6931471805599453
iteration 6 batch 5250 trainingloss 0.6931471805599453
iteration 6 batch 5260 trainingloss 0.6916632528527511
iteration 6 batch 5270 trainingloss 0.6931471805599453
iteration 6 batch 5280 trainingloss 0.6931471805599453
iteration 6 batch 5290 trainingloss 0.6931471805599453
iteration 6 batch 5300 trainingloss 0.6931471805599453
iteration 6 batch 5310 trainingloss 0.6931471805599453
iteration 6 batch 5320 trainingloss 0.6931471805599453
iteration 6 batch 5330 trainingloss 0.6931471805599453
iteration 6 batch 5340 trainingloss 0.6931471805599453
iteration 6 batch 5350 trainingloss 0.6931471805599453
iteration 6 batch 5360 trainingloss 0.6931471805599453
iteration 6 batch 5370 trainingloss 0.6931471805599453
iteration 6 batch 5380 trainingloss 0.6931471805599453
iteration 6 batch 5390 trainingloss 0.6931471805599453
iteration 6 batch 5400 trainingloss 0.6931471805599453
iteration 6 batch 5410 trainingloss 0.6931471805599453
iteration 6 batch 5420 trainingloss 0.6931471805599453
iteration 6 batch 5430 trainingloss 0.6931471805599453
iteration 6 batch 5440 trainingloss 0.6931471805599453
iteration 6 batch 5450 trainingloss 0.6931471805599453
iteration 6 batch 5460 trainingloss 0.6931471805599453
iteration 6 batch 5470 trainingloss 0.6931471805599453
iteration 6 batch 5480 trainingloss 0.6931471805599453
iteration 6 batch 5490 trainingloss 0.6916632528527511
iteration 6 batch 5500 trainingloss 0.6931471805599453
iteration 6 batch 5510 trainingloss 0.6931471805599453
iteration 6 batch 5520 trainingloss 0.6931471805599453
iteration 6 batch 5530 trainingloss 0.6931471805599453
iteration 6 batch 5540 trainingloss 0.6931471805599453
iteration 6 batch 5550 trainingloss 0.6931471805599453
iteration 6 batch 5560 trainingloss 0.6916632528527511
iteration 6 batch 5570 trainingloss 0.6931471805599453
iteration 6 batch 5580 trainingloss 0.6916632528527511
iteration 6 batch 5590 trainingloss 0.6931471805599453
iteration 6 batch 5600 trainingloss 0.6931471805599453
iteration 6 batch 5610 trainingloss 0.6931471805599453
iteration 6 batch 5620 trainingloss 0.6931471805599453
iteration 6 batch 5630 trainingloss 0.6931471805599453
iteration 6 batch 5640 trainingloss 0.6931471805599453
iteration 6 batch 5650 trainingloss 0.6931471805599453
iteration 6 batch 5660 trainingloss 0.6931471805599453
iteration 6 batch 5670 trainingloss 0.6931471805599453
iteration 6 batch 5680 trainingloss 0.6931471805599453
iteration 6 batch 5690 trainingloss 0.6931471805599453
iteration 6 batch 5700 trainingloss 0.6931471805599453
iteration 6 batch 5710 trainingloss 0.6931471805599453
iteration 6 batch 5720 trainingloss 0.6931471805599453
iteration 6 batch 5730 trainingloss 0.6931471805599453
iteration 6 batch 5740 trainingloss 0.6931471805599453
iteration 6 batch 5750 trainingloss 0.6931471805599453
iteration 6 batch 5760 trainingloss 0.6931471805599453
iteration 6 batch 5770 trainingloss 0.6931471805599453
iteration 6 batch 5780 trainingloss 0.6916632528527511
iteration 6 batch 5790 trainingloss 0.6931471805599453
iteration 6 batch 5800 trainingloss 0.6931471805599453
iteration 6 batch 5810 trainingloss 0.6916632528527511
iteration 6 batch 5820 trainingloss 0.6931471805599453
iteration 6 batch 5830 trainingloss 0.6931471805599453
iteration 6 batch 5840 trainingloss 0.6931471805599453
iteration 6 batch 5850 trainingloss 0.6931471805599453
iteration 6 batch 5860 trainingloss 0.6931471805599453
iteration 6 batch 5870 trainingloss 0.6931471805599453
iteration 6 batch 5880 trainingloss 0.6931471805599453
iteration 6 batch 5890 trainingloss 0.6931471805599453
iteration 6 batch 5900 trainingloss 0.6931471805599453
iteration 6 batch 5910 trainingloss 0.6931471805599453
iteration 6 batch 5920 trainingloss 0.6931471805599453
iteration 6 batch 5930 trainingloss 0.6931471805599453
iteration 6 batch 5940 trainingloss 0.6931471805599453
iteration 6 batch 5950 trainingloss 0.6931471805599453
iteration 6 batch 5960 trainingloss 0.6931471805599453
iteration 6 batch 5970 trainingloss 0.6931471805599453
iteration 6 batch 5980 trainingloss 0.6931471805599453
iteration 6 batch 5990 trainingloss 0.6931471805599453
iteration 6 batch 6000 trainingloss 0.6931471805599453
iteration 6 batch 6010 trainingloss 0.6931471805599453
iteration 6 batch 6020 trainingloss 0.6931471805599453
iteration 6 batch 6030 trainingloss 0.6931471805599453
iteration 6 batch 6040 trainingloss 0.6931471805599453
iteration 6 batch 6050 trainingloss 0.6931471805599453
iteration 6 batch 6060 trainingloss 0.6931471805599453
iteration 6 batch 6070 trainingloss 0.6931471805599453
iteration 6 batch 6080 trainingloss 0.6931471805599453
iteration 6 batch 6090 trainingloss 0.6931471805599453
iteration 6 batch 6100 trainingloss 0.6931471805599453
iteration 6 batch 6110 trainingloss 0.6931471805599453
iteration 6 batch 6120 trainingloss 0.6931471805599453
iteration 6 batch 6130 trainingloss 0.6931471805599453
iteration 6 batch 6140 trainingloss 0.6931471805599453
iteration 6 batch 6150 trainingloss 0.6931471805599453
iteration 6 batch 6160 trainingloss 0.6931471805599453
iteration 6 batch 6170 trainingloss 0.6916632528527511
iteration 6 batch 6180 trainingloss 0.6916632528527511
iteration 6 batch 6190 trainingloss 0.6931471805599453
iteration 6 batch 6200 trainingloss 0.6931471805599453
iteration 6 batch 6210 trainingloss 0.6931471805599453
iteration 6 batch 6220 trainingloss 0.6931471805599453
iteration 6 batch 6230 trainingloss 0.6916632528527511
iteration 6 batch 6240 trainingloss 0.6916632528527511
iteration 6 batch 6250 trainingloss 0.6931471805599453
iteration 6 batch 6260 trainingloss 0.6931471805599453
iteration 6 batch 6270 trainingloss 0.6931471805599453
iteration 6 batch 6280 trainingloss 0.6931471805599453
iteration 6 batch 6290 trainingloss 0.6901793251455568
iteration 6 batch 6300 trainingloss 0.6931471805599453
iteration 6 batch 6310 trainingloss 0.6931471805599453
iteration 6 batch 6320 trainingloss 0.6931471805599453
iteration 6 batch 6330 trainingloss 0.6931471805599453
iteration 6 batch 6340 trainingloss 0.6931471805599453
iteration 6 batch 6350 trainingloss 0.6931471805599453
iteration 6 batch 6360 trainingloss 0.6931471805599453
iteration 6 batch 6370 trainingloss 0.6916632528527511
iteration 6 batch 6380 trainingloss 0.6931471805599453
iteration 6 batch 6390 trainingloss 0.6931471805599453
iteration 6 batch 6400 trainingloss 0.6931471805599453
iteration 6 batch 6410 trainingloss 0.6916632528527511
iteration 6 batch 6420 trainingloss 0.6931471805599453
iteration 6 batch 6430 trainingloss 0.6931471805599453
iteration 6 batch 6440 trainingloss 0.6931471805599453
iteration 6 batch 6450 trainingloss 0.6931471805599453
iteration 6 batch 6460 trainingloss 0.6931471805599453
iteration 6 batch 6470 trainingloss 0.6931471805599453
iteration 6 batch 6480 trainingloss 0.6931471805599453
iteration 6 batch 6490 trainingloss 0.6931471805599453
iteration 6 batch 6500 trainingloss 0.6916632528527511
iteration 6 batch 6510 trainingloss 0.6931471805599453
iteration 6 batch 6520 trainingloss 0.6931471805599453
iteration 6 batch 6530 trainingloss 0.6931471805599453
iteration 6 batch 6540 trainingloss 0.6931471805599453
iteration 6 batch 6550 trainingloss 0.6931471805599453
iteration 6 batch 6560 trainingloss 0.6931471805599453
iteration 6 batch 6570 trainingloss 0.6931471805599453
iteration 6 batch 6580 trainingloss 0.6931471805599453
iteration 6 batch 6590 trainingloss 0.6931471805599453
iteration 6 batch 6600 trainingloss 0.6931471805599453
iteration 6 batch 6610 trainingloss 0.6931471805599453
iteration 6 batch 6620 trainingloss 0.6931471805599453
iteration 6 batch 6630 trainingloss 0.6931471805599453
iteration 6 batch 6640 trainingloss 0.6931471805599453
iteration 6 batch 6650 trainingloss 0.6931471805599453
iteration 6 batch 6660 trainingloss 0.6931471805599453
iteration 6 batch 6670 trainingloss 0.6931471805599453
iteration 6 batch 6680 trainingloss 0.6931471805599453
iteration 6 batch 6690 trainingloss 0.6931471805599453
iteration 6 batch 6700 trainingloss 0.6931471805599453
iteration 6 batch 6710 trainingloss 0.6931471805599453
iteration 6 batch 6720 trainingloss 0.6931471805599453
iteration 6 batch 6730 trainingloss 0.6931471805599453
iteration 6 batch 6740 trainingloss 0.6931471805599453
iteration 6 batch 6750 trainingloss 0.6931471805599453
iteration 6 batch 6760 trainingloss 0.6931471805599453
iteration 6 batch 6770 trainingloss 0.6931471805599453
iteration 6 batch 6780 trainingloss 0.6931471805599453
iteration 6 batch 6790 trainingloss 0.6931471805599453
iteration 6 batch 6800 trainingloss 0.6931471805599453
iteration 6 batch 6810 trainingloss 0.6931471805599453
iteration 6 batch 6820 trainingloss 0.6931471805599453
iteration 6 batch 6830 trainingloss 0.6931471805599453
iteration 6 batch 6840 trainingloss 0.6931471805599453
iteration 6 batch 6850 trainingloss 0.6931471805599453
iteration 6 batch 6860 trainingloss 0.6931471805599453
iteration 6 batch 6870 trainingloss 0.6931471805599453
iteration 6 batch 6880 trainingloss 0.6931471805599453
iteration 6 batch 6890 trainingloss 0.6931471805599453
iteration 6 batch 6900 trainingloss 0.6931471805599453
iteration 6 batch 6910 trainingloss 0.6931471805599453
iteration 6 batch 6920 trainingloss 0.6931471805599453
iteration 6 batch 6930 trainingloss 0.6931471805599453
iteration 6 batch 6940 trainingloss 0.6931471805599453
iteration 6 batch 6950 trainingloss 0.6931471805599453
iteration 6 batch 6960 trainingloss 0.6931471805599453
iteration 6 batch 6970 trainingloss 0.6931471805599453
iteration 6 batch 6980 trainingloss 0.6931471805599453
iteration 6 batch 6990 trainingloss 0.6931471805599453
iteration 6 batch 7000 trainingloss 0.6931471805599453
iteration 6 batch 7010 trainingloss 0.6931471805599453
iteration 6 batch 7020 trainingloss 0.6931471805599453
iteration 6 batch 7030 trainingloss 0.6931471805599453
iteration 6 batch 7040 trainingloss 0.6931471805599453
iteration 6 batch 7050 trainingloss 0.6931471805599453
iteration 6 batch 7060 trainingloss 0.6931471805599453
iteration 6 batch 7070 trainingloss 0.6931471805599453
iteration 6 batch 7080 trainingloss 0.6931471805599453
iteration 6 batch 7090 trainingloss 0.6931471805599453
iteration 6 batch 7100 trainingloss 0.6931471805599453
iteration 6 batch 7110 trainingloss 0.6931471805599453
iteration 6 batch 7120 trainingloss 0.6931471805599453
iteration 6 batch 7130 trainingloss 0.6931471805599453
iteration 6 batch 7140 trainingloss 0.6931471805599453
iteration 6 batch 7150 trainingloss 0.6931471805599453
iteration 6 batch 7160 trainingloss 0.6931471805599453
iteration 6 batch 7170 trainingloss 0.6931471805599453
iteration 6 batch 7180 trainingloss 0.6931471805599453
iteration 6 batch 7190 trainingloss 0.6931471805599453
iteration 6 batch 7200 trainingloss 0.6931471805599453
iteration 6 batch 7210 trainingloss 0.6931471805599453
iteration 6 batch 7220 trainingloss 0.6931471805599453
iteration 6 batch 7230 trainingloss 0.6931471805599453
iteration 6 batch 7240 trainingloss 0.6931471805599453
iteration 6 batch 7250 trainingloss 0.6931471805599453
iteration 6 batch 7260 trainingloss 0.6931471805599453
iteration 6 batch 7270 trainingloss 0.6931471805599453
iteration 6 batch 7280 trainingloss 0.6931471805599453
iteration 6 batch 7290 trainingloss 0.6931471805599453
iteration 6 batch 7300 trainingloss 0.6931471805599453
iteration 6 batch 7310 trainingloss 0.6931471805599453
iteration 6 batch 7320 trainingloss 0.6931471805599453
iteration 6 batch 7330 trainingloss 0.6931471805599453
iteration 6 batch 7340 trainingloss 0.6931471805599453
iteration 6 batch 7350 trainingloss 0.6931471805599453
iteration 6 batch 7360 trainingloss 0.6931471805599453
iteration 6 batch 7370 trainingloss 0.6901793251455568
iteration 6 batch 7380 trainingloss 0.6931471805599453
iteration 6 batch 7390 trainingloss 0.6931471805599453
iteration 6 batch 7400 trainingloss 0.6931471805599453
iteration 6 batch 7410 trainingloss 0.6931471805599453
iteration 6 batch 7420 trainingloss 0.6931471805599453
iteration 6 batch 7430 trainingloss 0.6931471805599453
iteration 6 batch 7440 trainingloss 0.6931471805599453
iteration 6 batch 7450 trainingloss 0.6931471805599453
iteration 6 batch 7460 trainingloss 0.6916632528527511
iteration 6 batch 7470 trainingloss 0.6931471805599453
iteration 6 batch 7480 trainingloss 0.6931471805599453
iteration 6 batch 7490 trainingloss 0.6931471805599453
iteration 6 batch 7500 trainingloss 0.6931471805599453
iteration 6 batch 7510 trainingloss 0.6931471805599453
iteration 6 batch 7520 trainingloss 0.6931471805599453
iteration 6 batch 7530 trainingloss 0.6931471805599453
iteration 6 batch 7540 trainingloss 0.6931471805599453
iteration 6 batch 7550 trainingloss 0.6931471805599453
iteration 6 batch 7560 trainingloss 0.6931471805599453
iteration 6 batch 7570 trainingloss 0.6931471805599453
iteration 6 batch 7580 trainingloss 0.6931471805599453
iteration 6 batch 7590 trainingloss 0.6931471805599453
iteration 6 batch 7600 trainingloss 0.6931471805599453
iteration 6 batch 7610 trainingloss 0.6931471805599453
iteration 6 batch 7620 trainingloss 0.6931471805599453
iteration 6 batch 7630 trainingloss 0.6931471805599453
iteration 6 batch 7640 trainingloss 0.6931471805599453
iteration 6 batch 7650 trainingloss 0.6931471805599453
iteration 6 batch 7660 trainingloss 0.6931471805599453
iteration 6 batch 7670 trainingloss 0.6931471805599453
iteration 6 batch 7680 trainingloss 0.6931471805599453
iteration 6 batch 7690 trainingloss 0.6916632528527511
iteration 6 batch 7700 trainingloss 0.6931471805599453
iteration 6 batch 7710 trainingloss 0.6931471805599453
iteration 6 batch 7720 trainingloss 0.6916632528527511
iteration 6 batch 7730 trainingloss 0.6931471805599453
iteration 6 batch 7740 trainingloss 0.6931471805599453
iteration 6 batch 7750 trainingloss 0.6931471805599453
iteration 6 batch 7760 trainingloss 0.6916632528527511
iteration 6 batch 7770 trainingloss 0.6931471805599453
iteration 6 batch 7780 trainingloss 0.6931471805599453
iteration 6 batch 7790 trainingloss 0.6931471805599453
iteration 6 batch 7800 trainingloss 0.6931471805599453
iteration 6 batch 7810 trainingloss 0.6931471805599453
iteration 6 batch 7820 trainingloss 0.6931471805599453
iteration 6 batch 7830 trainingloss 0.6931471805599453
iteration 6 batch 7840 trainingloss 0.6931471805599453
iteration 6 batch 7850 trainingloss 0.6931471805599453
iteration 6 batch 7860 trainingloss 0.6931471805599453
iteration 6 batch 7870 trainingloss 0.6931471805599453
iteration 6 batch 7880 trainingloss 0.6931471805599453
iteration 6 batch 7890 trainingloss 0.6931471805599453
iteration 6 batch 7900 trainingloss 0.6931471805599453
iteration 6 batch 7910 trainingloss 0.6931471805599453
iteration 6 batch 7920 trainingloss 0.6931471805599453
iteration 6 batch 7930 trainingloss 0.6931471805599453
iteration 6 batch 7940 trainingloss 0.6931471805599453
iteration 6 batch 7950 trainingloss 0.6931471805599453
iteration 6 batch 7960 trainingloss 0.6931471805599453
iteration 6 batch 7970 trainingloss 0.6931471805599453
iteration 6 batch 7980 trainingloss 0.6931471805599453
iteration 6 batch 7990 trainingloss 0.6931471805599453
iteration 6 batch 8000 trainingloss 0.6931471805599453
iteration 6 batch 8010 trainingloss 0.6931471805599453
iteration 6 batch 8020 trainingloss 0.6931471805599453
iteration 6 batch 8030 trainingloss 0.6931471805599453
iteration 6 batch 8040 trainingloss 0.6931471805599453
iteration 6 batch 8050 trainingloss 0.6931471805599453
iteration 6 batch 8060 trainingloss 0.6931471805599453
iteration 6 batch 8070 trainingloss 0.6931471805599453
iteration 6 batch 8080 trainingloss 0.6916632528527511
iteration 6 batch 8090 trainingloss 0.6931471805599453
iteration 6 batch 8100 trainingloss 0.6931471805599453
iteration 6 batch 8110 trainingloss 0.6931471805599453
iteration 6 batch 8120 trainingloss 0.6931471805599453
iteration 6 batch 8130 trainingloss 0.6931471805599453
iteration 6 batch 8140 trainingloss 0.6931471805599453
iteration 6 batch 8150 trainingloss 0.6931471805599453
iteration 6 batch 8160 trainingloss 0.6931471805599453
iteration 6 batch 8170 trainingloss 0.6931471805599453
iteration 6 batch 8180 trainingloss 0.6931471805599453
iteration 6 batch 8190 trainingloss 0.6931471805599453
iteration 6 batch 8200 trainingloss 0.6931471805599453
iteration 6 batch 8210 trainingloss 0.6931471805599453
iteration 6 batch 8220 trainingloss 0.6931471805599453
iteration 6 batch 8230 trainingloss 0.6931471805599453
iteration 6 batch 8240 trainingloss 0.6931471805599453
iteration 6 batch 8250 trainingloss 0.6931471805599453
iteration 6 batch 8260 trainingloss 0.6931471805599453
iteration 6 batch 8270 trainingloss 0.6931471805599453
iteration 6 batch 8280 trainingloss 0.6931471805599453
iteration 6 batch 8290 trainingloss 0.6916632528527511
iteration 6 batch 8300 trainingloss 0.6931471805599453
iteration 6 batch 8310 trainingloss 0.6931471805599453
iteration 6 batch 8320 trainingloss 0.6931471805599453
iteration 6 batch 8330 trainingloss 0.6931471805599453
iteration 6 batch 8340 trainingloss 0.6931471805599453
iteration 6 batch 8350 trainingloss 0.6931471805599453
iteration 6 batch 8360 trainingloss 0.6931471805599453
iteration 6 batch 8370 trainingloss 0.6931471805599453
iteration 6 batch 8380 trainingloss 0.6931471805599453
iteration 6 batch 8390 trainingloss 0.6931471805599453
iteration 6 batch 8400 trainingloss 0.6931471805599453
iteration 6 batch 8410 trainingloss 0.6931471805599453
iteration 6 batch 8420 trainingloss 0.6931471805599453
iteration 6 batch 8430 trainingloss 0.6931471805599453
iteration 6 batch 8440 trainingloss 0.6931471805599453
iteration 6 batch 8450 trainingloss 0.6931471805599453
iteration 6 batch 8460 trainingloss 0.6931471805599453
iteration 6 batch 8470 trainingloss 0.6931471805599453
iteration 6 batch 8480 trainingloss 0.6931471805599453
iteration 6 batch 8490 trainingloss 0.6931471805599453
iteration 6 batch 8500 trainingloss 0.6931471805599453
iteration 6 batch 8510 trainingloss 0.6931471805599453
iteration 6 batch 8520 trainingloss 0.6916632528527511
iteration 6 batch 8530 trainingloss 0.6931471805599453
iteration 6 batch 8540 trainingloss 0.6931471805599453
iteration 6 batch 8550 trainingloss 0.6931471805599453
iteration 6 batch 8560 trainingloss 0.6931471805599453
iteration 6 batch 8570 trainingloss 0.6931471805599453
iteration 6 batch 8580 trainingloss 0.6931471805599453
iteration 6 batch 8590 trainingloss 0.6931471805599453
iteration 6 batch 8600 trainingloss 0.6931471805599453
iteration 6 batch 8610 trainingloss 0.6931471805599453
iteration 6 batch 8620 trainingloss 0.6931471805599453
iteration 6 batch 8630 trainingloss 0.6931471805599453
iteration 6 batch 8640 trainingloss 0.6931471805599453
iteration 6 batch 8650 trainingloss 0.6931471805599453
iteration 6 batch 8660 trainingloss 0.6931471805599453
iteration 6 batch 8670 trainingloss 0.6931471805599453
iteration 6 batch 8680 trainingloss 0.6931471805599453
iteration 6 batch 8690 trainingloss 0.6931471805599453
iteration 6 batch 8700 trainingloss 0.6931471805599453
iteration 6 batch 8710 trainingloss 0.6931471805599453
iteration 6 batch 8720 trainingloss 0.6931471805599453
iteration 6 batch 8730 trainingloss 0.6931471805599453
iteration 6 batch 8740 trainingloss 0.6931471805599453
iteration 6 batch 8750 trainingloss 0.6931471805599453
iteration 6 batch 8760 trainingloss 0.6931471805599453
iteration 6 batch 8770 trainingloss 0.6931471805599453
iteration 6 batch 8780 trainingloss 0.6931471805599453
iteration 6 batch 8790 trainingloss 0.6931471805599453
iteration 6 batch 8800 trainingloss 0.6931471805599453
iteration 6 batch 8810 trainingloss 0.6931471805599453
iteration 6 batch 8820 trainingloss 0.6931471805599453
iteration 6 batch 8830 trainingloss 0.6931471805599453
iteration 6 batch 8840 trainingloss 0.6931471805599453
iteration 6 batch 8850 trainingloss 0.6931471805599453
iteration 6 batch 8860 trainingloss 0.6931471805599453
iteration 6 batch 8870 trainingloss 0.6931471805599453
iteration 6 batch 8880 trainingloss 0.6931471805599453
iteration 6 batch 8890 trainingloss 0.6931471805599453
iteration 6 batch 8900 trainingloss 0.6931471805599453
iteration 6 batch 8910 trainingloss 0.6931471805599453
iteration 6 batch 8920 trainingloss 0.6931471805599453
iteration 6 batch 8930 trainingloss 0.6931471805599453
iteration 6 batch 8940 trainingloss 0.6931471805599453
iteration 6 batch 8950 trainingloss 0.6931471805599453
iteration 6 batch 8960 trainingloss 0.6931471805599453
iteration 6 batch 8970 trainingloss 0.6931471805599453
iteration 6 batch 8980 trainingloss 0.6931471805599453
iteration 6 batch 8990 trainingloss 0.6931471805599453
iteration 6 batch 9000 trainingloss 0.6931471805599453
iteration 6 batch 9010 trainingloss 0.6931471805599453
iteration 6 batch 9020 trainingloss 0.6931471805599453
iteration 6 batch 9030 trainingloss 0.6931471805599453
iteration 6 batch 9040 trainingloss 0.6931471805599453
iteration 6 batch 9050 trainingloss 0.6931471805599453
iteration 6 batch 9060 trainingloss 0.6931471805599453
iteration 6 batch 9070 trainingloss 0.6931471805599453
iteration 6 batch 9080 trainingloss 0.6931471805599453
iteration 6 batch 9090 trainingloss 0.6931471805599453
iteration 6 batch 9100 trainingloss 0.6931471805599453
iteration 6 batch 9110 trainingloss 0.6931471805599453
iteration 6 batch 9120 trainingloss 0.6931471805599453
iteration 6 batch 9130 trainingloss 0.6931471805599453
iteration 6 batch 9140 trainingloss 0.6931471805599453
iteration 6 batch 9150 trainingloss 0.6931471805599453
iteration 6 batch 9160 trainingloss 0.6931471805599453
iteration 6 batch 9170 trainingloss 0.6931471805599453
iteration 6 batch 9180 trainingloss 0.6931471805599453
iteration 6 batch 9190 trainingloss 0.6916632528527511
iteration 6 batch 9200 trainingloss 0.6931471805599453
iteration 6 batch 9210 trainingloss 0.6931471805599453
iteration 6 batch 9220 trainingloss 0.6916632528527511
iteration 6 batch 9230 trainingloss 0.6916632528527511
iteration 6 batch 9240 trainingloss 0.6931471805599453
iteration 6 batch 9250 trainingloss 0.6931471805599453
iteration 6 batch 9260 trainingloss 0.6931471805599453
iteration 6 batch 9270 trainingloss 0.6931471805599453
iteration 6 batch 9280 trainingloss 0.6931471805599453
iteration 6 batch 9290 trainingloss 0.6931471805599453
iteration 6 batch 9300 trainingloss 0.6931471805599453
iteration 6 batch 9310 trainingloss 0.6931471805599453
iteration 6 batch 9320 trainingloss 0.6931471805599453
iteration 6 batch 9330 trainingloss 0.6931471805599453
iteration 6 batch 9340 trainingloss 0.6931471805599453
iteration 6 batch 9350 trainingloss 0.6931471805599453
iteration 6 batch 9360 trainingloss 0.6931471805599453
iteration 6 batch 9370 trainingloss 0.6916632528527511
iteration 6 batch 9380 trainingloss 0.6931471805599453
iteration 6 batch 9390 trainingloss 0.6931471805599453
iteration 6 batch 9400 trainingloss 0.6931471805599453
iteration 6 batch 9410 trainingloss 0.6931471805599453
iteration 6 batch 9420 trainingloss 0.6931471805599453
iteration 6 batch 9430 trainingloss 0.6931471805599453
iteration 6 batch 9440 trainingloss 0.6916632528527511
iteration 6 batch 9450 trainingloss 0.6931471805599453
iteration 6 batch 9460 trainingloss 0.6931471805599453
iteration 6 batch 9470 trainingloss 0.6931471805599453
iteration 6 batch 9480 trainingloss 0.6931471805599453
iteration 6 batch 9490 trainingloss 0.6931471805599453
iteration 6 batch 9500 trainingloss 0.6931471805599453
iteration 6 batch 9510 trainingloss 0.6931471805599453
iteration 6 batch 9520 trainingloss 0.6931471805599453
iteration 6 batch 9530 trainingloss 0.6931471805599453
iteration 6 batch 9540 trainingloss 0.6931471805599453
iteration 6 batch 9550 trainingloss 0.6916632528527511
iteration 6 batch 9560 trainingloss 0.6931471805599453
iteration 6 batch 9570 trainingloss 0.6916632528527511
iteration 6 batch 9580 trainingloss 0.6931471805599453
iteration 6 batch 9590 trainingloss 0.6931471805599453
iteration 6 batch 9600 trainingloss 0.6931471805599453
iteration 6 batch 9610 trainingloss 0.6931471805599453
iteration 6 batch 9620 trainingloss 0.6931471805599453
iteration 6 batch 9630 trainingloss 0.6931471805599453
iteration 6 batch 9640 trainingloss 0.6931471805599453
iteration 6 batch 9650 trainingloss 0.6931471805599453
iteration 6 batch 9660 trainingloss 0.6931471805599453
iteration 6 batch 9670 trainingloss 0.6931471805599453
iteration 6 batch 9680 trainingloss 0.6931471805599453
iteration 6 batch 9690 trainingloss 0.6916632528527511
iteration 6 batch 9700 trainingloss 0.6931471805599453
iteration 6 batch 9710 trainingloss 0.6931471805599453
iteration 6 batch 9720 trainingloss 0.6931471805599453
iteration 6 batch 9730 trainingloss 0.6931471805599453
iteration 6 batch 9740 trainingloss 0.6931471805599453
iteration 6 batch 9750 trainingloss 0.6931471805599453
iteration 6 batch 9760 trainingloss 0.6931471805599453
iteration 6 batch 9770 trainingloss 0.6931471805599453
iteration 6 batch 9780 trainingloss 0.6931471805599453
iteration 6 batch 9790 trainingloss 0.6931471805599453
iteration 6 batch 9800 trainingloss 0.6931471805599453
iteration 6 batch 9810 trainingloss 0.6931471805599453
iteration 6 batch 9820 trainingloss 0.6931471805599453
iteration 6 batch 9830 trainingloss 0.6931471805599453
iteration 6 batch 9840 trainingloss 0.6931471805599453
iteration 6 batch 9850 trainingloss 0.6931471805599453
iteration 6 batch 9860 trainingloss 0.6931471805599453
iteration 6 batch 9870 trainingloss 0.6931471805599453
iteration 6 batch 9880 trainingloss 0.6916632528527511
iteration 6 batch 9890 trainingloss 0.6931471805599453
iteration 6 batch 9900 trainingloss 0.6931471805599453
iteration 6 batch 9910 trainingloss 0.6931471805599453
iteration 6 batch 9920 trainingloss 0.6931471805599453
iteration 6 batch 9930 trainingloss 0.6931471805599453
iteration 6 batch 9940 trainingloss 0.6931471805599453
iteration 6 batch 9950 trainingloss 0.6931471805599453
iteration 6 batch 9960 trainingloss 0.6931471805599453
iteration 6 batch 9970 trainingloss 0.6916632528527511
iteration 6 batch 9980 trainingloss 0.6931471805599453
iteration 6 batch 9990 trainingloss 0.6931471805599453
iteration 6 batch 10000 trainingloss 0.6931471805599453
iteration 6 batch 10010 trainingloss 0.6931471805599453
iteration 6 batch 10020 trainingloss 0.6931471805599453
iteration 6 batch 10030 trainingloss 0.6931471805599453
iteration 6 batch 10040 trainingloss 0.6931471805599453
iteration 6 batch 10050 trainingloss 0.6931471805599453
iteration 6 batch 10060 trainingloss 0.6931471805599453
iteration 6 batch 10070 trainingloss 0.6931471805599453
iteration 6 batch 10080 trainingloss 0.6931471805599453
iteration 6 batch 10090 trainingloss 0.6931471805599453
iteration 6 batch 10100 trainingloss 0.6931471805599453
iteration 6 batch 10110 trainingloss 0.6931471805599453
iteration 6 batch 10120 trainingloss 0.6931471805599453
iteration 6 batch 10130 trainingloss 0.6931471805599453
iteration 6 batch 10140 trainingloss 0.6931471805599453
iteration 6 batch 10150 trainingloss 0.6931471805599453
iteration 6 batch 10160 trainingloss 0.6931471805599453
iteration 6 batch 10170 trainingloss 0.6931471805599453
iteration 6 batch 10180 trainingloss 0.6931471805599453
iteration 6 batch 10190 trainingloss 0.6931471805599453
iteration 6 batch 10200 trainingloss 0.6931471805599453
iteration 6 batch 10210 trainingloss 0.6931471805599453
iteration 6 batch 10220 trainingloss 0.6916632528527511
iteration 6 batch 10230 trainingloss 0.6916632528527511
iteration 6 batch 10240 trainingloss 0.6931471805599453
iteration 6 batch 10250 trainingloss 0.6931471805599453
iteration 6 batch 10260 trainingloss 0.6931471805599453
iteration 6 batch 10270 trainingloss 0.6931471805599453
iteration 6 batch 10280 trainingloss 0.6931471805599453
iteration 6 batch 10290 trainingloss 0.6931471805599453
iteration 6 batch 10300 trainingloss 0.6931471805599453
iteration 6 batch 10310 trainingloss 0.6931471805599453
iteration 6 batch 10320 trainingloss 0.6931471805599453
iteration 6 batch 10330 trainingloss 0.6931471805599453
iteration 6 batch 10340 trainingloss 0.6931471805599453
iteration 6 batch 10350 trainingloss 0.6916632528527511
iteration 6 batch 10360 trainingloss 0.6931471805599453
iteration 6 batch 10370 trainingloss 0.6931471805599453
iteration 6 batch 10380 trainingloss 0.6931471805599453
iteration 6 batch 10390 trainingloss 0.6931471805599453
iteration 6 batch 10400 trainingloss 0.6931471805599453
iteration 6 batch 10410 trainingloss 0.6931471805599453
iteration 6 batch 10420 trainingloss 0.6931471805599453
iteration 6 batch 10430 trainingloss 0.6931471805599453
iteration 6 batch 10440 trainingloss 0.6931471805599453
iteration 6 batch 10450 trainingloss 0.6931471805599453
iteration 6 batch 10460 trainingloss 0.6931471805599453
iteration 6 batch 10470 trainingloss 0.6931471805599453
iteration 6 batch 10480 trainingloss 0.6931471805599453
iteration 6 batch 10490 trainingloss 0.6931471805599453
iteration 6 batch 10500 trainingloss 0.6931471805599453
iteration 6 batch 10510 trainingloss 0.6931471805599453
iteration 6 batch 10520 trainingloss 0.6931471805599453
iteration 6 batch 10530 trainingloss 0.6931471805599453
iteration 6 batch 10540 trainingloss 0.6931471805599453
iteration 6 batch 10550 trainingloss 0.6931471805599453
iteration 6 batch 10560 trainingloss 0.6931471805599453
iteration 6 batch 10570 trainingloss 0.6931471805599453
iteration 6 batch 10580 trainingloss 0.6931471805599453
iteration 6 batch 10590 trainingloss 0.6931471805599453
iteration 6 batch 10600 trainingloss 0.6931471805599453
iteration 6 batch 10610 trainingloss 0.6931471805599453
iteration 6 batch 10620 trainingloss 0.6931471805599453
iteration 6 batch 10630 trainingloss 0.6931471805599453
iteration 6 batch 10640 trainingloss 0.6931471805599453
iteration 6 batch 10650 trainingloss 0.6931471805599453
iteration 6 batch 10660 trainingloss 0.6916632528527511
iteration 6 batch 10670 trainingloss 0.6931471805599453
iteration 6 batch 10680 trainingloss 0.6931471805599453
iteration 6 batch 10690 trainingloss 0.6931471805599453
iteration 6 batch 10700 trainingloss 0.6931471805599453
iteration 6 batch 10710 trainingloss 0.6931471805599453
iteration 6 batch 10720 trainingloss 0.6916632528527511
iteration 6 batch 10730 trainingloss 0.6931471805599453
iteration 6 batch 10740 trainingloss 0.6931471805599453
iteration 6 batch 10750 trainingloss 0.6931471805599453
iteration 6 batch 10760 trainingloss 0.6931471805599453
iteration 6 batch 10770 trainingloss 0.6931471805599453
iteration 6 batch 10780 trainingloss 0.6931471805599453
iteration 6 batch 10790 trainingloss 0.6931471805599453
iteration 6 batch 10800 trainingloss 0.6931471805599453
iteration 6 batch 10810 trainingloss 0.6931471805599453
iteration 6 batch 10820 trainingloss 0.6931471805599453
iteration 6 batch 10830 trainingloss 0.6931471805599453
iteration 6 batch 10840 trainingloss 0.6931471805599453
iteration 6 batch 10850 trainingloss 0.6931471805599453
iteration 6 batch 10860 trainingloss 0.6916632528527511
iteration 6 batch 10870 trainingloss 0.6931471805599453
iteration 6 batch 10880 trainingloss 0.6931471805599453
iteration 6 batch 10890 trainingloss 0.6931471805599453
iteration 6 batch 10900 trainingloss 0.6931471805599453
iteration 6 batch 10910 trainingloss 0.6931471805599453
iteration 6 batch 10920 trainingloss 0.6931471805599453
iteration 6 batch 10930 trainingloss 0.6931471805599453
iteration 6 batch 10940 trainingloss 0.6931471805599453
iteration 6 batch 10950 trainingloss 0.6931471805599453
iteration 6 batch 10960 trainingloss 0.6931471805599453
iteration 6 batch 10970 trainingloss 0.6931471805599453
iteration 6 batch 10980 trainingloss 0.6916632528527511
iteration 6 batch 10990 trainingloss 0.6931471805599453
iteration 6 batch 11000 trainingloss 0.6931471805599453
iteration 6 batch 11010 trainingloss 0.6931471805599453
iteration 6 batch 11020 trainingloss 0.6931471805599453
iteration 6 batch 11030 trainingloss 0.6931471805599453
iteration 6 batch 11040 trainingloss 0.6931471805599453
iteration 6 batch 11050 trainingloss 0.6931471805599453
iteration 6 batch 11060 trainingloss 0.6931471805599453
iteration 6 batch 11070 trainingloss 0.6931471805599453
iteration 6 batch 11080 trainingloss 0.6916632528527511
iteration 6 batch 11090 trainingloss 0.6931471805599453
iteration 6 batch 11100 trainingloss 0.6931471805599453
iteration 6 batch 11110 trainingloss 0.6931471805599453
iteration 6 batch 11120 trainingloss 0.6931471805599453
iteration 6 batch 11130 trainingloss 0.6931471805599453
iteration 6 batch 11140 trainingloss 0.6931471805599453
iteration 6 batch 11150 trainingloss 0.6931471805599453
iteration 6 batch 11160 trainingloss 0.6931471805599453
iteration 6 batch 11170 trainingloss 0.6931471805599453
iteration 6 batch 11180 trainingloss 0.6916632528527511
iteration 6 batch 11190 trainingloss 0.6931471805599453
iteration 6 batch 11200 trainingloss 0.6931471805599453
iteration 6 batch 11210 trainingloss 0.6931471805599453
iteration 6 batch 11220 trainingloss 0.6931471805599453
iteration 6 batch 11230 trainingloss 0.6931471805599453
iteration 6 batch 11240 trainingloss 0.6931471805599453
iteration 6 batch 11250 trainingloss 0.6931471805599453
iteration 6 batch 11260 trainingloss 0.6931471805599453
iteration 6 batch 11270 trainingloss 0.6931471805599453
iteration 6 batch 11280 trainingloss 0.6931471805599453
iteration 6 batch 11290 trainingloss 0.6931471805599453
iteration 6 batch 11300 trainingloss 0.6931471805599453
iteration 6 batch 11310 trainingloss 0.6931471805599453
iteration 6 batch 11320 trainingloss 0.6931471805599453
iteration 6 batch 11330 trainingloss 0.6931471805599453
iteration 6 batch 11340 trainingloss 0.6931471805599453
iteration 6 batch 11350 trainingloss 0.6931471805599453
iteration 6 batch 11360 trainingloss 0.6931471805599453
iteration 6 batch 11370 trainingloss 0.6931471805599453
iteration 6 batch 11380 trainingloss 0.6931471805599453
iteration 6 batch 11390 trainingloss 0.6916632528527511
iteration 6 batch 11400 trainingloss 0.6931471805599453
iteration 6 batch 11410 trainingloss 0.6931471805599453
iteration 6 batch 11420 trainingloss 0.6931471805599453
iteration 6 batch 11430 trainingloss 0.6931471805599453
iteration 6 batch 11440 trainingloss 0.6931471805599453
iteration 6 batch 11450 trainingloss 0.6931471805599453
iteration 6 batch 11460 trainingloss 0.6931471805599453
iteration 6 batch 11470 trainingloss 0.6931471805599453
iteration 6 batch 11480 trainingloss 0.6931471805599453
iteration 6 batch 11490 trainingloss 0.6931471805599453
iteration 6 batch 11500 trainingloss 0.6931471805599453
iteration 6 batch 11510 trainingloss 0.6931471805599453
iteration 6 batch 11520 trainingloss 0.6931471805599453
iteration 6 batch 11530 trainingloss 0.6931471805599453
iteration 6 batch 11540 trainingloss 0.6931471805599453
iteration 6 batch 11550 trainingloss 0.6931471805599453
iteration 6 batch 11560 trainingloss 0.6931471805599453
iteration 6 batch 11570 trainingloss 0.6931471805599453
iteration 6 batch 11580 trainingloss 0.6931471805599453
iteration 6 batch 11590 trainingloss 0.6931471805599453
iteration 6 batch 11600 trainingloss 0.6931471805599453
iteration 6 batch 11610 trainingloss 0.6931471805599453
iteration 6 batch 11620 trainingloss 0.6931471805599453
iteration 6 batch 11630 trainingloss 0.6931471805599453
iteration 6 batch 11640 trainingloss 0.6931471805599453
iteration 6 batch 11650 trainingloss 0.6916632528527511
iteration 6 batch 11660 trainingloss 0.6931471805599453
iteration 6 batch 11670 trainingloss 0.6931471805599453
iteration 6 batch 11680 trainingloss 0.6931471805599453
iteration 6 batch 11690 trainingloss 0.6931471805599453
iteration 6 batch 11700 trainingloss 0.6931471805599453
iteration 6 batch 11710 trainingloss 0.6931471805599453
iteration 6 batch 11720 trainingloss 0.6931471805599453
iteration 6 batch 11730 trainingloss 0.6916632528527511
iteration 6 batch 11740 trainingloss 0.6931471805599453
iteration 6 batch 11750 trainingloss 0.6916632528527511
iteration 6 batch 11760 trainingloss 0.6931471805599453
iteration 6 batch 11770 trainingloss 0.6931471805599453
iteration 6 batch 11780 trainingloss 0.6916632528527511
iteration 6 batch 11790 trainingloss 0.6931471805599453
iteration 6 batch 11800 trainingloss 0.6931471805599453
iteration 6 batch 11810 trainingloss 0.6931471805599453
iteration 6 batch 11820 trainingloss 0.6931471805599453
iteration 6 batch 11830 trainingloss 0.6931471805599453
iteration 6 batch 11840 trainingloss 0.6931471805599453
iteration 6 batch 11850 trainingloss 0.6931471805599453
iteration 6 batch 11860 trainingloss 0.6931471805599453
iteration 6 batch 11870 trainingloss 0.6931471805599453
iteration 6 batch 11880 trainingloss 0.6931471805599453
iteration 6 batch 11890 trainingloss 0.6931471805599453
iteration 6 batch 11900 trainingloss 0.6931471805599453
iteration 6 batch 11910 trainingloss 0.6931471805599453
iteration 6 batch 11920 trainingloss 0.6931471805599453
iteration 6 batch 11930 trainingloss 0.6931471805599453
iteration 6 batch 11940 trainingloss 0.6931471805599453
iteration 6 batch 11950 trainingloss 0.6931471805599453
iteration 6 batch 11960 trainingloss 0.6931471805599453
iteration 6 batch 11970 trainingloss 0.6916632528527511
iteration 6 batch 11980 trainingloss 0.6931471805599453
iteration 6 batch 11990 trainingloss 0.6931471805599453
iteration 6 batch 12000 trainingloss 0.6931471805599453
iteration 6 batch 12010 trainingloss 0.6916632528527511
iteration 6 batch 12020 trainingloss 0.6931471805599453
iteration 6 batch 12030 trainingloss 0.6931471805599453
iteration 6 batch 12040 trainingloss 0.6931471805599453
iteration 6 batch 12050 trainingloss 0.6931471805599453
iteration 6 batch 12060 trainingloss 0.6931471805599453
iteration 6 batch 12070 trainingloss 0.6931471805599453
iteration 6 batch 12080 trainingloss 0.6931471805599453
iteration 6 batch 12090 trainingloss 0.6931471805599453
iteration 6 batch 12100 trainingloss 0.6931471805599453
iteration 6 batch 12110 trainingloss 0.6931471805599453
iteration 6 batch 12120 trainingloss 0.6931471805599453
iteration 6 batch 12130 trainingloss 0.6931471805599453
iteration 6 batch 12140 trainingloss 0.6931471805599453
iteration 6 batch 12150 trainingloss 0.6931471805599453
iteration 6 batch 12160 trainingloss 0.6931471805599453
iteration 6 batch 12170 trainingloss 0.6931471805599453
iteration 6 batch 12180 trainingloss 0.6931471805599453
iteration 6 batch 12190 trainingloss 0.6931471805599453
iteration 6 batch 12200 trainingloss 0.6931471805599453
iteration 6 batch 12210 trainingloss 0.6931471805599453
iteration 6 batch 12220 trainingloss 0.6931471805599453
iteration 6 batch 12230 trainingloss 0.6931471805599453
iteration 6 batch 12240 trainingloss 0.6931471805599453
iteration 6 batch 12250 trainingloss 0.6931471805599453
iteration 6 batch 12260 trainingloss 0.6931471805599453
iteration 6 batch 12270 trainingloss 0.6931471805599453
iteration 6 batch 12280 trainingloss 0.6931471805599453
iteration 6 batch 12290 trainingloss 0.6931471805599453
iteration 6 batch 12300 trainingloss 0.6931471805599453
iteration 6 batch 12310 trainingloss 0.6931471805599453
iteration 6 batch 12320 trainingloss 0.6931471805599453
iteration 6 batch 12330 trainingloss 0.6931471805599453
iteration 6 batch 12340 trainingloss 0.6931471805599453
iteration 6 batch 12350 trainingloss 0.6931471805599453
iteration 6 batch 12360 trainingloss 0.6931471805599453
iteration 6 batch 12370 trainingloss 0.6931471805599453
iteration 6 batch 12380 trainingloss 0.6931471805599453
iteration 6 batch 12390 trainingloss 0.6931471805599453
iteration 6 batch 12400 trainingloss 0.6931471805599453
iteration 6 batch 12410 trainingloss 0.6916632528527511
iteration 6 batch 12420 trainingloss 0.6931471805599453
iteration 6 batch 12430 trainingloss 0.6931471805599453
iteration 6 batch 12440 trainingloss 0.6931471805599453
iteration 6 batch 12450 trainingloss 0.6931471805599453
iteration 6 batch 12460 trainingloss 0.6931471805599453
iteration 6 batch 12470 trainingloss 0.6931471805599453
iteration 6 batch 12480 trainingloss 0.6931471805599453
iteration 6 batch 12490 trainingloss 0.6931471805599453
iteration 6 batch 12500 trainingloss 0.6931471805599453
iteration 6 batch 12510 trainingloss 0.6931471805599453
iteration 6 batch 12520 trainingloss 0.6931471805599453
iteration 6 batch 12530 trainingloss 0.6931471805599453
iteration 6 batch 12540 trainingloss 0.6931471805599453
iteration 6 batch 12550 trainingloss 0.6931471805599453
iteration 6 batch 12560 trainingloss 0.6931471805599453
iteration 6 batch 12570 trainingloss 0.6931471805599453
iteration 6 batch 12580 trainingloss 0.6931471805599453
iteration 6 batch 12590 trainingloss 0.6931471805599453
iteration 6 batch 12600 trainingloss 0.6931471805599453
iteration 6 batch 12610 trainingloss 0.6931471805599453
iteration 6 batch 12620 trainingloss 0.6931471805599453
iteration 6 batch 12630 trainingloss 0.6931471805599453
iteration 6 batch 12640 trainingloss 0.6931471805599453
iteration 6 batch 12650 trainingloss 0.6931471805599453
iteration 6 batch 12660 trainingloss 0.6931471805599453
iteration 6 batch 12670 trainingloss 0.6931471805599453
iteration 6 batch 12680 trainingloss 0.6916632528527511
iteration 6 batch 12690 trainingloss 0.6931471805599453
iteration 6 batch 12700 trainingloss 0.6931471805599453
iteration 6 batch 12710 trainingloss 0.6931471805599453
iteration 6 batch 12720 trainingloss 0.6931471805599453
iteration 6 batch 12730 trainingloss 0.6931471805599453
iteration 6 batch 12740 trainingloss 0.6931471805599453
iteration 6 batch 12750 trainingloss 0.6931471805599453
iteration 6 batch 12760 trainingloss 0.6931471805599453
iteration 6 batch 12770 trainingloss 0.6931471805599453
iteration 6 batch 12780 trainingloss 0.6931471805599453
iteration 6 batch 12790 trainingloss 0.6931471805599453
iteration 6 batch 12800 trainingloss 0.6931471805599453
iteration 6 batch 12810 trainingloss 0.6931471805599453
iteration 6 batch 12820 trainingloss 0.6931471805599453
iteration 6 batch 12830 trainingloss 0.6931471805599453
iteration 6 batch 12840 trainingloss 0.6931471805599453
iteration 6 batch 12850 trainingloss 0.6931471805599453
iteration 6 batch 12860 trainingloss 0.6931471805599453
iteration 6 batch 12870 trainingloss 0.6931471805599453
iteration 6 batch 12880 trainingloss 0.6931471805599453
iteration 6 batch 12890 trainingloss 0.6931471805599453
iteration 6 batch 12900 trainingloss 0.6931471805599453
iteration 6 batch 12910 trainingloss 0.6931471805599453
iteration 6 batch 12920 trainingloss 0.6931471805599453
iteration 6 batch 12930 trainingloss 0.6931471805599453
iteration 6 batch 12940 trainingloss 0.6931471805599453
iteration 6 batch 12950 trainingloss 0.6916632528527511
iteration 6 batch 12960 trainingloss 0.6931471805599453
iteration 6 batch 12970 trainingloss 0.6931471805599453
iteration 6 batch 12980 trainingloss 0.6931471805599453
iteration 6 batch 12990 trainingloss 0.6931471805599453
iteration 6 batch 13000 trainingloss 0.6931471805599453
iteration 6 batch 13010 trainingloss 0.6931471805599453
iteration 6 batch 13020 trainingloss 0.6931471805599453
iteration 6 batch 13030 trainingloss 0.6931471805599453
iteration 6 batch 13040 trainingloss 0.6931471805599453
iteration 6 batch 13050 trainingloss 0.6931471805599453
iteration 6 batch 13060 trainingloss 0.6931471805599453
iteration 6 batch 13070 trainingloss 0.6931471805599453
iteration 6 batch 13080 trainingloss 0.6931471805599453
iteration 6 batch 13090 trainingloss 0.6931471805599453
iteration 6 batch 13100 trainingloss 0.6931471805599453
iteration 6 batch 13110 trainingloss 0.6931471805599453
iteration 6 batch 13120 trainingloss 0.6931471805599453
iteration 6 batch 13130 trainingloss 0.6931471805599453
iteration 6 batch 13140 trainingloss 0.6931471805599453
iteration 6 batch 13150 trainingloss 0.6931471805599453
iteration 6 batch 13160 trainingloss 0.6931471805599453
iteration 6 batch 13170 trainingloss 0.6931471805599453
iteration 6 batch 13180 trainingloss 0.6931471805599453
iteration 6 batch 13190 trainingloss 0.6931471805599453
iteration 6 batch 13200 trainingloss 0.6931471805599453
iteration 6 batch 13210 trainingloss 0.6901793251455568
iteration 6 batch 13220 trainingloss 0.6931471805599453
iteration 6 batch 13230 trainingloss 0.6931471805599453
iteration 6 batch 13240 trainingloss 0.6931471805599453
iteration 6 batch 13250 trainingloss 0.6931471805599453
iteration 6 batch 13260 trainingloss 0.6931471805599453
iteration 6 batch 13270 trainingloss 0.6931471805599453
iteration 6 batch 13280 trainingloss 0.6931471805599453
iteration 6 batch 13290 trainingloss 0.6931471805599453
iteration 6 batch 13300 trainingloss 0.6931471805599453
iteration 6 batch 13310 trainingloss 0.6931471805599453
iteration 6 batch 13320 trainingloss 0.6916632528527511
iteration 6 batch 13330 trainingloss 0.6931471805599453
iteration 6 batch 13340 trainingloss 0.6931471805599453
iteration 6 batch 13350 trainingloss 0.6931471805599453
iteration 6 batch 13360 trainingloss 0.6931471805599453
iteration 6 batch 13370 trainingloss 0.6931471805599453
iteration 6 batch 13380 trainingloss 0.6931471805599453
iteration 6 batch 13390 trainingloss 0.6931471805599453
iteration 6 batch 13400 trainingloss 0.6916632528527511
iteration 6 batch 13410 trainingloss 0.6931471805599453
iteration 6 batch 13420 trainingloss 0.6931471805599453
iteration 6 batch 13430 trainingloss 0.6931471805599453
iteration 6 batch 13440 trainingloss 0.6916632528527511
iteration 6 batch 13450 trainingloss 0.6931471805599453
iteration 6 batch 13460 trainingloss 0.6931471805599453
iteration 6 batch 13470 trainingloss 0.6931471805599453
iteration 6 batch 13480 trainingloss 0.6931471805599453
iteration 6 batch 13490 trainingloss 0.6931471805599453
iteration 6 batch 13500 trainingloss 0.6931471805599453
iteration 6 batch 13510 trainingloss 0.6931471805599453
iteration 6 batch 13520 trainingloss 0.6931471805599453
iteration 6 batch 13530 trainingloss 0.6931471805599453
iteration 6 batch 13540 trainingloss 0.6931471805599453
iteration 6 batch 13550 trainingloss 0.6931471805599453
iteration 6 batch 13560 trainingloss 0.6931471805599453
iteration 6 batch 13570 trainingloss 0.6931471805599453
iteration 6 batch 13580 trainingloss 0.6931471805599453
iteration 6 batch 13590 trainingloss 0.6931471805599453
iteration 6 batch 13600 trainingloss 0.6931471805599453
iteration 6 batch 13610 trainingloss 0.6931471805599453
iteration 6 batch 13620 trainingloss 0.6916632528527511
iteration 6 batch 13630 trainingloss 0.6931471805599453
iteration 6 batch 13640 trainingloss 0.6931471805599453
iteration 6 batch 13650 trainingloss 0.6931471805599453
iteration 6 batch 13660 trainingloss 0.6931471805599453
iteration 6 batch 13670 trainingloss 0.6931471805599453
iteration 6 batch 13680 trainingloss 0.6931471805599453
iteration 6 batch 13690 trainingloss 0.6931471805599453
iteration 6 batch 13700 trainingloss 0.6931471805599453
iteration 6 batch 13710 trainingloss 0.6931471805599453
iteration 6 batch 13720 trainingloss 0.6931471805599453
iteration 6 batch 13730 trainingloss 0.6931471805599453
iteration 6 batch 13740 trainingloss 0.6931471805599453
iteration 6 batch 13750 trainingloss 0.6931471805599453
iteration 6 batch 13760 trainingloss 0.6931471805599453
iteration 6 batch 13770 trainingloss 0.6931471805599453
iteration 6 batch 13780 trainingloss 0.6931471805599453
iteration 6 batch 13790 trainingloss 0.6931471805599453
iteration 6 batch 13800 trainingloss 0.6931471805599453
iteration 6 batch 13810 trainingloss 0.6931471805599453
iteration 6 batch 13820 trainingloss 0.6931471805599453
iteration 6 batch 13830 trainingloss 0.6916632528527511
iteration 6 batch 13840 trainingloss 0.6931471805599453
iteration 6 batch 13850 trainingloss 0.6916632528527511
iteration 6 batch 13860 trainingloss 0.6931471805599453
iteration 6 batch 13870 trainingloss 0.6916632528527511
iteration 6 batch 13880 trainingloss 0.6931471805599453
iteration 6 batch 13890 trainingloss 0.6931471805599453
iteration 6 batch 13900 trainingloss 0.6931471805599453
iteration 6 batch 13910 trainingloss 0.6931471805599453
iteration 6 batch 13920 trainingloss 0.6931471805599453
iteration 6 batch 13930 trainingloss 0.6916632528527511
iteration 6 batch 13940 trainingloss 0.6931471805599453
iteration 6 batch 13950 trainingloss 0.6931471805599453
iteration 6 batch 13960 trainingloss 0.6931471805599453
iteration 6 batch 13970 trainingloss 0.6931471805599453
iteration 6 batch 13980 trainingloss 0.6931471805599453
iteration 6 batch 13990 trainingloss 0.6931471805599453
iteration 6 batch 14000 trainingloss 0.6931471805599453
iteration 6 batch 14010 trainingloss 0.6931471805599453
iteration 6 batch 14020 trainingloss 0.6931471805599453
iteration 6 batch 14030 trainingloss 0.6931471805599453
iteration 6 batch 14040 trainingloss 0.6931471805599453
iteration 6 batch 14050 trainingloss 0.6916632528527511
iteration 6 batch 14060 trainingloss 0.6916632528527511
iteration 6 batch 14070 trainingloss 0.6931471805599453
iteration 6 batch 14080 trainingloss 0.6931471805599453
iteration 6 batch 14090 trainingloss 0.6931471805599453
iteration 6 batch 14100 trainingloss 0.6931471805599453
iteration 6 batch 14110 trainingloss 0.6931471805599453
iteration 6 batch 14120 trainingloss 0.6931471805599453
iteration 6 batch 14130 trainingloss 0.6931471805599453
iteration 6 batch 14140 trainingloss 0.6931471805599453
iteration 6 batch 14150 trainingloss 0.6931471805599453
iteration 6 batch 14160 trainingloss 0.6931471805599453
iteration 6 batch 14170 trainingloss 0.6931471805599453
iteration 6 batch 14180 trainingloss 0.6931471805599453
iteration 6 batch 14190 trainingloss 0.6931471805599453
iteration 6 batch 14200 trainingloss 0.6931471805599453
iteration 6 batch 14210 trainingloss 0.6931471805599453
iteration 6 batch 14220 trainingloss 0.6931471805599453
iteration 6 batch 14230 trainingloss 0.6931471805599453
iteration 6 batch 14240 trainingloss 0.6931471805599453
iteration 6 batch 14250 trainingloss 0.6931471805599453
iteration 6 batch 14260 trainingloss 0.6931471805599453
iteration 6 batch 14270 trainingloss 0.6931471805599453
iteration 6 batch 14280 trainingloss 0.6931471805599453
iteration 6 batch 14290 trainingloss 0.6931471805599453
iteration 6 batch 14300 trainingloss 0.6931471805599453
iteration 6 batch 14310 trainingloss 0.6931471805599453
iteration 6 batch 14320 trainingloss 0.6931471805599453
iteration 6 batch 14330 trainingloss 0.6931471805599453
iteration 6 batch 14340 trainingloss 0.6931471805599453
iteration 6 batch 14350 trainingloss 0.6931471805599453
iteration 6 batch 14360 trainingloss 0.6931471805599453
iteration 6 batch 14370 trainingloss 0.6931471805599453
iteration 6 batch 14380 trainingloss 0.6931471805599453
iteration 6 batch 14390 trainingloss 0.6931471805599453
iteration 6 batch 14400 trainingloss 0.6931471805599453
iteration 6 batch 14410 trainingloss 0.6931471805599453
iteration 6 batch 14420 trainingloss 0.6931471805599453
iteration 6 batch 14430 trainingloss 0.6931471805599453
iteration 6 batch 14440 trainingloss 0.6931471805599453
iteration 6 batch 14450 trainingloss 0.6931471805599453
iteration 6 batch 14460 trainingloss 0.6931471805599453
iteration 6 batch 14470 trainingloss 0.6931471805599453
iteration 6 batch 14480 trainingloss 0.6931471805599453
iteration 6 batch 14490 trainingloss 0.6931471805599453
iteration 6 batch 14500 trainingloss 0.6931471805599453
iteration 6 batch 14510 trainingloss 0.6931471805599453
iteration 6 batch 14520 trainingloss 0.6931471805599453
iteration 6 batch 14530 trainingloss 0.6931471805599453
iteration 6 batch 14540 trainingloss 0.6931471805599453
iteration 6 batch 14550 trainingloss 0.6931471805599453
iteration 6 batch 14560 trainingloss 0.6931471805599453
iteration 6 batch 14570 trainingloss 0.6931471805599453
iteration 6 batch 14580 trainingloss 0.6916632528527511
iteration 6 batch 14590 trainingloss 0.6931471805599453
iteration 6 batch 14600 trainingloss 0.6931471805599453
iteration 6 batch 14610 trainingloss 0.6931471805599453
iteration 6 batch 14620 trainingloss 0.6931471805599453
iteration 6 batch 14630 trainingloss 0.6931471805599453
iteration 6 batch 14640 trainingloss 0.6931471805599453
iteration 6 batch 14650 trainingloss 0.6931471805599453
iteration 6 batch 14660 trainingloss 0.6931471805599453
iteration 6 batch 14670 trainingloss 0.6931471805599453
iteration 6 batch 14680 trainingloss 0.6931471805599453
iteration 6 batch 14690 trainingloss 0.6931471805599453
iteration 6 batch 14700 trainingloss 0.6931471805599453
iteration 6 batch 14710 trainingloss 0.6931471805599453
iteration 6 batch 14720 trainingloss 0.6931471805599453
iteration 6 batch 14730 trainingloss 0.6931471805599453
iteration 6 batch 14740 trainingloss 0.6931471805599453
iteration 6 batch 14750 trainingloss 0.6931471805599453
iteration 6 batch 14760 trainingloss 0.6931471805599453
iteration 6 batch 14770 trainingloss 0.6931471805599453
iteration 6 batch 14780 trainingloss 0.6931471805599453
iteration 6 batch 14790 trainingloss 0.6931471805599453
iteration 6 batch 14800 trainingloss 0.6931471805599453
iteration 6 batch 14810 trainingloss 0.6931471805599453
iteration 6 batch 14820 trainingloss 0.6931471805599453
iteration 6 batch 14830 trainingloss 0.6931471805599453
iteration 6 batch 14840 trainingloss 0.6931471805599453
iteration 6 batch 14850 trainingloss 0.6931471805599453
iteration 6 batch 14860 trainingloss 0.6931471805599453
iteration 6 batch 14870 trainingloss 0.6931471805599453
iteration 6 batch 14880 trainingloss 0.6931471805599453
iteration 6 batch 14890 trainingloss 0.6931471805599453
iteration 6 batch 14900 trainingloss 0.6931471805599453
iteration 6 batch 14910 trainingloss 0.6931471805599453
iteration 6 batch 14920 trainingloss 0.6931471805599453
iteration 6 batch 14930 trainingloss 0.6916632528527511
iteration 6 batch 14940 trainingloss 0.6931471805599453
iteration 6 batch 14950 trainingloss 0.6931471805599453
iteration 6 batch 14960 trainingloss 0.6931471805599453
iteration 6 batch 14970 trainingloss 0.6931471805599453
iteration 6 batch 14980 trainingloss 0.6931471805599453
iteration 6 batch 14990 trainingloss 0.6931471805599453
iteration 6 batch 15000 trainingloss 0.6931471805599453
iteration 6 batch 15010 trainingloss 0.6931471805599453
iteration 6 batch 15020 trainingloss 0.6931471805599453
iteration 6 batch 15030 trainingloss 0.6931471805599453
iteration 6 batch 15040 trainingloss 0.6931471805599453
iteration 6 batch 15050 trainingloss 0.6931471805599453
iteration 6 batch 15060 trainingloss 0.6931471805599453
iteration 6 batch 15070 trainingloss 0.6931471805599453
iteration 6 batch 15080 trainingloss 0.6931471805599453
iteration 6 batch 15090 trainingloss 0.6931471805599453
iteration 6 batch 15100 trainingloss 0.6916632528527511
iteration 6 batch 15110 trainingloss 0.6931471805599453
iteration 6 batch 15120 trainingloss 0.6931471805599453
iteration 6 batch 15130 trainingloss 0.6931471805599453
iteration 6 batch 15140 trainingloss 0.6931471805599453
iteration 6 batch 15150 trainingloss 0.6916632528527511
iteration 6 batch 15160 trainingloss 0.6931471805599453
iteration 6 batch 15170 trainingloss 0.6931471805599453
iteration 6 batch 15180 trainingloss 0.6931471805599453
iteration 6 batch 15190 trainingloss 0.6931471805599453
iteration 6 batch 15200 trainingloss 0.6931471805599453
iteration 6 batch 15210 trainingloss 0.6931471805599453
iteration 6 batch 15220 trainingloss 0.6931471805599453
iteration 6 batch 15230 trainingloss 0.6931471805599453
iteration 6 batch 15240 trainingloss 0.6931471805599453
iteration 6 batch 15250 trainingloss 0.6931471805599453
iteration 6 batch 15260 trainingloss 0.6931471805599453
iteration 6 batch 15270 trainingloss 0.6931471805599453
iteration 6 batch 15280 trainingloss 0.6931471805599453
iteration 6 batch 15290 trainingloss 0.6931471805599453
iteration 6 batch 15300 trainingloss 0.6931471805599453
iteration 6 batch 15310 trainingloss 0.6931471805599453
iteration 6 batch 15320 trainingloss 0.6931471805599453
iteration 6 batch 15330 trainingloss 0.6931471805599453
iteration 6 batch 15340 trainingloss 0.6931471805599453
iteration 6 batch 15350 trainingloss 0.6931471805599453
iteration 6 batch 15360 trainingloss 0.6931471805599453
iteration 6 batch 15370 trainingloss 0.6916632528527511
iteration 6 batch 15380 trainingloss 0.6931471805599453
iteration 6 batch 15390 trainingloss 0.6931471805599453
iteration 6 batch 15400 trainingloss 0.6931471805599453
iteration 6 batch 15410 trainingloss 0.6931471805599453
iteration 6 batch 15420 trainingloss 0.6931471805599453
iteration 6 batch 15430 trainingloss 0.6931471805599453
iteration 6 batch 15440 trainingloss 0.6931471805599453
iteration 6 batch 15450 trainingloss 0.6931471805599453
iteration 6 batch 15460 trainingloss 0.6931471805599453
iteration 6 batch 15470 trainingloss 0.6931471805599453
iteration 6 batch 15480 trainingloss 0.6931471805599453
iteration 6 batch 15490 trainingloss 0.6931471805599453
iteration 6 batch 15500 trainingloss 0.6931471805599453
iteration 6 batch 15510 trainingloss 0.6931471805599453
iteration 6 batch 15520 trainingloss 0.6931471805599453
iteration 6 batch 15530 trainingloss 0.6931471805599453
iteration 6 batch 15540 trainingloss 0.6916632528527511
iteration 6 batch 15550 trainingloss 0.6931471805599453
iteration 6 batch 15560 trainingloss 0.6931471805599453
iteration 6 batch 15570 trainingloss 0.6931471805599453
iteration 6 batch 15580 trainingloss 0.6931471805599453
iteration 6 batch 15590 trainingloss 0.6931471805599453
iteration 6 batch 15600 trainingloss 0.6931471805599453
iteration 6 batch 15610 trainingloss 0.6931471805599453
iteration 6 batch 15620 trainingloss 0.6931471805599453
iteration 6 batch 15630 trainingloss 0.6931471805599453
iteration 6 batch 15640 trainingloss 0.6931471805599453
iteration 6 batch 15650 trainingloss 0.6931471805599453
iteration 6 batch 15660 trainingloss 0.6931471805599453
iteration 6 batch 15670 trainingloss 0.6916632528527511
iteration 6 batch 15680 trainingloss 0.6931471805599453
iteration 6 batch 15690 trainingloss 0.6931471805599453
iteration 6 batch 15700 trainingloss 0.6931471805599453
iteration 6 batch 15710 trainingloss 0.6931471805599453
iteration 6 batch 15720 trainingloss 0.6931471805599453
iteration 6 batch 15730 trainingloss 0.6931471805599453
iteration 6 batch 15740 trainingloss 0.6931471805599453
iteration 6 batch 15750 trainingloss 0.6931471805599453
iteration 6 batch 15760 trainingloss 0.6931471805599453
iteration 6 batch 15770 trainingloss 0.6931471805599453
iteration 6 batch 15780 trainingloss 0.6931471805599453
iteration 6 batch 15790 trainingloss 0.6931471805599453
iteration 6 batch 15800 trainingloss 0.6931471805599453
iteration 6 batch 15810 trainingloss 0.6931471805599453
iteration 6 batch 15820 trainingloss 0.6931471805599453
iteration 6 batch 15830 trainingloss 0.6931471805599453
iteration 6 batch 15840 trainingloss 0.6931471805599453
iteration 6 batch 15850 trainingloss 0.6931471805599453
iteration 6 batch 15860 trainingloss 0.6931471805599453
iteration 6 batch 15870 trainingloss 0.6931471805599453
iteration 6 batch 15880 trainingloss 0.6931471805599453
iteration 6 batch 15890 trainingloss 0.6931471805599453
iteration 6 batch 15900 trainingloss 0.6931471805599453
iteration 6 batch 15910 trainingloss 0.6931471805599453
iteration 6 batch 15920 trainingloss 0.6931471805599453
iteration 6 batch 15930 trainingloss 0.6931471805599453
iteration 6 batch 15940 trainingloss 0.6931471805599453
iteration 6 batch 15950 trainingloss 0.6931471805599453
iteration 6 batch 15960 trainingloss 0.6931471805599453
iteration 6 batch 15970 trainingloss 0.6931471805599453
iteration 6 batch 15980 trainingloss 0.6931471805599453
iteration 6 batch 15990 trainingloss 0.6931471805599453
iteration 6 batch 16000 trainingloss 0.6931471805599453
iteration 6 batch 16010 trainingloss 0.6931471805599453
iteration 6 batch 16020 trainingloss 0.6931471805599453
iteration 6 batch 16030 trainingloss 0.6931471805599453
iteration 6 batch 16040 trainingloss 0.6931471805599453
iteration 6 batch 16050 trainingloss 0.6931471805599453
iteration 6 batch 16060 trainingloss 0.6931471805599453
iteration 6 batch 16070 trainingloss 0.6931471805599453
iteration 6 batch 16080 trainingloss 0.6931471805599453
iteration 6 batch 16090 trainingloss 0.6931471805599453
iteration 6 batch 16100 trainingloss 0.6931471805599453
iteration 6 batch 16110 trainingloss 0.6916632528527511
iteration 6 batch 16120 trainingloss 0.6916632528527511
iteration 6 batch 16130 trainingloss 0.6931471805599453
iteration 6 batch 16140 trainingloss 0.6931471805599453
iteration 6 batch 16150 trainingloss 0.6931471805599453
iteration 6 batch 16160 trainingloss 0.6931471805599453
iteration 6 batch 16170 trainingloss 0.6931471805599453
iteration 6 batch 16180 trainingloss 0.6931471805599453
iteration 6 batch 16190 trainingloss 0.6931471805599453
iteration 6 batch 16200 trainingloss 0.6931471805599453
iteration 6 batch 16210 trainingloss 0.6916632528527511
iteration 6 batch 16220 trainingloss 0.6931471805599453
iteration 6 batch 16230 trainingloss 0.6931471805599453
iteration 6 batch 16240 trainingloss 0.6931471805599453
iteration 6 batch 16250 trainingloss 0.6931471805599453
iteration 6 batch 16260 trainingloss 0.6931471805599453
iteration 6 batch 16270 trainingloss 0.6931471805599453
iteration 6 batch 16280 trainingloss 0.6931471805599453
iteration 6 batch 16290 trainingloss 0.6931471805599453
iteration 6 batch 16300 trainingloss 0.6931471805599453
iteration 6 batch 16310 trainingloss 0.6931471805599453
iteration 6 batch 16320 trainingloss 0.6931471805599453
iteration 6 batch 16330 trainingloss 0.6931471805599453
iteration 6 batch 16340 trainingloss 0.6931471805599453
iteration 6 batch 16350 trainingloss 0.6931471805599453
iteration 6 batch 16360 trainingloss 0.6931471805599453
iteration 6 batch 16370 trainingloss 0.6931471805599453
iteration 6 batch 16380 trainingloss 0.6931471805599453
iteration 6 batch 16390 trainingloss 0.6916632528527511
iteration 6 batch 16400 trainingloss 0.6931471805599453
iteration 6 batch 16410 trainingloss 0.6931471805599453
iteration 6 batch 16420 trainingloss 0.6931471805599453
iteration 6 batch 16430 trainingloss 0.6931471805599453
iteration 6 batch 16440 trainingloss 0.6931471805599453
iteration 6 batch 16450 trainingloss 0.6931471805599453
iteration 6 batch 16460 trainingloss 0.6931471805599453
iteration 6 batch 16470 trainingloss 0.6931471805599453
iteration 6 batch 16480 trainingloss 0.6931471805599453
iteration 6 batch 16490 trainingloss 0.6931471805599453
iteration 6 batch 16500 trainingloss 0.6931471805599453
iteration 6 batch 16510 trainingloss 0.6931471805599453
iteration 6 batch 16520 trainingloss 0.6931471805599453
iteration 6 batch 16530 trainingloss 0.6931471805599453
iteration 6 batch 16540 trainingloss 0.6931471805599453
iteration 6 batch 16550 trainingloss 0.6931471805599453
iteration 6 batch 16560 trainingloss 0.6931471805599453
iteration 6 batch 16570 trainingloss 0.6931471805599453
iteration 6 batch 16580 trainingloss 0.6931471805599453
iteration 6 batch 16590 trainingloss 0.6931471805599453
iteration 6 batch 16600 trainingloss 0.6931471805599453
iteration 6 batch 16610 trainingloss 0.6931471805599453
iteration 6 batch 16620 trainingloss 0.6931471805599453
iteration 6 batch 16630 trainingloss 0.6931471805599453
iteration 6 batch 16640 trainingloss 0.6916632528527511
iteration 6 batch 16650 trainingloss 0.6931471805599453
iteration 6 batch 16660 trainingloss 0.6931471805599453
iteration 6 batch 16670 trainingloss 0.6931471805599453
iteration 6 batch 16680 trainingloss 0.6931471805599453
iteration 6 batch 16690 trainingloss 0.6931471805599453
iteration 6 batch 16700 trainingloss 0.6931471805599453
iteration 6 batch 16710 trainingloss 0.6931471805599453
iteration 6 batch 16720 trainingloss 0.6931471805599453
iteration 6 batch 16730 trainingloss 0.6931471805599453
iteration 6 batch 16740 trainingloss 0.6931471805599453
iteration 6 batch 16750 trainingloss 0.6931471805599453
iteration 6 batch 16760 trainingloss 0.6931471805599453
iteration 6 batch 16770 trainingloss 0.6931471805599453
iteration 6 batch 16780 trainingloss 0.6931471805599453
iteration 6 batch 16790 trainingloss 0.6931471805599453
iteration 6 batch 16800 trainingloss 0.6931471805599453
iteration 6 batch 16810 trainingloss 0.6931471805599453
iteration 6 batch 16820 trainingloss 0.6931471805599453
iteration 6 batch 16830 trainingloss 0.6931471805599453
iteration 6 batch 16840 trainingloss 0.6931471805599453
iteration 6 batch 16850 trainingloss 0.6931471805599453
iteration 6 batch 16860 trainingloss 0.6931471805599453
iteration 6 batch 16870 trainingloss 0.6931471805599453
iteration 6 batch 16880 trainingloss 0.6931471805599453
iteration 6 batch 16890 trainingloss 0.6931471805599453
iteration 6 batch 16900 trainingloss 0.6916632528527511
iteration 6 batch 16910 trainingloss 0.6931471805599453
iteration 6 batch 16920 trainingloss 0.6931471805599453
iteration 6 batch 16930 trainingloss 0.6931471805599453
iteration 6 batch 16940 trainingloss 0.6931471805599453
iteration 6 batch 16950 trainingloss 0.6931471805599453
iteration 6 batch 16960 trainingloss 0.6931471805599453
iteration 6 batch 16970 trainingloss 0.6931471805599453
iteration 6 batch 16980 trainingloss 0.6931471805599453
iteration 6 batch 16990 trainingloss 0.6916632528527511
iteration 6 batch 17000 trainingloss 0.6931471805599453
iteration 6 batch 17010 trainingloss 0.6931471805599453
iteration 6 batch 17020 trainingloss 0.6931471805599453
iteration 6 batch 17030 trainingloss 0.6931471805599453
iteration 6 batch 17040 trainingloss 0.6931471805599453
iteration 6 batch 17050 trainingloss 0.6916632528527511
iteration 6 batch 17060 trainingloss 0.6931471805599453
iteration 6 batch 17070 trainingloss 0.6931471805599453
iteration 6 batch 17080 trainingloss 0.6931471805599453
iteration 6 batch 17090 trainingloss 0.6931471805599453
iteration 6 batch 17100 trainingloss 0.6931471805599453
iteration 6 batch 17110 trainingloss 0.6931471805599453
iteration 6 batch 17120 trainingloss 0.6931471805599453
iteration 6 batch 17130 trainingloss 0.6931471805599453
iteration 6 batch 17140 trainingloss 0.6931471805599453
iteration 6 batch 17150 trainingloss 0.6931471805599453
iteration 6 batch 17160 trainingloss 0.6931471805599453
iteration 6 batch 17170 trainingloss 0.6931471805599453
iteration 6 batch 17180 trainingloss 0.6931471805599453
iteration 6 batch 17190 trainingloss 0.6931471805599453
iteration 6 batch 17200 trainingloss 0.6931471805599453
iteration 6 batch 17210 trainingloss 0.6931471805599453
iteration 6 batch 17220 trainingloss 0.6931471805599453
iteration 6 batch 17230 trainingloss 0.6931471805599453
iteration 6 batch 17240 trainingloss 0.6931471805599453
iteration 6 batch 17250 trainingloss 0.6916632528527511
iteration 6 batch 17260 trainingloss 0.6931471805599453
iteration 6 batch 17270 trainingloss 0.6931471805599453
iteration 6 batch 17280 trainingloss 0.6931471805599453
iteration 6 batch 17290 trainingloss 0.6931471805599453
iteration 6 batch 17300 trainingloss 0.6931471805599453
iteration 6 batch 17310 trainingloss 0.6931471805599453
iteration 6 batch 17320 trainingloss 0.6931471805599453
iteration 6 batch 17330 trainingloss 0.6916632528527511
iteration 6 batch 17340 trainingloss 0.6931471805599453
iteration 6 batch 17350 trainingloss 0.6931471805599453
iteration 6 batch 17360 trainingloss 0.6931471805599453
iteration 6 batch 17370 trainingloss 0.6931471805599453
iteration 6 batch 17380 trainingloss 0.6931471805599453
iteration 6 batch 17390 trainingloss 0.6931471805599453
iteration 6 batch 17400 trainingloss 0.6931471805599453
iteration 6 batch 17410 trainingloss 0.6931471805599453
iteration 6 batch 17420 trainingloss 0.6931471805599453
iteration 6 batch 17430 trainingloss 0.6931471805599453
iteration 6 batch 17440 trainingloss 0.6931471805599453
iteration 6 batch 17450 trainingloss 0.6916632528527511
iteration 6 batch 17460 trainingloss 0.6931471805599453
iteration 6 batch 17470 trainingloss 0.6931471805599453
iteration 6 batch 17480 trainingloss 0.6931471805599453
iteration 6 batch 17490 trainingloss 0.6931471805599453
iteration 6 batch 17500 trainingloss 0.6916632528527511
iteration 6 batch 17510 trainingloss 0.6931471805599453
iteration 6 batch 17520 trainingloss 0.6931471805599453
iteration 6 batch 17530 trainingloss 0.6931471805599453
iteration 6 batch 17540 trainingloss 0.6931471805599453
iteration 6 batch 17550 trainingloss 0.6931471805599453
iteration 6 batch 17560 trainingloss 0.6931471805599453
iteration 6 batch 17570 trainingloss 0.6931471805599453
iteration 6 batch 17580 trainingloss 0.6931471805599453
iteration 6 batch 17590 trainingloss 0.6931471805599453
iteration 6 batch 17600 trainingloss 0.6931471805599453
iteration 6 batch 17610 trainingloss 0.6931471805599453
iteration 6 batch 17620 trainingloss 0.6931471805599453
iteration 6 batch 17630 trainingloss 0.6931471805599453
iteration 6 batch 17640 trainingloss 0.6931471805599453
iteration 6 batch 17650 trainingloss 0.6931471805599453
iteration 6 batch 17660 trainingloss 0.6931471805599453
iteration 6 batch 17670 trainingloss 0.6931471805599453
iteration 6 batch 17680 trainingloss 0.6931471805599453
iteration 6 batch 17690 trainingloss 0.6931471805599453
iteration 6 batch 17700 trainingloss 0.6931471805599453
iteration 6 batch 17710 trainingloss 0.6931471805599453
iteration 6 batch 17720 trainingloss 0.6931471805599453
iteration 6 batch 17730 trainingloss 0.6916632528527511
iteration 6 batch 17740 trainingloss 0.6931471805599453
iteration 6 batch 17750 trainingloss 0.6931471805599453
iteration 6 batch 17760 trainingloss 0.6931471805599453
iteration 6 batch 17770 trainingloss 0.6931471805599453
iteration 6 batch 17780 trainingloss 0.6931471805599453
iteration 6 batch 17790 trainingloss 0.6931471805599453
iteration 6 batch 17800 trainingloss 0.6931471805599453
iteration 6 batch 17810 trainingloss 0.6931471805599453
iteration 6 batch 17820 trainingloss 0.6931471805599453
iteration 6 batch 17830 trainingloss 0.6931471805599453
iteration 6 batch 17840 trainingloss 0.6931471805599453
iteration 6 batch 17850 trainingloss 0.6931471805599453
iteration 6 batch 17860 trainingloss 0.6931471805599453
iteration 6 batch 17870 trainingloss 0.6931471805599453
iteration 6 batch 17880 trainingloss 0.6931471805599453
iteration 6 batch 17890 trainingloss 0.6931471805599453
iteration 6 batch 17900 trainingloss 0.6931471805599453
iteration 6 batch 17910 trainingloss 0.6931471805599453
iteration 6 batch 17920 trainingloss 0.6931471805599453
iteration 6 batch 17930 trainingloss 0.6931471805599453
iteration 6 batch 17940 trainingloss 0.6931471805599453
iteration 6 batch 17950 trainingloss 0.6931471805599453
iteration 6 batch 17960 trainingloss 0.6931471805599453
iteration 6 batch 17970 trainingloss 0.6931471805599453
iteration 6 batch 17980 trainingloss 0.6931471805599453
iteration 6 batch 17990 trainingloss 0.6931471805599453
iteration 6 batch 18000 trainingloss 0.6931471805599453
iteration 6 batch 18010 trainingloss 0.6931471805599453
iteration 6 batch 18020 trainingloss 0.6931471805599453
iteration 6 batch 18030 trainingloss 0.6931471805599453
iteration 6 batch 18040 trainingloss 0.6931471805599453
iteration 6 batch 18050 trainingloss 0.6931471805599453
iteration 6 batch 18060 trainingloss 0.6931471805599453
iteration 6 batch 18070 trainingloss 0.6931471805599453
iteration 6 batch 18080 trainingloss 0.6931471805599453
iteration 6 batch 18090 trainingloss 0.6931471805599453
iteration 6 batch 18100 trainingloss 0.6931471805599453
iteration 6 batch 18110 trainingloss 0.6931471805599453
iteration 6 batch 18120 trainingloss 0.6931471805599453
iteration 6 batch 18130 trainingloss 0.6931471805599453
iteration 6 batch 18140 trainingloss 0.6931471805599453
iteration 6 batch 18150 trainingloss 0.6931471805599453
iteration 6 batch 18160 trainingloss 0.6931471805599453
iteration 6 batch 18170 trainingloss 0.6931471805599453
iteration 6 batch 18180 trainingloss 0.6916632528527511
iteration 6 batch 18190 trainingloss 0.6931471805599453
iteration 6 batch 18200 trainingloss 0.6931471805599453
iteration 6 batch 18210 trainingloss 0.6931471805599453
iteration 6 batch 18220 trainingloss 0.6931471805599453
iteration 6 batch 18230 trainingloss 0.6931471805599453
iteration 6 batch 18240 trainingloss 0.6931471805599453
iteration 6 batch 18250 trainingloss 0.6931471805599453
iteration 6 batch 18260 trainingloss 0.6931471805599453
iteration 6 batch 18270 trainingloss 0.6931471805599453
iteration 6 batch 18280 trainingloss 0.6931471805599453
iteration 6 batch 18290 trainingloss 0.6931471805599453
iteration 6 batch 18300 trainingloss 0.6931471805599453
iteration 6 batch 18310 trainingloss 0.6931471805599453
iteration 6 batch 18320 trainingloss 0.6931471805599453
iteration 6 batch 18330 trainingloss 0.6931471805599453
iteration 6 batch 18340 trainingloss 0.6931471805599453
iteration 6 batch 18350 trainingloss 0.6931471805599453
iteration 6 batch 18360 trainingloss 0.6931471805599453
iteration 6 batch 18370 trainingloss 0.6931471805599453
iteration 6 batch 18380 trainingloss 0.6931471805599453
iteration 6 batch 18390 trainingloss 0.6916632528527511
iteration 6 batch 18400 trainingloss 0.6931471805599453
iteration 6 batch 18410 trainingloss 0.6931471805599453
iteration 6 batch 18420 trainingloss 0.6931471805599453
iteration 6 batch 18430 trainingloss 0.6931471805599453
iteration 6 batch 18440 trainingloss 0.6931471805599453
iteration 6 batch 18450 trainingloss 0.6931471805599453
iteration 6 batch 18460 trainingloss 0.6931471805599453
iteration 6 batch 18470 trainingloss 0.6931471805599453
iteration 6 batch 18480 trainingloss 0.6931471805599453
iteration 6 batch 18490 trainingloss 0.6931471805599453
iteration 6 batch 18500 trainingloss 0.6931471805599453
iteration 6 batch 18510 trainingloss 0.6931471805599453
iteration 6 batch 18520 trainingloss 0.6931471805599453
iteration 6 batch 18530 trainingloss 0.6931471805599453
iteration 6 batch 18540 trainingloss 0.6931471805599453
iteration 6 batch 18550 trainingloss 0.6931471805599453
iteration 6 batch 18560 trainingloss 0.6931471805599453
iteration 6 batch 18570 trainingloss 0.6931471805599453
iteration 6 batch 18580 trainingloss 0.6931471805599453
iteration 6 batch 18590 trainingloss 0.6931471805599453
iteration 6 batch 18600 trainingloss 0.6931471805599453
iteration 6 batch 18610 trainingloss 0.6931471805599453
iteration 7 batch 0 trainingloss 0.6931471805599453
iteration 7 batch 10 trainingloss 0.6931471805599453
iteration 7 batch 20 trainingloss 0.6931471805599453
iteration 7 batch 30 trainingloss 0.6931471805599453
iteration 7 batch 40 trainingloss 0.6931471805599453
iteration 7 batch 50 trainingloss 0.6916632528527511
iteration 7 batch 60 trainingloss 0.6931471805599453
iteration 7 batch 70 trainingloss 0.6931471805599453
iteration 7 batch 80 trainingloss 0.6931471805599453
iteration 7 batch 90 trainingloss 0.6931471805599453
iteration 7 batch 100 trainingloss 0.6931471805599453
iteration 7 batch 110 trainingloss 0.6931471805599453
iteration 7 batch 120 trainingloss 0.6931471805599453
iteration 7 batch 130 trainingloss 0.6931471805599453
iteration 7 batch 140 trainingloss 0.6931471805599453
iteration 7 batch 150 trainingloss 0.6931471805599453
iteration 7 batch 160 trainingloss 0.6931471805599453
iteration 7 batch 170 trainingloss 0.6931471805599453
iteration 7 batch 180 trainingloss 0.6931471805599453
iteration 7 batch 190 trainingloss 0.6931471805599453
iteration 7 batch 200 trainingloss 0.6931471805599453
iteration 7 batch 210 trainingloss 0.6931471805599453
iteration 7 batch 220 trainingloss 0.6931471805599453
iteration 7 batch 230 trainingloss 0.6931471805599453
iteration 7 batch 240 trainingloss 0.6931471805599453
iteration 7 batch 250 trainingloss 0.6931471805599453
iteration 7 batch 260 trainingloss 0.6931471805599453
iteration 7 batch 270 trainingloss 0.6931471805599453
iteration 7 batch 280 trainingloss 0.6931471805599453
iteration 7 batch 290 trainingloss 0.6931471805599453
iteration 7 batch 300 trainingloss 0.6931471805599453
iteration 7 batch 310 trainingloss 0.6931471805599453
iteration 7 batch 320 trainingloss 0.6931471805599453
iteration 7 batch 330 trainingloss 0.6931471805599453
iteration 7 batch 340 trainingloss 0.6931471805599453
iteration 7 batch 350 trainingloss 0.6931471805599453
iteration 7 batch 360 trainingloss 0.6931471805599453
iteration 7 batch 370 trainingloss 0.6931471805599453
iteration 7 batch 380 trainingloss 0.6931471805599453
iteration 7 batch 390 trainingloss 0.6931471805599453
iteration 7 batch 400 trainingloss 0.6916632528527511
iteration 7 batch 410 trainingloss 0.6931471805599453
iteration 7 batch 420 trainingloss 0.6931471805599453
iteration 7 batch 430 trainingloss 0.6916632528527511
iteration 7 batch 440 trainingloss 0.6931471805599453
iteration 7 batch 450 trainingloss 0.6931471805599453
iteration 7 batch 460 trainingloss 0.6931471805599453
iteration 7 batch 470 trainingloss 0.6931471805599453
iteration 7 batch 480 trainingloss 0.6931471805599453
iteration 7 batch 490 trainingloss 0.6931471805599453
iteration 7 batch 500 trainingloss 0.6931471805599453
iteration 7 batch 510 trainingloss 0.6931471805599453
iteration 7 batch 520 trainingloss 0.6931471805599453
iteration 7 batch 530 trainingloss 0.6931471805599453
iteration 7 batch 540 trainingloss 0.6931471805599453
iteration 7 batch 550 trainingloss 0.6931471805599453
iteration 7 batch 560 trainingloss 0.6931471805599453
iteration 7 batch 570 trainingloss 0.6931471805599453
iteration 7 batch 580 trainingloss 0.6931471805599453
iteration 7 batch 590 trainingloss 0.6931471805599453
iteration 7 batch 600 trainingloss 0.6931471805599453
iteration 7 batch 610 trainingloss 0.6931471805599453
iteration 7 batch 620 trainingloss 0.6931471805599453
iteration 7 batch 630 trainingloss 0.6931471805599453
iteration 7 batch 640 trainingloss 0.6931471805599453
iteration 7 batch 650 trainingloss 0.6931471805599453
iteration 7 batch 660 trainingloss 0.6931471805599453
iteration 7 batch 670 trainingloss 0.6931471805599453
iteration 7 batch 680 trainingloss 0.6931471805599453
iteration 7 batch 690 trainingloss 0.6916632528527511
iteration 7 batch 700 trainingloss 0.6931471805599453
iteration 7 batch 710 trainingloss 0.6931471805599453
iteration 7 batch 720 trainingloss 0.6931471805599453
iteration 7 batch 730 trainingloss 0.6931471805599453
iteration 7 batch 740 trainingloss 0.6931471805599453
iteration 7 batch 750 trainingloss 0.6931471805599453
iteration 7 batch 760 trainingloss 0.6931471805599453
iteration 7 batch 770 trainingloss 0.6931471805599453
iteration 7 batch 780 trainingloss 0.6931471805599453
iteration 7 batch 790 trainingloss 0.6931471805599453
iteration 7 batch 800 trainingloss 0.6931471805599453
iteration 7 batch 810 trainingloss 0.6931471805599453
iteration 7 batch 820 trainingloss 0.6916632528527511
iteration 7 batch 830 trainingloss 0.6916632528527511
iteration 7 batch 840 trainingloss 0.6931471805599453
iteration 7 batch 850 trainingloss 0.6931471805599453
iteration 7 batch 860 trainingloss 0.6931471805599453
iteration 7 batch 870 trainingloss 0.6931471805599453
iteration 7 batch 880 trainingloss 0.6916632528527511
iteration 7 batch 890 trainingloss 0.6931471805599453
iteration 7 batch 900 trainingloss 0.6931471805599453
iteration 7 batch 910 trainingloss 0.6931471805599453
iteration 7 batch 920 trainingloss 0.6931471805599453
iteration 7 batch 930 trainingloss 0.6931471805599453
iteration 7 batch 940 trainingloss 0.6931471805599453
iteration 7 batch 950 trainingloss 0.6931471805599453
iteration 7 batch 960 trainingloss 0.6931471805599453
iteration 7 batch 970 trainingloss 0.6931471805599453
iteration 7 batch 980 trainingloss 0.6916632528527511
iteration 7 batch 990 trainingloss 0.6931471805599453
iteration 7 batch 1000 trainingloss 0.6931471805599453
iteration 7 batch 1010 trainingloss 0.6931471805599453
iteration 7 batch 1020 trainingloss 0.6931471805599453
iteration 7 batch 1030 trainingloss 0.6931471805599453
iteration 7 batch 1040 trainingloss 0.6931471805599453
iteration 7 batch 1050 trainingloss 0.6931471805599453
iteration 7 batch 1060 trainingloss 0.6931471805599453
iteration 7 batch 1070 trainingloss 0.6931471805599453
iteration 7 batch 1080 trainingloss 0.6931471805599453
iteration 7 batch 1090 trainingloss 0.6931471805599453
iteration 7 batch 1100 trainingloss 0.6931471805599453
iteration 7 batch 1110 trainingloss 0.6931471805599453
iteration 7 batch 1120 trainingloss 0.6931471805599453
iteration 7 batch 1130 trainingloss 0.6931471805599453
iteration 7 batch 1140 trainingloss 0.6931471805599453
iteration 7 batch 1150 trainingloss 0.6931471805599453
iteration 7 batch 1160 trainingloss 0.6931471805599453
iteration 7 batch 1170 trainingloss 0.6931471805599453
iteration 7 batch 1180 trainingloss 0.6931471805599453
iteration 7 batch 1190 trainingloss 0.6931471805599453
iteration 7 batch 1200 trainingloss 0.6931471805599453
iteration 7 batch 1210 trainingloss 0.6931471805599453
iteration 7 batch 1220 trainingloss 0.6931471805599453
iteration 7 batch 1230 trainingloss 0.6916632528527511
iteration 7 batch 1240 trainingloss 0.6931471805599453
iteration 7 batch 1250 trainingloss 0.6931471805599453
iteration 7 batch 1260 trainingloss 0.6931471805599453
iteration 7 batch 1270 trainingloss 0.6931471805599453
iteration 7 batch 1280 trainingloss 0.6931471805599453
iteration 7 batch 1290 trainingloss 0.6931471805599453
iteration 7 batch 1300 trainingloss 0.6931471805599453
iteration 7 batch 1310 trainingloss 0.6931471805599453
iteration 7 batch 1320 trainingloss 0.6931471805599453
iteration 7 batch 1330 trainingloss 0.6931471805599453
iteration 7 batch 1340 trainingloss 0.6931471805599453
iteration 7 batch 1350 trainingloss 0.6931471805599453
iteration 7 batch 1360 trainingloss 0.6931471805599453
iteration 7 batch 1370 trainingloss 0.6931471805599453
iteration 7 batch 1380 trainingloss 0.6931471805599453
iteration 7 batch 1390 trainingloss 0.6931471805599453
iteration 7 batch 1400 trainingloss 0.6931471805599453
iteration 7 batch 1410 trainingloss 0.6931471805599453
iteration 7 batch 1420 trainingloss 0.6931471805599453
iteration 7 batch 1430 trainingloss 0.6931471805599453
iteration 7 batch 1440 trainingloss 0.6931471805599453
iteration 7 batch 1450 trainingloss 0.6931471805599453
iteration 7 batch 1460 trainingloss 0.6931471805599453
iteration 7 batch 1470 trainingloss 0.6931471805599453
iteration 7 batch 1480 trainingloss 0.6931471805599453
iteration 7 batch 1490 trainingloss 0.6931471805599453
iteration 7 batch 1500 trainingloss 0.6931471805599453
iteration 7 batch 1510 trainingloss 0.6931471805599453
iteration 7 batch 1520 trainingloss 0.6931471805599453
iteration 7 batch 1530 trainingloss 0.6931471805599453
iteration 7 batch 1540 trainingloss 0.6931471805599453
iteration 7 batch 1550 trainingloss 0.6931471805599453
iteration 7 batch 1560 trainingloss 0.6931471805599453
iteration 7 batch 1570 trainingloss 0.6931471805599453
iteration 7 batch 1580 trainingloss 0.6931471805599453
iteration 7 batch 1590 trainingloss 0.6931471805599453
iteration 7 batch 1600 trainingloss 0.6931471805599453
iteration 7 batch 1610 trainingloss 0.6931471805599453
iteration 7 batch 1620 trainingloss 0.6931471805599453
iteration 7 batch 1630 trainingloss 0.6931471805599453
iteration 7 batch 1640 trainingloss 0.6931471805599453
iteration 7 batch 1650 trainingloss 0.6931471805599453
iteration 7 batch 1660 trainingloss 0.6931471805599453
iteration 7 batch 1670 trainingloss 0.6916632528527511
iteration 7 batch 1680 trainingloss 0.6931471805599453
iteration 7 batch 1690 trainingloss 0.6931471805599453
iteration 7 batch 1700 trainingloss 0.6931471805599453
iteration 7 batch 1710 trainingloss 0.6931471805599453
iteration 7 batch 1720 trainingloss 0.6931471805599453
iteration 7 batch 1730 trainingloss 0.6931471805599453
iteration 7 batch 1740 trainingloss 0.6916632528527511
iteration 7 batch 1750 trainingloss 0.6931471805599453
iteration 7 batch 1760 trainingloss 0.6931471805599453
iteration 7 batch 1770 trainingloss 0.6931471805599453
iteration 7 batch 1780 trainingloss 0.6931471805599453
iteration 7 batch 1790 trainingloss 0.6931471805599453
iteration 7 batch 1800 trainingloss 0.6931471805599453
iteration 7 batch 1810 trainingloss 0.6931471805599453
iteration 7 batch 1820 trainingloss 0.6931471805599453
iteration 7 batch 1830 trainingloss 0.6931471805599453
iteration 7 batch 1840 trainingloss 0.6931471805599453
iteration 7 batch 1850 trainingloss 0.6916632528527511
iteration 7 batch 1860 trainingloss 0.6931471805599453
iteration 7 batch 1870 trainingloss 0.6931471805599453
iteration 7 batch 1880 trainingloss 0.6931471805599453
iteration 7 batch 1890 trainingloss 0.6931471805599453
iteration 7 batch 1900 trainingloss 0.6931471805599453
iteration 7 batch 1910 trainingloss 0.6931471805599453
iteration 7 batch 1920 trainingloss 0.6931471805599453
iteration 7 batch 1930 trainingloss 0.6931471805599453
iteration 7 batch 1940 trainingloss 0.6931471805599453
iteration 7 batch 1950 trainingloss 0.6931471805599453
iteration 7 batch 1960 trainingloss 0.6931471805599453
iteration 7 batch 1970 trainingloss 0.6931471805599453
iteration 7 batch 1980 trainingloss 0.6931471805599453
iteration 7 batch 1990 trainingloss 0.6931471805599453
iteration 7 batch 2000 trainingloss 0.6931471805599453
iteration 7 batch 2010 trainingloss 0.6931471805599453
iteration 7 batch 2020 trainingloss 0.6931471805599453
iteration 7 batch 2030 trainingloss 0.6931471805599453
iteration 7 batch 2040 trainingloss 0.6931471805599453
iteration 7 batch 2050 trainingloss 0.6931471805599453
iteration 7 batch 2060 trainingloss 0.6931471805599453
iteration 7 batch 2070 trainingloss 0.6931471805599453
iteration 7 batch 2080 trainingloss 0.6931471805599453
iteration 7 batch 2090 trainingloss 0.6931471805599453
iteration 7 batch 2100 trainingloss 0.6931471805599453
iteration 7 batch 2110 trainingloss 0.6931471805599453
iteration 7 batch 2120 trainingloss 0.6931471805599453
iteration 7 batch 2130 trainingloss 0.6931471805599453
iteration 7 batch 2140 trainingloss 0.6931471805599453
iteration 7 batch 2150 trainingloss 0.6931471805599453
iteration 7 batch 2160 trainingloss 0.6931471805599453
iteration 7 batch 2170 trainingloss 0.6931471805599453
iteration 7 batch 2180 trainingloss 0.6931471805599453
iteration 7 batch 2190 trainingloss 0.6931471805599453
iteration 7 batch 2200 trainingloss 0.6931471805599453
iteration 7 batch 2210 trainingloss 0.6931471805599453
iteration 7 batch 2220 trainingloss 0.6931471805599453
iteration 7 batch 2230 trainingloss 0.6931471805599453
iteration 7 batch 2240 trainingloss 0.6931471805599453
iteration 7 batch 2250 trainingloss 0.6931471805599453
iteration 7 batch 2260 trainingloss 0.6931471805599453
iteration 7 batch 2270 trainingloss 0.6931471805599453
iteration 7 batch 2280 trainingloss 0.6931471805599453
iteration 7 batch 2290 trainingloss 0.6931471805599453
iteration 7 batch 2300 trainingloss 0.6931471805599453
iteration 7 batch 2310 trainingloss 0.6931471805599453
iteration 7 batch 2320 trainingloss 0.6931471805599453
iteration 7 batch 2330 trainingloss 0.6931471805599453
iteration 7 batch 2340 trainingloss 0.6931471805599453
iteration 7 batch 2350 trainingloss 0.6931471805599453
iteration 7 batch 2360 trainingloss 0.6931471805599453
iteration 7 batch 2370 trainingloss 0.6931471805599453
iteration 7 batch 2380 trainingloss 0.6931471805599453
iteration 7 batch 2390 trainingloss 0.6931471805599453
iteration 7 batch 2400 trainingloss 0.6931471805599453
iteration 7 batch 2410 trainingloss 0.6931471805599453
iteration 7 batch 2420 trainingloss 0.6931471805599453
iteration 7 batch 2430 trainingloss 0.6931471805599453
iteration 7 batch 2440 trainingloss 0.6931471805599453
iteration 7 batch 2450 trainingloss 0.6931471805599453
iteration 7 batch 2460 trainingloss 0.6931471805599453
iteration 7 batch 2470 trainingloss 0.6931471805599453
iteration 7 batch 2480 trainingloss 0.6931471805599453
iteration 7 batch 2490 trainingloss 0.6931471805599453
iteration 7 batch 2500 trainingloss 0.6931471805599453
iteration 7 batch 2510 trainingloss 0.6931471805599453
iteration 7 batch 2520 trainingloss 0.6931471805599453
iteration 7 batch 2530 trainingloss 0.6931471805599453
iteration 7 batch 2540 trainingloss 0.6931471805599453
iteration 7 batch 2550 trainingloss 0.6931471805599453
iteration 7 batch 2560 trainingloss 0.6931471805599453
iteration 7 batch 2570 trainingloss 0.6931471805599453
iteration 7 batch 2580 trainingloss 0.6931471805599453
iteration 7 batch 2590 trainingloss 0.6931471805599453
iteration 7 batch 2600 trainingloss 0.6931471805599453
iteration 7 batch 2610 trainingloss 0.6931471805599453
iteration 7 batch 2620 trainingloss 0.6931471805599453
iteration 7 batch 2630 trainingloss 0.6931471805599453
iteration 7 batch 2640 trainingloss 0.6931471805599453
iteration 7 batch 2650 trainingloss 0.6931471805599453
iteration 7 batch 2660 trainingloss 0.6931471805599453
iteration 7 batch 2670 trainingloss 0.6931471805599453
iteration 7 batch 2680 trainingloss 0.6931471805599453
iteration 7 batch 2690 trainingloss 0.6931471805599453
iteration 7 batch 2700 trainingloss 0.6931471805599453
iteration 7 batch 2710 trainingloss 0.6931471805599453
iteration 7 batch 2720 trainingloss 0.6931471805599453
iteration 7 batch 2730 trainingloss 0.6916632528527511
iteration 7 batch 2740 trainingloss 0.6931471805599453
iteration 7 batch 2750 trainingloss 0.6931471805599453
iteration 7 batch 2760 trainingloss 0.6931471805599453
iteration 7 batch 2770 trainingloss 0.6931471805599453
iteration 7 batch 2780 trainingloss 0.6931471805599453
iteration 7 batch 2790 trainingloss 0.6916632528527511
iteration 7 batch 2800 trainingloss 0.6931471805599453
iteration 7 batch 2810 trainingloss 0.6931471805599453
iteration 7 batch 2820 trainingloss 0.6931471805599453
iteration 7 batch 2830 trainingloss 0.6931471805599453
iteration 7 batch 2840 trainingloss 0.6931471805599453
iteration 7 batch 2850 trainingloss 0.6931471805599453
iteration 7 batch 2860 trainingloss 0.6931471805599453
iteration 7 batch 2870 trainingloss 0.6931471805599453
iteration 7 batch 2880 trainingloss 0.6931471805599453
iteration 7 batch 2890 trainingloss 0.6931471805599453
iteration 7 batch 2900 trainingloss 0.6931471805599453
iteration 7 batch 2910 trainingloss 0.6931471805599453
iteration 7 batch 2920 trainingloss 0.6916632528527511
iteration 7 batch 2930 trainingloss 0.6931471805599453
iteration 7 batch 2940 trainingloss 0.6931471805599453
iteration 7 batch 2950 trainingloss 0.6931471805599453
iteration 7 batch 2960 trainingloss 0.6931471805599453
iteration 7 batch 2970 trainingloss 0.6931471805599453
iteration 7 batch 2980 trainingloss 0.6931471805599453
iteration 7 batch 2990 trainingloss 0.6931471805599453
iteration 7 batch 3000 trainingloss 0.6931471805599453
iteration 7 batch 3010 trainingloss 0.6931471805599453
iteration 7 batch 3020 trainingloss 0.6931471805599453
iteration 7 batch 3030 trainingloss 0.6931471805599453
iteration 7 batch 3040 trainingloss 0.6931471805599453
iteration 7 batch 3050 trainingloss 0.6931471805599453
iteration 7 batch 3060 trainingloss 0.6931471805599453
iteration 7 batch 3070 trainingloss 0.6931471805599453
iteration 7 batch 3080 trainingloss 0.6931471805599453
iteration 7 batch 3090 trainingloss 0.6931471805599453
iteration 7 batch 3100 trainingloss 0.6931471805599453
iteration 7 batch 3110 trainingloss 0.6931471805599453
iteration 7 batch 3120 trainingloss 0.6931471805599453
iteration 7 batch 3130 trainingloss 0.6931471805599453
iteration 7 batch 3140 trainingloss 0.6931471805599453
iteration 7 batch 3150 trainingloss 0.6931471805599453
iteration 7 batch 3160 trainingloss 0.6931471805599453
iteration 7 batch 3170 trainingloss 0.6931471805599453
iteration 7 batch 3180 trainingloss 0.6931471805599453
iteration 7 batch 3190 trainingloss 0.6931471805599453
iteration 7 batch 3200 trainingloss 0.6931471805599453
iteration 7 batch 3210 trainingloss 0.6931471805599453
iteration 7 batch 3220 trainingloss 0.6931471805599453
iteration 7 batch 3230 trainingloss 0.6931471805599453
iteration 7 batch 3240 trainingloss 0.6916632528527511
iteration 7 batch 3250 trainingloss 0.6931471805599453
iteration 7 batch 3260 trainingloss 0.6931471805599453
iteration 7 batch 3270 trainingloss 0.6931471805599453
iteration 7 batch 3280 trainingloss 0.6931471805599453
iteration 7 batch 3290 trainingloss 0.6931471805599453
iteration 7 batch 3300 trainingloss 0.6931471805599453
iteration 7 batch 3310 trainingloss 0.6931471805599453
iteration 7 batch 3320 trainingloss 0.6931471805599453
iteration 7 batch 3330 trainingloss 0.6931471805599453
iteration 7 batch 3340 trainingloss 0.6931471805599453
iteration 7 batch 3350 trainingloss 0.6931471805599453
iteration 7 batch 3360 trainingloss 0.6931471805599453
iteration 7 batch 3370 trainingloss 0.6931471805599453
iteration 7 batch 3380 trainingloss 0.6931471805599453
iteration 7 batch 3390 trainingloss 0.6931471805599453
iteration 7 batch 3400 trainingloss 0.6931471805599453
iteration 7 batch 3410 trainingloss 0.6931471805599453
iteration 7 batch 3420 trainingloss 0.6931471805599453
iteration 7 batch 3430 trainingloss 0.6931471805599453
iteration 7 batch 3440 trainingloss 0.6931471805599453
iteration 7 batch 3450 trainingloss 0.6931471805599453
iteration 7 batch 3460 trainingloss 0.6931471805599453
iteration 7 batch 3470 trainingloss 0.6931471805599453
iteration 7 batch 3480 trainingloss 0.6931471805599453
iteration 7 batch 3490 trainingloss 0.6931471805599453
iteration 7 batch 3500 trainingloss 0.6931471805599453
iteration 7 batch 3510 trainingloss 0.6931471805599453
iteration 7 batch 3520 trainingloss 0.6931471805599453
iteration 7 batch 3530 trainingloss 0.6931471805599453
iteration 7 batch 3540 trainingloss 0.6931471805599453
iteration 7 batch 3550 trainingloss 0.6931471805599453
iteration 7 batch 3560 trainingloss 0.6931471805599453
iteration 7 batch 3570 trainingloss 0.6931471805599453
iteration 7 batch 3580 trainingloss 0.6931471805599453
iteration 7 batch 3590 trainingloss 0.6916632528527511
iteration 7 batch 3600 trainingloss 0.6931471805599453
iteration 7 batch 3610 trainingloss 0.6931471805599453
iteration 7 batch 3620 trainingloss 0.6931471805599453
iteration 7 batch 3630 trainingloss 0.6916632528527511
iteration 7 batch 3640 trainingloss 0.6931471805599453
iteration 7 batch 3650 trainingloss 0.6931471805599453
iteration 7 batch 3660 trainingloss 0.6931471805599453
iteration 7 batch 3670 trainingloss 0.6931471805599453
iteration 7 batch 3680 trainingloss 0.6931471805599453
iteration 7 batch 3690 trainingloss 0.6931471805599453
iteration 7 batch 3700 trainingloss 0.6931471805599453
iteration 7 batch 3710 trainingloss 0.6916632528527511
iteration 7 batch 3720 trainingloss 0.6931471805599453
iteration 7 batch 3730 trainingloss 0.6931471805599453
iteration 7 batch 3740 trainingloss 0.6931471805599453
iteration 7 batch 3750 trainingloss 0.6931471805599453
iteration 7 batch 3760 trainingloss 0.6916632528527511
iteration 7 batch 3770 trainingloss 0.6931471805599453
iteration 7 batch 3780 trainingloss 0.6931471805599453
iteration 7 batch 3790 trainingloss 0.6916632528527511
iteration 7 batch 3800 trainingloss 0.6931471805599453
iteration 7 batch 3810 trainingloss 0.6931471805599453
iteration 7 batch 3820 trainingloss 0.6931471805599453
iteration 7 batch 3830 trainingloss 0.6931471805599453
iteration 7 batch 3840 trainingloss 0.6931471805599453
iteration 7 batch 3850 trainingloss 0.6931471805599453
iteration 7 batch 3860 trainingloss 0.6931471805599453
iteration 7 batch 3870 trainingloss 0.6931471805599453
iteration 7 batch 3880 trainingloss 0.6931471805599453
iteration 7 batch 3890 trainingloss 0.6931471805599453
iteration 7 batch 3900 trainingloss 0.6931471805599453
iteration 7 batch 3910 trainingloss 0.6931471805599453
iteration 7 batch 3920 trainingloss 0.6931471805599453
iteration 7 batch 3930 trainingloss 0.6931471805599453
iteration 7 batch 3940 trainingloss 0.6931471805599453
iteration 7 batch 3950 trainingloss 0.6931471805599453
iteration 7 batch 3960 trainingloss 0.6931471805599453
iteration 7 batch 3970 trainingloss 0.6931471805599453
iteration 7 batch 3980 trainingloss 0.6931471805599453
iteration 7 batch 3990 trainingloss 0.6931471805599453
iteration 7 batch 4000 trainingloss 0.6931471805599453
iteration 7 batch 4010 trainingloss 0.6931471805599453
iteration 7 batch 4020 trainingloss 0.6931471805599453
iteration 7 batch 4030 trainingloss 0.6931471805599453
iteration 7 batch 4040 trainingloss 0.6931471805599453
iteration 7 batch 4050 trainingloss 0.6931471805599453
iteration 7 batch 4060 trainingloss 0.6931471805599453
iteration 7 batch 4070 trainingloss 0.6931471805599453
iteration 7 batch 4080 trainingloss 0.6931471805599453
iteration 7 batch 4090 trainingloss 0.6931471805599453
iteration 7 batch 4100 trainingloss 0.6931471805599453
iteration 7 batch 4110 trainingloss 0.6931471805599453
iteration 7 batch 4120 trainingloss 0.6931471805599453
iteration 7 batch 4130 trainingloss 0.6931471805599453
iteration 7 batch 4140 trainingloss 0.6931471805599453
iteration 7 batch 4150 trainingloss 0.6931471805599453
iteration 7 batch 4160 trainingloss 0.6931471805599453
iteration 7 batch 4170 trainingloss 0.6931471805599453
iteration 7 batch 4180 trainingloss 0.6931471805599453
iteration 7 batch 4190 trainingloss 0.6931471805599453
iteration 7 batch 4200 trainingloss 0.6931471805599453
iteration 7 batch 4210 trainingloss 0.6931471805599453
iteration 7 batch 4220 trainingloss 0.6931471805599453
iteration 7 batch 4230 trainingloss 0.6931471805599453
iteration 7 batch 4240 trainingloss 0.6931471805599453
iteration 7 batch 4250 trainingloss 0.6931471805599453
iteration 7 batch 4260 trainingloss 0.6931471805599453
iteration 7 batch 4270 trainingloss 0.6931471805599453
iteration 7 batch 4280 trainingloss 0.6931471805599453
iteration 7 batch 4290 trainingloss 0.6916632528527511
iteration 7 batch 4300 trainingloss 0.6931471805599453
iteration 7 batch 4310 trainingloss 0.6931471805599453
iteration 7 batch 4320 trainingloss 0.6931471805599453
iteration 7 batch 4330 trainingloss 0.6931471805599453
iteration 7 batch 4340 trainingloss 0.6931471805599453
iteration 7 batch 4350 trainingloss 0.6931471805599453
iteration 7 batch 4360 trainingloss 0.6931471805599453
iteration 7 batch 4370 trainingloss 0.6931471805599453
iteration 7 batch 4380 trainingloss 0.6931471805599453
iteration 7 batch 4390 trainingloss 0.6931471805599453
iteration 7 batch 4400 trainingloss 0.6931471805599453
iteration 7 batch 4410 trainingloss 0.6931471805599453
iteration 7 batch 4420 trainingloss 0.6931471805599453
iteration 7 batch 4430 trainingloss 0.6931471805599453
iteration 7 batch 4440 trainingloss 0.6931471805599453
iteration 7 batch 4450 trainingloss 0.6931471805599453
iteration 7 batch 4460 trainingloss 0.6931471805599453
iteration 7 batch 4470 trainingloss 0.6931471805599453
iteration 7 batch 4480 trainingloss 0.6931471805599453
iteration 7 batch 4490 trainingloss 0.6931471805599453
iteration 7 batch 4500 trainingloss 0.6931471805599453
iteration 7 batch 4510 trainingloss 0.6931471805599453
iteration 7 batch 4520 trainingloss 0.6931471805599453
iteration 7 batch 4530 trainingloss 0.6931471805599453
iteration 7 batch 4540 trainingloss 0.6931471805599453
iteration 7 batch 4550 trainingloss 0.6916632528527511
iteration 7 batch 4560 trainingloss 0.6931471805599453
iteration 7 batch 4570 trainingloss 0.6931471805599453
iteration 7 batch 4580 trainingloss 0.6931471805599453
iteration 7 batch 4590 trainingloss 0.6931471805599453
iteration 7 batch 4600 trainingloss 0.6931471805599453
iteration 7 batch 4610 trainingloss 0.6916632528527511
iteration 7 batch 4620 trainingloss 0.6931471805599453
iteration 7 batch 4630 trainingloss 0.6931471805599453
iteration 7 batch 4640 trainingloss 0.6931471805599453
iteration 7 batch 4650 trainingloss 0.6931471805599453
iteration 7 batch 4660 trainingloss 0.6931471805599453
iteration 7 batch 4670 trainingloss 0.6931471805599453
iteration 7 batch 4680 trainingloss 0.6931471805599453
iteration 7 batch 4690 trainingloss 0.6931471805599453
iteration 7 batch 4700 trainingloss 0.6931471805599453
iteration 7 batch 4710 trainingloss 0.6931471805599453
iteration 7 batch 4720 trainingloss 0.6931471805599453
iteration 7 batch 4730 trainingloss 0.6931471805599453
iteration 7 batch 4740 trainingloss 0.6931471805599453
iteration 7 batch 4750 trainingloss 0.6931471805599453
iteration 7 batch 4760 trainingloss 0.6931471805599453
iteration 7 batch 4770 trainingloss 0.6931471805599453
iteration 7 batch 4780 trainingloss 0.6931471805599453
iteration 7 batch 4790 trainingloss 0.6916632528527511
iteration 7 batch 4800 trainingloss 0.6931471805599453
iteration 7 batch 4810 trainingloss 0.6931471805599453
iteration 7 batch 4820 trainingloss 0.6931471805599453
iteration 7 batch 4830 trainingloss 0.6931471805599453
iteration 7 batch 4840 trainingloss 0.6931471805599453
iteration 7 batch 4850 trainingloss 0.6931471805599453
iteration 7 batch 4860 trainingloss 0.6931471805599453
iteration 7 batch 4870 trainingloss 0.6931471805599453
iteration 7 batch 4880 trainingloss 0.6931471805599453
iteration 7 batch 4890 trainingloss 0.6931471805599453
iteration 7 batch 4900 trainingloss 0.6931471805599453
iteration 7 batch 4910 trainingloss 0.6931471805599453
iteration 7 batch 4920 trainingloss 0.6931471805599453
iteration 7 batch 4930 trainingloss 0.6931471805599453
iteration 7 batch 4940 trainingloss 0.6931471805599453
iteration 7 batch 4950 trainingloss 0.6931471805599453
iteration 7 batch 4960 trainingloss 0.6931471805599453
iteration 7 batch 4970 trainingloss 0.6931471805599453
iteration 7 batch 4980 trainingloss 0.6931471805599453
iteration 7 batch 4990 trainingloss 0.6931471805599453
iteration 7 batch 5000 trainingloss 0.6931471805599453
iteration 7 batch 5010 trainingloss 0.6931471805599453
iteration 7 batch 5020 trainingloss 0.6931471805599453
iteration 7 batch 5030 trainingloss 0.6931471805599453
iteration 7 batch 5040 trainingloss 0.6931471805599453
iteration 7 batch 5050 trainingloss 0.6916632528527511
iteration 7 batch 5060 trainingloss 0.6931471805599453
iteration 7 batch 5070 trainingloss 0.6931471805599453
iteration 7 batch 5080 trainingloss 0.6931471805599453
iteration 7 batch 5090 trainingloss 0.6916632528527511
iteration 7 batch 5100 trainingloss 0.6931471805599453
iteration 7 batch 5110 trainingloss 0.6931471805599453
iteration 7 batch 5120 trainingloss 0.6931471805599453
iteration 7 batch 5130 trainingloss 0.6931471805599453
iteration 7 batch 5140 trainingloss 0.6931471805599453
iteration 7 batch 5150 trainingloss 0.6931471805599453
iteration 7 batch 5160 trainingloss 0.6931471805599453
iteration 7 batch 5170 trainingloss 0.6931471805599453
iteration 7 batch 5180 trainingloss 0.6931471805599453
iteration 7 batch 5190 trainingloss 0.6931471805599453
iteration 7 batch 5200 trainingloss 0.6931471805599453
iteration 7 batch 5210 trainingloss 0.6916632528527511
iteration 7 batch 5220 trainingloss 0.6931471805599453
iteration 7 batch 5230 trainingloss 0.6931471805599453
iteration 7 batch 5240 trainingloss 0.6931471805599453
iteration 7 batch 5250 trainingloss 0.6931471805599453
iteration 7 batch 5260 trainingloss 0.6916632528527511
iteration 7 batch 5270 trainingloss 0.6931471805599453
iteration 7 batch 5280 trainingloss 0.6931471805599453
iteration 7 batch 5290 trainingloss 0.6931471805599453
iteration 7 batch 5300 trainingloss 0.6931471805599453
iteration 7 batch 5310 trainingloss 0.6931471805599453
iteration 7 batch 5320 trainingloss 0.6931471805599453
iteration 7 batch 5330 trainingloss 0.6931471805599453
iteration 7 batch 5340 trainingloss 0.6931471805599453
iteration 7 batch 5350 trainingloss 0.6931471805599453
iteration 7 batch 5360 trainingloss 0.6931471805599453
iteration 7 batch 5370 trainingloss 0.6931471805599453
iteration 7 batch 5380 trainingloss 0.6931471805599453
iteration 7 batch 5390 trainingloss 0.6931471805599453
iteration 7 batch 5400 trainingloss 0.6931471805599453
iteration 7 batch 5410 trainingloss 0.6931471805599453
iteration 7 batch 5420 trainingloss 0.6931471805599453
iteration 7 batch 5430 trainingloss 0.6931471805599453
iteration 7 batch 5440 trainingloss 0.6931471805599453
iteration 7 batch 5450 trainingloss 0.6931471805599453
iteration 7 batch 5460 trainingloss 0.6931471805599453
iteration 7 batch 5470 trainingloss 0.6931471805599453
iteration 7 batch 5480 trainingloss 0.6931471805599453
iteration 7 batch 5490 trainingloss 0.6916632528527511
iteration 7 batch 5500 trainingloss 0.6931471805599453
iteration 7 batch 5510 trainingloss 0.6931471805599453
iteration 7 batch 5520 trainingloss 0.6931471805599453
iteration 7 batch 5530 trainingloss 0.6931471805599453
iteration 7 batch 5540 trainingloss 0.6931471805599453
iteration 7 batch 5550 trainingloss 0.6931471805599453
iteration 7 batch 5560 trainingloss 0.6916632528527511
iteration 7 batch 5570 trainingloss 0.6931471805599453
iteration 7 batch 5580 trainingloss 0.6916632528527511
iteration 7 batch 5590 trainingloss 0.6931471805599453
iteration 7 batch 5600 trainingloss 0.6931471805599453
iteration 7 batch 5610 trainingloss 0.6931471805599453
iteration 7 batch 5620 trainingloss 0.6931471805599453
iteration 7 batch 5630 trainingloss 0.6931471805599453
iteration 7 batch 5640 trainingloss 0.6931471805599453
iteration 7 batch 5650 trainingloss 0.6931471805599453
iteration 7 batch 5660 trainingloss 0.6931471805599453
iteration 7 batch 5670 trainingloss 0.6931471805599453
iteration 7 batch 5680 trainingloss 0.6931471805599453
iteration 7 batch 5690 trainingloss 0.6931471805599453
iteration 7 batch 5700 trainingloss 0.6931471805599453
iteration 7 batch 5710 trainingloss 0.6931471805599453
iteration 7 batch 5720 trainingloss 0.6931471805599453
iteration 7 batch 5730 trainingloss 0.6931471805599453
iteration 7 batch 5740 trainingloss 0.6931471805599453
iteration 7 batch 5750 trainingloss 0.6931471805599453
iteration 7 batch 5760 trainingloss 0.6931471805599453
iteration 7 batch 5770 trainingloss 0.6931471805599453
iteration 7 batch 5780 trainingloss 0.6916632528527511
iteration 7 batch 5790 trainingloss 0.6931471805599453
iteration 7 batch 5800 trainingloss 0.6931471805599453
iteration 7 batch 5810 trainingloss 0.6916632528527511
iteration 7 batch 5820 trainingloss 0.6931471805599453
iteration 7 batch 5830 trainingloss 0.6931471805599453
iteration 7 batch 5840 trainingloss 0.6931471805599453
iteration 7 batch 5850 trainingloss 0.6931471805599453
iteration 7 batch 5860 trainingloss 0.6931471805599453
iteration 7 batch 5870 trainingloss 0.6931471805599453
iteration 7 batch 5880 trainingloss 0.6931471805599453
iteration 7 batch 5890 trainingloss 0.6931471805599453
iteration 7 batch 5900 trainingloss 0.6931471805599453
iteration 7 batch 5910 trainingloss 0.6931471805599453
iteration 7 batch 5920 trainingloss 0.6931471805599453
iteration 7 batch 5930 trainingloss 0.6931471805599453
iteration 7 batch 5940 trainingloss 0.6931471805599453
iteration 7 batch 5950 trainingloss 0.6931471805599453
iteration 7 batch 5960 trainingloss 0.6931471805599453
iteration 7 batch 5970 trainingloss 0.6931471805599453
iteration 7 batch 5980 trainingloss 0.6931471805599453
iteration 7 batch 5990 trainingloss 0.6931471805599453
iteration 7 batch 6000 trainingloss 0.6931471805599453
iteration 7 batch 6010 trainingloss 0.6931471805599453
iteration 7 batch 6020 trainingloss 0.6931471805599453
iteration 7 batch 6030 trainingloss 0.6931471805599453
iteration 7 batch 6040 trainingloss 0.6931471805599453
iteration 7 batch 6050 trainingloss 0.6931471805599453
iteration 7 batch 6060 trainingloss 0.6931471805599453
iteration 7 batch 6070 trainingloss 0.6931471805599453
iteration 7 batch 6080 trainingloss 0.6931471805599453
iteration 7 batch 6090 trainingloss 0.6931471805599453
iteration 7 batch 6100 trainingloss 0.6931471805599453
iteration 7 batch 6110 trainingloss 0.6931471805599453
iteration 7 batch 6120 trainingloss 0.6931471805599453
iteration 7 batch 6130 trainingloss 0.6931471805599453
iteration 7 batch 6140 trainingloss 0.6931471805599453
iteration 7 batch 6150 trainingloss 0.6931471805599453
iteration 7 batch 6160 trainingloss 0.6931471805599453
iteration 7 batch 6170 trainingloss 0.6916632528527511
iteration 7 batch 6180 trainingloss 0.6916632528527511
iteration 7 batch 6190 trainingloss 0.6931471805599453
iteration 7 batch 6200 trainingloss 0.6931471805599453
iteration 7 batch 6210 trainingloss 0.6931471805599453
iteration 7 batch 6220 trainingloss 0.6931471805599453
iteration 7 batch 6230 trainingloss 0.6916632528527511
iteration 7 batch 6240 trainingloss 0.6916632528527511
iteration 7 batch 6250 trainingloss 0.6931471805599453
iteration 7 batch 6260 trainingloss 0.6931471805599453
iteration 7 batch 6270 trainingloss 0.6931471805599453
iteration 7 batch 6280 trainingloss 0.6931471805599453
iteration 7 batch 6290 trainingloss 0.6901793251455568
iteration 7 batch 6300 trainingloss 0.6931471805599453
iteration 7 batch 6310 trainingloss 0.6931471805599453
iteration 7 batch 6320 trainingloss 0.6931471805599453
iteration 7 batch 6330 trainingloss 0.6931471805599453
iteration 7 batch 6340 trainingloss 0.6931471805599453
iteration 7 batch 6350 trainingloss 0.6931471805599453
iteration 7 batch 6360 trainingloss 0.6931471805599453
iteration 7 batch 6370 trainingloss 0.6931471805599453
iteration 7 batch 6380 trainingloss 0.6931471805599453
iteration 7 batch 6390 trainingloss 0.6931471805599453
iteration 7 batch 6400 trainingloss 0.6931471805599453
iteration 7 batch 6410 trainingloss 0.6931471805599453
iteration 7 batch 6420 trainingloss 0.6931471805599453
iteration 7 batch 6430 trainingloss 0.6931471805599453
iteration 7 batch 6440 trainingloss 0.6931471805599453
iteration 7 batch 6450 trainingloss 0.6931471805599453
iteration 7 batch 6460 trainingloss 0.6931471805599453
iteration 7 batch 6470 trainingloss 0.6931471805599453
iteration 7 batch 6480 trainingloss 0.6931471805599453
iteration 7 batch 6490 trainingloss 0.6931471805599453
iteration 7 batch 6500 trainingloss 0.6916632528527511
iteration 7 batch 6510 trainingloss 0.6931471805599453
iteration 7 batch 6520 trainingloss 0.6931471805599453
iteration 7 batch 6530 trainingloss 0.6931471805599453
iteration 7 batch 6540 trainingloss 0.6931471805599453
iteration 7 batch 6550 trainingloss 0.6931471805599453
iteration 7 batch 6560 trainingloss 0.6931471805599453
iteration 7 batch 6570 trainingloss 0.6931471805599453
iteration 7 batch 6580 trainingloss 0.6931471805599453
iteration 7 batch 6590 trainingloss 0.6931471805599453
iteration 7 batch 6600 trainingloss 0.6931471805599453
iteration 7 batch 6610 trainingloss 0.6931471805599453
iteration 7 batch 6620 trainingloss 0.6931471805599453
iteration 7 batch 6630 trainingloss 0.6931471805599453
iteration 7 batch 6640 trainingloss 0.6931471805599453
iteration 7 batch 6650 trainingloss 0.6931471805599453
iteration 7 batch 6660 trainingloss 0.6931471805599453
iteration 7 batch 6670 trainingloss 0.6931471805599453
iteration 7 batch 6680 trainingloss 0.6931471805599453
iteration 7 batch 6690 trainingloss 0.6931471805599453
iteration 7 batch 6700 trainingloss 0.6931471805599453
iteration 7 batch 6710 trainingloss 0.6931471805599453
iteration 7 batch 6720 trainingloss 0.6931471805599453
iteration 7 batch 6730 trainingloss 0.6931471805599453
iteration 7 batch 6740 trainingloss 0.6931471805599453
iteration 7 batch 6750 trainingloss 0.6931471805599453
iteration 7 batch 6760 trainingloss 0.6931471805599453
iteration 7 batch 6770 trainingloss 0.6931471805599453
iteration 7 batch 6780 trainingloss 0.6931471805599453
iteration 7 batch 6790 trainingloss 0.6931471805599453
iteration 7 batch 6800 trainingloss 0.6931471805599453
iteration 7 batch 6810 trainingloss 0.6931471805599453
iteration 7 batch 6820 trainingloss 0.6931471805599453
iteration 7 batch 6830 trainingloss 0.6931471805599453
iteration 7 batch 6840 trainingloss 0.6931471805599453
iteration 7 batch 6850 trainingloss 0.6931471805599453
iteration 7 batch 6860 trainingloss 0.6931471805599453
iteration 7 batch 6870 trainingloss 0.6931471805599453
iteration 7 batch 6880 trainingloss 0.6931471805599453
iteration 7 batch 6890 trainingloss 0.6931471805599453
iteration 7 batch 6900 trainingloss 0.6931471805599453
iteration 7 batch 6910 trainingloss 0.6931471805599453
iteration 7 batch 6920 trainingloss 0.6931471805599453
iteration 7 batch 6930 trainingloss 0.6931471805599453
iteration 7 batch 6940 trainingloss 0.6931471805599453
iteration 7 batch 6950 trainingloss 0.6931471805599453
iteration 7 batch 6960 trainingloss 0.6931471805599453
iteration 7 batch 6970 trainingloss 0.6931471805599453
iteration 7 batch 6980 trainingloss 0.6931471805599453
iteration 7 batch 6990 trainingloss 0.6931471805599453
iteration 7 batch 7000 trainingloss 0.6931471805599453
iteration 7 batch 7010 trainingloss 0.6931471805599453
iteration 7 batch 7020 trainingloss 0.6931471805599453
iteration 7 batch 7030 trainingloss 0.6931471805599453
iteration 7 batch 7040 trainingloss 0.6931471805599453
iteration 7 batch 7050 trainingloss 0.6931471805599453
iteration 7 batch 7060 trainingloss 0.6931471805599453
iteration 7 batch 7070 trainingloss 0.6931471805599453
iteration 7 batch 7080 trainingloss 0.6931471805599453
iteration 7 batch 7090 trainingloss 0.6931471805599453
iteration 7 batch 7100 trainingloss 0.6931471805599453
iteration 7 batch 7110 trainingloss 0.6931471805599453
iteration 7 batch 7120 trainingloss 0.6931471805599453
iteration 7 batch 7130 trainingloss 0.6931471805599453
iteration 7 batch 7140 trainingloss 0.6931471805599453
iteration 7 batch 7150 trainingloss 0.6931471805599453
iteration 7 batch 7160 trainingloss 0.6931471805599453
iteration 7 batch 7170 trainingloss 0.6931471805599453
iteration 7 batch 7180 trainingloss 0.6931471805599453
iteration 7 batch 7190 trainingloss 0.6931471805599453
iteration 7 batch 7200 trainingloss 0.6931471805599453
iteration 7 batch 7210 trainingloss 0.6931471805599453
iteration 7 batch 7220 trainingloss 0.6931471805599453
iteration 7 batch 7230 trainingloss 0.6931471805599453
iteration 7 batch 7240 trainingloss 0.6931471805599453
iteration 7 batch 7250 trainingloss 0.6931471805599453
iteration 7 batch 7260 trainingloss 0.6931471805599453
iteration 7 batch 7270 trainingloss 0.6931471805599453
iteration 7 batch 7280 trainingloss 0.6931471805599453
iteration 7 batch 7290 trainingloss 0.6931471805599453
iteration 7 batch 7300 trainingloss 0.6931471805599453
iteration 7 batch 7310 trainingloss 0.6931471805599453
iteration 7 batch 7320 trainingloss 0.6931471805599453
iteration 7 batch 7330 trainingloss 0.6931471805599453
iteration 7 batch 7340 trainingloss 0.6931471805599453
iteration 7 batch 7350 trainingloss 0.6931471805599453
iteration 7 batch 7360 trainingloss 0.6931471805599453
iteration 7 batch 7370 trainingloss 0.6916632528527511
iteration 7 batch 7380 trainingloss 0.6931471805599453
iteration 7 batch 7390 trainingloss 0.6931471805599453
iteration 7 batch 7400 trainingloss 0.6931471805599453
iteration 7 batch 7410 trainingloss 0.6931471805599453
iteration 7 batch 7420 trainingloss 0.6931471805599453
iteration 7 batch 7430 trainingloss 0.6931471805599453
iteration 7 batch 7440 trainingloss 0.6931471805599453
iteration 7 batch 7450 trainingloss 0.6931471805599453
iteration 7 batch 7460 trainingloss 0.6916632528527511
iteration 7 batch 7470 trainingloss 0.6931471805599453
iteration 7 batch 7480 trainingloss 0.6931471805599453
iteration 7 batch 7490 trainingloss 0.6931471805599453
iteration 7 batch 7500 trainingloss 0.6931471805599453
iteration 7 batch 7510 trainingloss 0.6931471805599453
iteration 7 batch 7520 trainingloss 0.6931471805599453
iteration 7 batch 7530 trainingloss 0.6931471805599453
iteration 7 batch 7540 trainingloss 0.6931471805599453
iteration 7 batch 7550 trainingloss 0.6931471805599453
iteration 7 batch 7560 trainingloss 0.6931471805599453
iteration 7 batch 7570 trainingloss 0.6931471805599453
iteration 7 batch 7580 trainingloss 0.6931471805599453
iteration 7 batch 7590 trainingloss 0.6931471805599453
iteration 7 batch 7600 trainingloss 0.6931471805599453
iteration 7 batch 7610 trainingloss 0.6931471805599453
iteration 7 batch 7620 trainingloss 0.6931471805599453
iteration 7 batch 7630 trainingloss 0.6931471805599453
iteration 7 batch 7640 trainingloss 0.6931471805599453
iteration 7 batch 7650 trainingloss 0.6931471805599453
iteration 7 batch 7660 trainingloss 0.6931471805599453
iteration 7 batch 7670 trainingloss 0.6931471805599453
iteration 7 batch 7680 trainingloss 0.6931471805599453
iteration 7 batch 7690 trainingloss 0.6916632528527511
iteration 7 batch 7700 trainingloss 0.6931471805599453
iteration 7 batch 7710 trainingloss 0.6931471805599453
iteration 7 batch 7720 trainingloss 0.6916632528527511
iteration 7 batch 7730 trainingloss 0.6931471805599453
iteration 7 batch 7740 trainingloss 0.6931471805599453
iteration 7 batch 7750 trainingloss 0.6931471805599453
iteration 7 batch 7760 trainingloss 0.6916632528527511
iteration 7 batch 7770 trainingloss 0.6931471805599453
iteration 7 batch 7780 trainingloss 0.6931471805599453
iteration 7 batch 7790 trainingloss 0.6931471805599453
iteration 7 batch 7800 trainingloss 0.6931471805599453
iteration 7 batch 7810 trainingloss 0.6931471805599453
iteration 7 batch 7820 trainingloss 0.6931471805599453
iteration 7 batch 7830 trainingloss 0.6931471805599453
iteration 7 batch 7840 trainingloss 0.6931471805599453
iteration 7 batch 7850 trainingloss 0.6931471805599453
iteration 7 batch 7860 trainingloss 0.6931471805599453
iteration 7 batch 7870 trainingloss 0.6931471805599453
iteration 7 batch 7880 trainingloss 0.6931471805599453
iteration 7 batch 7890 trainingloss 0.6931471805599453
iteration 7 batch 7900 trainingloss 0.6931471805599453
iteration 7 batch 7910 trainingloss 0.6931471805599453
iteration 7 batch 7920 trainingloss 0.6931471805599453
iteration 7 batch 7930 trainingloss 0.6931471805599453
iteration 7 batch 7940 trainingloss 0.6931471805599453
iteration 7 batch 7950 trainingloss 0.6931471805599453
iteration 7 batch 7960 trainingloss 0.6931471805599453
iteration 7 batch 7970 trainingloss 0.6931471805599453
iteration 7 batch 7980 trainingloss 0.6931471805599453
iteration 7 batch 7990 trainingloss 0.6931471805599453
iteration 7 batch 8000 trainingloss 0.6931471805599453
iteration 7 batch 8010 trainingloss 0.6931471805599453
iteration 7 batch 8020 trainingloss 0.6931471805599453
iteration 7 batch 8030 trainingloss 0.6931471805599453
iteration 7 batch 8040 trainingloss 0.6931471805599453
iteration 7 batch 8050 trainingloss 0.6931471805599453
iteration 7 batch 8060 trainingloss 0.6931471805599453
iteration 7 batch 8070 trainingloss 0.6931471805599453
iteration 7 batch 8080 trainingloss 0.6916632528527511
iteration 7 batch 8090 trainingloss 0.6931471805599453
iteration 7 batch 8100 trainingloss 0.6931471805599453
iteration 7 batch 8110 trainingloss 0.6931471805599453
iteration 7 batch 8120 trainingloss 0.6931471805599453
iteration 7 batch 8130 trainingloss 0.6931471805599453
iteration 7 batch 8140 trainingloss 0.6931471805599453
iteration 7 batch 8150 trainingloss 0.6931471805599453
iteration 7 batch 8160 trainingloss 0.6931471805599453
iteration 7 batch 8170 trainingloss 0.6931471805599453
iteration 7 batch 8180 trainingloss 0.6931471805599453
iteration 7 batch 8190 trainingloss 0.6931471805599453
iteration 7 batch 8200 trainingloss 0.6931471805599453
iteration 7 batch 8210 trainingloss 0.6931471805599453
iteration 7 batch 8220 trainingloss 0.6931471805599453
iteration 7 batch 8230 trainingloss 0.6931471805599453
iteration 7 batch 8240 trainingloss 0.6931471805599453
iteration 7 batch 8250 trainingloss 0.6931471805599453
iteration 7 batch 8260 trainingloss 0.6931471805599453
iteration 7 batch 8270 trainingloss 0.6931471805599453
iteration 7 batch 8280 trainingloss 0.6931471805599453
iteration 7 batch 8290 trainingloss 0.6916632528527511
iteration 7 batch 8300 trainingloss 0.6931471805599453
iteration 7 batch 8310 trainingloss 0.6931471805599453
iteration 7 batch 8320 trainingloss 0.6931471805599453
iteration 7 batch 8330 trainingloss 0.6931471805599453
iteration 7 batch 8340 trainingloss 0.6931471805599453
iteration 7 batch 8350 trainingloss 0.6931471805599453
iteration 7 batch 8360 trainingloss 0.6931471805599453
iteration 7 batch 8370 trainingloss 0.6931471805599453
iteration 7 batch 8380 trainingloss 0.6931471805599453
iteration 7 batch 8390 trainingloss 0.6931471805599453
iteration 7 batch 8400 trainingloss 0.6931471805599453
iteration 7 batch 8410 trainingloss 0.6931471805599453
iteration 7 batch 8420 trainingloss 0.6931471805599453
iteration 7 batch 8430 trainingloss 0.6931471805599453
iteration 7 batch 8440 trainingloss 0.6931471805599453
iteration 7 batch 8450 trainingloss 0.6931471805599453
iteration 7 batch 8460 trainingloss 0.6931471805599453
iteration 7 batch 8470 trainingloss 0.6931471805599453
iteration 7 batch 8480 trainingloss 0.6931471805599453
iteration 7 batch 8490 trainingloss 0.6931471805599453
iteration 7 batch 8500 trainingloss 0.6931471805599453
iteration 7 batch 8510 trainingloss 0.6931471805599453
iteration 7 batch 8520 trainingloss 0.6916632528527511
iteration 7 batch 8530 trainingloss 0.6931471805599453
iteration 7 batch 8540 trainingloss 0.6931471805599453
iteration 7 batch 8550 trainingloss 0.6931471805599453
iteration 7 batch 8560 trainingloss 0.6931471805599453
iteration 7 batch 8570 trainingloss 0.6931471805599453
iteration 7 batch 8580 trainingloss 0.6931471805599453
iteration 7 batch 8590 trainingloss 0.6931471805599453
iteration 7 batch 8600 trainingloss 0.6931471805599453
iteration 7 batch 8610 trainingloss 0.6931471805599453
iteration 7 batch 8620 trainingloss 0.6931471805599453
iteration 7 batch 8630 trainingloss 0.6931471805599453
iteration 7 batch 8640 trainingloss 0.6931471805599453
iteration 7 batch 8650 trainingloss 0.6931471805599453
iteration 7 batch 8660 trainingloss 0.6931471805599453
iteration 7 batch 8670 trainingloss 0.6931471805599453
iteration 7 batch 8680 trainingloss 0.6931471805599453
iteration 7 batch 8690 trainingloss 0.6931471805599453
iteration 7 batch 8700 trainingloss 0.6931471805599453
iteration 7 batch 8710 trainingloss 0.6931471805599453
iteration 7 batch 8720 trainingloss 0.6931471805599453
iteration 7 batch 8730 trainingloss 0.6931471805599453
iteration 7 batch 8740 trainingloss 0.6931471805599453
iteration 7 batch 8750 trainingloss 0.6931471805599453
iteration 7 batch 8760 trainingloss 0.6931471805599453
iteration 7 batch 8770 trainingloss 0.6931471805599453
iteration 7 batch 8780 trainingloss 0.6931471805599453
iteration 7 batch 8790 trainingloss 0.6931471805599453
iteration 7 batch 8800 trainingloss 0.6931471805599453
iteration 7 batch 8810 trainingloss 0.6931471805599453
iteration 7 batch 8820 trainingloss 0.6931471805599453
iteration 7 batch 8830 trainingloss 0.6931471805599453
iteration 7 batch 8840 trainingloss 0.6931471805599453
iteration 7 batch 8850 trainingloss 0.6931471805599453
iteration 7 batch 8860 trainingloss 0.6931471805599453
iteration 7 batch 8870 trainingloss 0.6931471805599453
iteration 7 batch 8880 trainingloss 0.6931471805599453
iteration 7 batch 8890 trainingloss 0.6931471805599453
iteration 7 batch 8900 trainingloss 0.6931471805599453
iteration 7 batch 8910 trainingloss 0.6931471805599453
iteration 7 batch 8920 trainingloss 0.6931471805599453
iteration 7 batch 8930 trainingloss 0.6931471805599453
iteration 7 batch 8940 trainingloss 0.6931471805599453
iteration 7 batch 8950 trainingloss 0.6931471805599453
iteration 7 batch 8960 trainingloss 0.6931471805599453
iteration 7 batch 8970 trainingloss 0.6931471805599453
iteration 7 batch 8980 trainingloss 0.6931471805599453
iteration 7 batch 8990 trainingloss 0.6931471805599453
iteration 7 batch 9000 trainingloss 0.6931471805599453
iteration 7 batch 9010 trainingloss 0.6931471805599453
iteration 7 batch 9020 trainingloss 0.6931471805599453
iteration 7 batch 9030 trainingloss 0.6931471805599453
iteration 7 batch 9040 trainingloss 0.6931471805599453
iteration 7 batch 9050 trainingloss 0.6931471805599453
iteration 7 batch 9060 trainingloss 0.6931471805599453
iteration 7 batch 9070 trainingloss 0.6931471805599453
iteration 7 batch 9080 trainingloss 0.6931471805599453
iteration 7 batch 9090 trainingloss 0.6931471805599453
iteration 7 batch 9100 trainingloss 0.6931471805599453
iteration 7 batch 9110 trainingloss 0.6931471805599453
iteration 7 batch 9120 trainingloss 0.6931471805599453
iteration 7 batch 9130 trainingloss 0.6931471805599453
iteration 7 batch 9140 trainingloss 0.6931471805599453
iteration 7 batch 9150 trainingloss 0.6931471805599453
iteration 7 batch 9160 trainingloss 0.6931471805599453
iteration 7 batch 9170 trainingloss 0.6931471805599453
iteration 7 batch 9180 trainingloss 0.6931471805599453
iteration 7 batch 9190 trainingloss 0.6916632528527511
iteration 7 batch 9200 trainingloss 0.6931471805599453
iteration 7 batch 9210 trainingloss 0.6931471805599453
iteration 7 batch 9220 trainingloss 0.6916632528527511
iteration 7 batch 9230 trainingloss 0.6916632528527511
iteration 7 batch 9240 trainingloss 0.6931471805599453
iteration 7 batch 9250 trainingloss 0.6931471805599453
iteration 7 batch 9260 trainingloss 0.6931471805599453
iteration 7 batch 9270 trainingloss 0.6931471805599453
iteration 7 batch 9280 trainingloss 0.6931471805599453
iteration 7 batch 9290 trainingloss 0.6931471805599453
iteration 7 batch 9300 trainingloss 0.6931471805599453
iteration 7 batch 9310 trainingloss 0.6931471805599453
iteration 7 batch 9320 trainingloss 0.6931471805599453
iteration 7 batch 9330 trainingloss 0.6931471805599453
iteration 7 batch 9340 trainingloss 0.6931471805599453
iteration 7 batch 9350 trainingloss 0.6931471805599453
iteration 7 batch 9360 trainingloss 0.6931471805599453
iteration 7 batch 9370 trainingloss 0.6916632528527511
iteration 7 batch 9380 trainingloss 0.6931471805599453
iteration 7 batch 9390 trainingloss 0.6931471805599453
iteration 7 batch 9400 trainingloss 0.6931471805599453
iteration 7 batch 9410 trainingloss 0.6931471805599453
iteration 7 batch 9420 trainingloss 0.6931471805599453
iteration 7 batch 9430 trainingloss 0.6931471805599453
iteration 7 batch 9440 trainingloss 0.6916632528527511
iteration 7 batch 9450 trainingloss 0.6931471805599453
iteration 7 batch 9460 trainingloss 0.6931471805599453
iteration 7 batch 9470 trainingloss 0.6931471805599453
iteration 7 batch 9480 trainingloss 0.6931471805599453
iteration 7 batch 9490 trainingloss 0.6931471805599453
iteration 7 batch 9500 trainingloss 0.6931471805599453
iteration 7 batch 9510 trainingloss 0.6931471805599453
iteration 7 batch 9520 trainingloss 0.6931471805599453
iteration 7 batch 9530 trainingloss 0.6931471805599453
iteration 7 batch 9540 trainingloss 0.6931471805599453
iteration 7 batch 9550 trainingloss 0.6916632528527511
iteration 7 batch 9560 trainingloss 0.6931471805599453
iteration 7 batch 9570 trainingloss 0.6916632528527511
iteration 7 batch 9580 trainingloss 0.6931471805599453
iteration 7 batch 9590 trainingloss 0.6931471805599453
iteration 7 batch 9600 trainingloss 0.6931471805599453
iteration 7 batch 9610 trainingloss 0.6931471805599453
iteration 7 batch 9620 trainingloss 0.6931471805599453
iteration 7 batch 9630 trainingloss 0.6931471805599453
iteration 7 batch 9640 trainingloss 0.6931471805599453
iteration 7 batch 9650 trainingloss 0.6931471805599453
iteration 7 batch 9660 trainingloss 0.6931471805599453
iteration 7 batch 9670 trainingloss 0.6931471805599453
iteration 7 batch 9680 trainingloss 0.6931471805599453
iteration 7 batch 9690 trainingloss 0.6916632528527511
iteration 7 batch 9700 trainingloss 0.6931471805599453
iteration 7 batch 9710 trainingloss 0.6931471805599453
iteration 7 batch 9720 trainingloss 0.6931471805599453
iteration 7 batch 9730 trainingloss 0.6931471805599453
iteration 7 batch 9740 trainingloss 0.6931471805599453
iteration 7 batch 9750 trainingloss 0.6931471805599453
iteration 7 batch 9760 trainingloss 0.6931471805599453
iteration 7 batch 9770 trainingloss 0.6931471805599453
iteration 7 batch 9780 trainingloss 0.6931471805599453
iteration 7 batch 9790 trainingloss 0.6931471805599453
iteration 7 batch 9800 trainingloss 0.6931471805599453
iteration 7 batch 9810 trainingloss 0.6931471805599453
iteration 7 batch 9820 trainingloss 0.6931471805599453
iteration 7 batch 9830 trainingloss 0.6931471805599453
iteration 7 batch 9840 trainingloss 0.6931471805599453
iteration 7 batch 9850 trainingloss 0.6931471805599453
iteration 7 batch 9860 trainingloss 0.6931471805599453
iteration 7 batch 9870 trainingloss 0.6931471805599453
iteration 7 batch 9880 trainingloss 0.6916632528527511
iteration 7 batch 9890 trainingloss 0.6931471805599453
iteration 7 batch 9900 trainingloss 0.6931471805599453
iteration 7 batch 9910 trainingloss 0.6931471805599453
iteration 7 batch 9920 trainingloss 0.6931471805599453
iteration 7 batch 9930 trainingloss 0.6931471805599453
iteration 7 batch 9940 trainingloss 0.6931471805599453
iteration 7 batch 9950 trainingloss 0.6931471805599453
iteration 7 batch 9960 trainingloss 0.6931471805599453
iteration 7 batch 9970 trainingloss 0.6916632528527511
iteration 7 batch 9980 trainingloss 0.6931471805599453
iteration 7 batch 9990 trainingloss 0.6931471805599453
iteration 7 batch 10000 trainingloss 0.6931471805599453
iteration 7 batch 10010 trainingloss 0.6931471805599453
iteration 7 batch 10020 trainingloss 0.6931471805599453
iteration 7 batch 10030 trainingloss 0.6931471805599453
iteration 7 batch 10040 trainingloss 0.6931471805599453
iteration 7 batch 10050 trainingloss 0.6931471805599453
iteration 7 batch 10060 trainingloss 0.6931471805599453
iteration 7 batch 10070 trainingloss 0.6931471805599453
iteration 7 batch 10080 trainingloss 0.6931471805599453
iteration 7 batch 10090 trainingloss 0.6931471805599453
iteration 7 batch 10100 trainingloss 0.6931471805599453
iteration 7 batch 10110 trainingloss 0.6931471805599453
iteration 7 batch 10120 trainingloss 0.6931471805599453
iteration 7 batch 10130 trainingloss 0.6931471805599453
iteration 7 batch 10140 trainingloss 0.6931471805599453
iteration 7 batch 10150 trainingloss 0.6931471805599453
iteration 7 batch 10160 trainingloss 0.6931471805599453
iteration 7 batch 10170 trainingloss 0.6931471805599453
iteration 7 batch 10180 trainingloss 0.6931471805599453
iteration 7 batch 10190 trainingloss 0.6931471805599453
iteration 7 batch 10200 trainingloss 0.6931471805599453
iteration 7 batch 10210 trainingloss 0.6931471805599453
iteration 7 batch 10220 trainingloss 0.6916632528527511
iteration 7 batch 10230 trainingloss 0.6916632528527511
iteration 7 batch 10240 trainingloss 0.6931471805599453
iteration 7 batch 10250 trainingloss 0.6931471805599453
iteration 7 batch 10260 trainingloss 0.6931471805599453
iteration 7 batch 10270 trainingloss 0.6931471805599453
iteration 7 batch 10280 trainingloss 0.6931471805599453
iteration 7 batch 10290 trainingloss 0.6931471805599453
iteration 7 batch 10300 trainingloss 0.6931471805599453
iteration 7 batch 10310 trainingloss 0.6931471805599453
iteration 7 batch 10320 trainingloss 0.6931471805599453
iteration 7 batch 10330 trainingloss 0.6931471805599453
iteration 7 batch 10340 trainingloss 0.6931471805599453
iteration 7 batch 10350 trainingloss 0.6916632528527511
iteration 7 batch 10360 trainingloss 0.6931471805599453
iteration 7 batch 10370 trainingloss 0.6931471805599453
iteration 7 batch 10380 trainingloss 0.6931471805599453
iteration 7 batch 10390 trainingloss 0.6931471805599453
iteration 7 batch 10400 trainingloss 0.6931471805599453
iteration 7 batch 10410 trainingloss 0.6931471805599453
iteration 7 batch 10420 trainingloss 0.6931471805599453
iteration 7 batch 10430 trainingloss 0.6931471805599453
iteration 7 batch 10440 trainingloss 0.6931471805599453
iteration 7 batch 10450 trainingloss 0.6931471805599453
iteration 7 batch 10460 trainingloss 0.6931471805599453
iteration 7 batch 10470 trainingloss 0.6931471805599453
iteration 7 batch 10480 trainingloss 0.6931471805599453
iteration 7 batch 10490 trainingloss 0.6931471805599453
iteration 7 batch 10500 trainingloss 0.6931471805599453
iteration 7 batch 10510 trainingloss 0.6931471805599453
iteration 7 batch 10520 trainingloss 0.6931471805599453
iteration 7 batch 10530 trainingloss 0.6931471805599453
iteration 7 batch 10540 trainingloss 0.6931471805599453
iteration 7 batch 10550 trainingloss 0.6931471805599453
iteration 7 batch 10560 trainingloss 0.6931471805599453
iteration 7 batch 10570 trainingloss 0.6931471805599453
iteration 7 batch 10580 trainingloss 0.6931471805599453
iteration 7 batch 10590 trainingloss 0.6931471805599453
iteration 7 batch 10600 trainingloss 0.6931471805599453
iteration 7 batch 10610 trainingloss 0.6931471805599453
iteration 7 batch 10620 trainingloss 0.6931471805599453
iteration 7 batch 10630 trainingloss 0.6931471805599453
iteration 7 batch 10640 trainingloss 0.6931471805599453
iteration 7 batch 10650 trainingloss 0.6931471805599453
iteration 7 batch 10660 trainingloss 0.6916632528527511
iteration 7 batch 10670 trainingloss 0.6931471805599453
iteration 7 batch 10680 trainingloss 0.6931471805599453
iteration 7 batch 10690 trainingloss 0.6931471805599453
iteration 7 batch 10700 trainingloss 0.6931471805599453
iteration 7 batch 10710 trainingloss 0.6931471805599453
iteration 7 batch 10720 trainingloss 0.6916632528527511
iteration 7 batch 10730 trainingloss 0.6931471805599453
iteration 7 batch 10740 trainingloss 0.6931471805599453
iteration 7 batch 10750 trainingloss 0.6931471805599453
iteration 7 batch 10760 trainingloss 0.6931471805599453
iteration 7 batch 10770 trainingloss 0.6931471805599453
iteration 7 batch 10780 trainingloss 0.6931471805599453
iteration 7 batch 10790 trainingloss 0.6931471805599453
iteration 7 batch 10800 trainingloss 0.6931471805599453
iteration 7 batch 10810 trainingloss 0.6931471805599453
iteration 7 batch 10820 trainingloss 0.6931471805599453
iteration 7 batch 10830 trainingloss 0.6931471805599453
iteration 7 batch 10840 trainingloss 0.6931471805599453
iteration 7 batch 10850 trainingloss 0.6931471805599453
iteration 7 batch 10860 trainingloss 0.6916632528527511
iteration 7 batch 10870 trainingloss 0.6931471805599453
iteration 7 batch 10880 trainingloss 0.6931471805599453
iteration 7 batch 10890 trainingloss 0.6931471805599453
iteration 7 batch 10900 trainingloss 0.6931471805599453
iteration 7 batch 10910 trainingloss 0.6931471805599453
iteration 7 batch 10920 trainingloss 0.6931471805599453
iteration 7 batch 10930 trainingloss 0.6931471805599453
iteration 7 batch 10940 trainingloss 0.6931471805599453
iteration 7 batch 10950 trainingloss 0.6931471805599453
iteration 7 batch 10960 trainingloss 0.6931471805599453
iteration 7 batch 10970 trainingloss 0.6931471805599453
iteration 7 batch 10980 trainingloss 0.6916632528527511
iteration 7 batch 10990 trainingloss 0.6931471805599453
iteration 7 batch 11000 trainingloss 0.6931471805599453
iteration 7 batch 11010 trainingloss 0.6931471805599453
iteration 7 batch 11020 trainingloss 0.6931471805599453
iteration 7 batch 11030 trainingloss 0.6931471805599453
iteration 7 batch 11040 trainingloss 0.6931471805599453
iteration 7 batch 11050 trainingloss 0.6931471805599453
iteration 7 batch 11060 trainingloss 0.6931471805599453
iteration 7 batch 11070 trainingloss 0.6931471805599453
iteration 7 batch 11080 trainingloss 0.6916632528527511
iteration 7 batch 11090 trainingloss 0.6931471805599453
iteration 7 batch 11100 trainingloss 0.6931471805599453
iteration 7 batch 11110 trainingloss 0.6931471805599453
iteration 7 batch 11120 trainingloss 0.6931471805599453
iteration 7 batch 11130 trainingloss 0.6931471805599453
iteration 7 batch 11140 trainingloss 0.6931471805599453
iteration 7 batch 11150 trainingloss 0.6931471805599453
iteration 7 batch 11160 trainingloss 0.6931471805599453
iteration 7 batch 11170 trainingloss 0.6931471805599453
iteration 7 batch 11180 trainingloss 0.6916632528527511
iteration 7 batch 11190 trainingloss 0.6931471805599453
iteration 7 batch 11200 trainingloss 0.6931471805599453
iteration 7 batch 11210 trainingloss 0.6931471805599453
iteration 7 batch 11220 trainingloss 0.6931471805599453
iteration 7 batch 11230 trainingloss 0.6931471805599453
iteration 7 batch 11240 trainingloss 0.6931471805599453
iteration 7 batch 11250 trainingloss 0.6931471805599453
iteration 7 batch 11260 trainingloss 0.6931471805599453
iteration 7 batch 11270 trainingloss 0.6931471805599453
iteration 7 batch 11280 trainingloss 0.6931471805599453
iteration 7 batch 11290 trainingloss 0.6931471805599453
iteration 7 batch 11300 trainingloss 0.6931471805599453
iteration 7 batch 11310 trainingloss 0.6931471805599453
iteration 7 batch 11320 trainingloss 0.6931471805599453
iteration 7 batch 11330 trainingloss 0.6931471805599453
iteration 7 batch 11340 trainingloss 0.6931471805599453
iteration 7 batch 11350 trainingloss 0.6931471805599453
iteration 7 batch 11360 trainingloss 0.6931471805599453
iteration 7 batch 11370 trainingloss 0.6931471805599453
iteration 7 batch 11380 trainingloss 0.6931471805599453
iteration 7 batch 11390 trainingloss 0.6916632528527511
iteration 7 batch 11400 trainingloss 0.6931471805599453
iteration 7 batch 11410 trainingloss 0.6931471805599453
iteration 7 batch 11420 trainingloss 0.6931471805599453
iteration 7 batch 11430 trainingloss 0.6931471805599453
iteration 7 batch 11440 trainingloss 0.6931471805599453
iteration 7 batch 11450 trainingloss 0.6931471805599453
iteration 7 batch 11460 trainingloss 0.6931471805599453
iteration 7 batch 11470 trainingloss 0.6931471805599453
iteration 7 batch 11480 trainingloss 0.6931471805599453
iteration 7 batch 11490 trainingloss 0.6931471805599453
iteration 7 batch 11500 trainingloss 0.6931471805599453
iteration 7 batch 11510 trainingloss 0.6931471805599453
iteration 7 batch 11520 trainingloss 0.6931471805599453
iteration 7 batch 11530 trainingloss 0.6931471805599453
iteration 7 batch 11540 trainingloss 0.6931471805599453
iteration 7 batch 11550 trainingloss 0.6931471805599453
iteration 7 batch 11560 trainingloss 0.6931471805599453
iteration 7 batch 11570 trainingloss 0.6931471805599453
iteration 7 batch 11580 trainingloss 0.6931471805599453
iteration 7 batch 11590 trainingloss 0.6931471805599453
iteration 7 batch 11600 trainingloss 0.6931471805599453
iteration 7 batch 11610 trainingloss 0.6931471805599453
iteration 7 batch 11620 trainingloss 0.6931471805599453
iteration 7 batch 11630 trainingloss 0.6931471805599453
iteration 7 batch 11640 trainingloss 0.6931471805599453
iteration 7 batch 11650 trainingloss 0.6931471805599453
iteration 7 batch 11660 trainingloss 0.6931471805599453
iteration 7 batch 11670 trainingloss 0.6931471805599453
iteration 7 batch 11680 trainingloss 0.6931471805599453
iteration 7 batch 11690 trainingloss 0.6931471805599453
iteration 7 batch 11700 trainingloss 0.6931471805599453
iteration 7 batch 11710 trainingloss 0.6931471805599453
iteration 7 batch 11720 trainingloss 0.6931471805599453
iteration 7 batch 11730 trainingloss 0.6916632528527511
iteration 7 batch 11740 trainingloss 0.6931471805599453
iteration 7 batch 11750 trainingloss 0.6916632528527511
iteration 7 batch 11760 trainingloss 0.6931471805599453
iteration 7 batch 11770 trainingloss 0.6931471805599453
iteration 7 batch 11780 trainingloss 0.6916632528527511
iteration 7 batch 11790 trainingloss 0.6931471805599453
iteration 7 batch 11800 trainingloss 0.6931471805599453
iteration 7 batch 11810 trainingloss 0.6931471805599453
iteration 7 batch 11820 trainingloss 0.6931471805599453
iteration 7 batch 11830 trainingloss 0.6931471805599453
iteration 7 batch 11840 trainingloss 0.6931471805599453
iteration 7 batch 11850 trainingloss 0.6931471805599453
iteration 7 batch 11860 trainingloss 0.6931471805599453
iteration 7 batch 11870 trainingloss 0.6931471805599453
iteration 7 batch 11880 trainingloss 0.6931471805599453
iteration 7 batch 11890 trainingloss 0.6931471805599453
iteration 7 batch 11900 trainingloss 0.6931471805599453
iteration 7 batch 11910 trainingloss 0.6931471805599453
iteration 7 batch 11920 trainingloss 0.6931471805599453
iteration 7 batch 11930 trainingloss 0.6931471805599453
iteration 7 batch 11940 trainingloss 0.6931471805599453
iteration 7 batch 11950 trainingloss 0.6931471805599453
iteration 7 batch 11960 trainingloss 0.6931471805599453
iteration 7 batch 11970 trainingloss 0.6916632528527511
iteration 7 batch 11980 trainingloss 0.6931471805599453
iteration 7 batch 11990 trainingloss 0.6931471805599453
iteration 7 batch 12000 trainingloss 0.6931471805599453
iteration 7 batch 12010 trainingloss 0.6916632528527511
iteration 7 batch 12020 trainingloss 0.6931471805599453
iteration 7 batch 12030 trainingloss 0.6931471805599453
iteration 7 batch 12040 trainingloss 0.6931471805599453
iteration 7 batch 12050 trainingloss 0.6931471805599453
iteration 7 batch 12060 trainingloss 0.6931471805599453
iteration 7 batch 12070 trainingloss 0.6931471805599453
iteration 7 batch 12080 trainingloss 0.6931471805599453
iteration 7 batch 12090 trainingloss 0.6931471805599453
iteration 7 batch 12100 trainingloss 0.6931471805599453
iteration 7 batch 12110 trainingloss 0.6931471805599453
iteration 7 batch 12120 trainingloss 0.6931471805599453
iteration 7 batch 12130 trainingloss 0.6931471805599453
iteration 7 batch 12140 trainingloss 0.6931471805599453
iteration 7 batch 12150 trainingloss 0.6931471805599453
iteration 7 batch 12160 trainingloss 0.6931471805599453
iteration 7 batch 12170 trainingloss 0.6931471805599453
iteration 7 batch 12180 trainingloss 0.6931471805599453
iteration 7 batch 12190 trainingloss 0.6931471805599453
iteration 7 batch 12200 trainingloss 0.6931471805599453
iteration 7 batch 12210 trainingloss 0.6931471805599453
iteration 7 batch 12220 trainingloss 0.6931471805599453
iteration 7 batch 12230 trainingloss 0.6931471805599453
iteration 7 batch 12240 trainingloss 0.6931471805599453
iteration 7 batch 12250 trainingloss 0.6931471805599453
iteration 7 batch 12260 trainingloss 0.6931471805599453
iteration 7 batch 12270 trainingloss 0.6931471805599453
iteration 7 batch 12280 trainingloss 0.6931471805599453
iteration 7 batch 12290 trainingloss 0.6931471805599453
iteration 7 batch 12300 trainingloss 0.6931471805599453
iteration 7 batch 12310 trainingloss 0.6931471805599453
iteration 7 batch 12320 trainingloss 0.6931471805599453
iteration 7 batch 12330 trainingloss 0.6931471805599453
iteration 7 batch 12340 trainingloss 0.6931471805599453
iteration 7 batch 12350 trainingloss 0.6931471805599453
iteration 7 batch 12360 trainingloss 0.6931471805599453
iteration 7 batch 12370 trainingloss 0.6931471805599453
iteration 7 batch 12380 trainingloss 0.6931471805599453
iteration 7 batch 12390 trainingloss 0.6931471805599453
iteration 7 batch 12400 trainingloss 0.6931471805599453
iteration 7 batch 12410 trainingloss 0.6916632528527511
iteration 7 batch 12420 trainingloss 0.6931471805599453
iteration 7 batch 12430 trainingloss 0.6931471805599453
iteration 7 batch 12440 trainingloss 0.6931471805599453
iteration 7 batch 12450 trainingloss 0.6931471805599453
iteration 7 batch 12460 trainingloss 0.6931471805599453
iteration 7 batch 12470 trainingloss 0.6931471805599453
iteration 7 batch 12480 trainingloss 0.6931471805599453
iteration 7 batch 12490 trainingloss 0.6931471805599453
iteration 7 batch 12500 trainingloss 0.6931471805599453
iteration 7 batch 12510 trainingloss 0.6931471805599453
iteration 7 batch 12520 trainingloss 0.6931471805599453
iteration 7 batch 12530 trainingloss 0.6931471805599453
iteration 7 batch 12540 trainingloss 0.6931471805599453
iteration 7 batch 12550 trainingloss 0.6931471805599453
iteration 7 batch 12560 trainingloss 0.6931471805599453
iteration 7 batch 12570 trainingloss 0.6931471805599453
iteration 7 batch 12580 trainingloss 0.6931471805599453
iteration 7 batch 12590 trainingloss 0.6931471805599453
iteration 7 batch 12600 trainingloss 0.6931471805599453
iteration 7 batch 12610 trainingloss 0.6931471805599453
iteration 7 batch 12620 trainingloss 0.6931471805599453
iteration 7 batch 12630 trainingloss 0.6931471805599453
iteration 7 batch 12640 trainingloss 0.6931471805599453
iteration 7 batch 12650 trainingloss 0.6931471805599453
iteration 7 batch 12660 trainingloss 0.6931471805599453
iteration 7 batch 12670 trainingloss 0.6931471805599453
iteration 7 batch 12680 trainingloss 0.6916632528527511
iteration 7 batch 12690 trainingloss 0.6931471805599453
iteration 7 batch 12700 trainingloss 0.6931471805599453
iteration 7 batch 12710 trainingloss 0.6931471805599453
iteration 7 batch 12720 trainingloss 0.6931471805599453
iteration 7 batch 12730 trainingloss 0.6931471805599453
iteration 7 batch 12740 trainingloss 0.6931471805599453
iteration 7 batch 12750 trainingloss 0.6931471805599453
iteration 7 batch 12760 trainingloss 0.6931471805599453
iteration 7 batch 12770 trainingloss 0.6931471805599453
iteration 7 batch 12780 trainingloss 0.6931471805599453
iteration 7 batch 12790 trainingloss 0.6931471805599453
iteration 7 batch 12800 trainingloss 0.6931471805599453
iteration 7 batch 12810 trainingloss 0.6931471805599453
iteration 7 batch 12820 trainingloss 0.6931471805599453
iteration 7 batch 12830 trainingloss 0.6931471805599453
iteration 7 batch 12840 trainingloss 0.6931471805599453
iteration 7 batch 12850 trainingloss 0.6931471805599453
iteration 7 batch 12860 trainingloss 0.6931471805599453
iteration 7 batch 12870 trainingloss 0.6931471805599453
iteration 7 batch 12880 trainingloss 0.6931471805599453
iteration 7 batch 12890 trainingloss 0.6931471805599453
iteration 7 batch 12900 trainingloss 0.6931471805599453
iteration 7 batch 12910 trainingloss 0.6931471805599453
iteration 7 batch 12920 trainingloss 0.6931471805599453
iteration 7 batch 12930 trainingloss 0.6931471805599453
iteration 7 batch 12940 trainingloss 0.6931471805599453
iteration 7 batch 12950 trainingloss 0.6916632528527511
iteration 7 batch 12960 trainingloss 0.6931471805599453
iteration 7 batch 12970 trainingloss 0.6931471805599453
iteration 7 batch 12980 trainingloss 0.6931471805599453
iteration 7 batch 12990 trainingloss 0.6931471805599453
iteration 7 batch 13000 trainingloss 0.6931471805599453
iteration 7 batch 13010 trainingloss 0.6931471805599453
iteration 7 batch 13020 trainingloss 0.6931471805599453
iteration 7 batch 13030 trainingloss 0.6931471805599453
iteration 7 batch 13040 trainingloss 0.6931471805599453
iteration 7 batch 13050 trainingloss 0.6931471805599453
iteration 7 batch 13060 trainingloss 0.6931471805599453
iteration 7 batch 13070 trainingloss 0.6931471805599453
iteration 7 batch 13080 trainingloss 0.6931471805599453
iteration 7 batch 13090 trainingloss 0.6931471805599453
iteration 7 batch 13100 trainingloss 0.6931471805599453
iteration 7 batch 13110 trainingloss 0.6931471805599453
iteration 7 batch 13120 trainingloss 0.6931471805599453
iteration 7 batch 13130 trainingloss 0.6931471805599453
iteration 7 batch 13140 trainingloss 0.6931471805599453
iteration 7 batch 13150 trainingloss 0.6931471805599453
iteration 7 batch 13160 trainingloss 0.6931471805599453
iteration 7 batch 13170 trainingloss 0.6931471805599453
iteration 7 batch 13180 trainingloss 0.6931471805599453
iteration 7 batch 13190 trainingloss 0.6931471805599453
iteration 7 batch 13200 trainingloss 0.6931471805599453
iteration 7 batch 13210 trainingloss 0.6901793251455568
iteration 7 batch 13220 trainingloss 0.6931471805599453
iteration 7 batch 13230 trainingloss 0.6931471805599453
iteration 7 batch 13240 trainingloss 0.6931471805599453
iteration 7 batch 13250 trainingloss 0.6931471805599453
iteration 7 batch 13260 trainingloss 0.6931471805599453
iteration 7 batch 13270 trainingloss 0.6931471805599453
iteration 7 batch 13280 trainingloss 0.6931471805599453
iteration 7 batch 13290 trainingloss 0.6931471805599453
iteration 7 batch 13300 trainingloss 0.6931471805599453
iteration 7 batch 13310 trainingloss 0.6931471805599453
iteration 7 batch 13320 trainingloss 0.6916632528527511
iteration 7 batch 13330 trainingloss 0.6931471805599453
iteration 7 batch 13340 trainingloss 0.6931471805599453
iteration 7 batch 13350 trainingloss 0.6931471805599453
iteration 7 batch 13360 trainingloss 0.6931471805599453
iteration 7 batch 13370 trainingloss 0.6931471805599453
iteration 7 batch 13380 trainingloss 0.6931471805599453
iteration 7 batch 13390 trainingloss 0.6931471805599453
iteration 7 batch 13400 trainingloss 0.6931471805599453
iteration 7 batch 13410 trainingloss 0.6931471805599453
iteration 7 batch 13420 trainingloss 0.6931471805599453
iteration 7 batch 13430 trainingloss 0.6931471805599453
iteration 7 batch 13440 trainingloss 0.6916632528527511
iteration 7 batch 13450 trainingloss 0.6931471805599453
iteration 7 batch 13460 trainingloss 0.6931471805599453
iteration 7 batch 13470 trainingloss 0.6931471805599453
iteration 7 batch 13480 trainingloss 0.6931471805599453
iteration 7 batch 13490 trainingloss 0.6931471805599453
iteration 7 batch 13500 trainingloss 0.6931471805599453
iteration 7 batch 13510 trainingloss 0.6931471805599453
iteration 7 batch 13520 trainingloss 0.6931471805599453
iteration 7 batch 13530 trainingloss 0.6931471805599453
iteration 7 batch 13540 trainingloss 0.6931471805599453
iteration 7 batch 13550 trainingloss 0.6931471805599453
iteration 7 batch 13560 trainingloss 0.6931471805599453
iteration 7 batch 13570 trainingloss 0.6931471805599453
iteration 7 batch 13580 trainingloss 0.6931471805599453
iteration 7 batch 13590 trainingloss 0.6931471805599453
iteration 7 batch 13600 trainingloss 0.6931471805599453
iteration 7 batch 13610 trainingloss 0.6931471805599453
iteration 7 batch 13620 trainingloss 0.6916632528527511
iteration 7 batch 13630 trainingloss 0.6931471805599453
iteration 7 batch 13640 trainingloss 0.6931471805599453
iteration 7 batch 13650 trainingloss 0.6931471805599453
iteration 7 batch 13660 trainingloss 0.6931471805599453
iteration 7 batch 13670 trainingloss 0.6931471805599453
iteration 7 batch 13680 trainingloss 0.6931471805599453
iteration 7 batch 13690 trainingloss 0.6931471805599453
iteration 7 batch 13700 trainingloss 0.6931471805599453
iteration 7 batch 13710 trainingloss 0.6931471805599453
iteration 7 batch 13720 trainingloss 0.6931471805599453
iteration 7 batch 13730 trainingloss 0.6931471805599453
iteration 7 batch 13740 trainingloss 0.6931471805599453
iteration 7 batch 13750 trainingloss 0.6931471805599453
iteration 7 batch 13760 trainingloss 0.6931471805599453
iteration 7 batch 13770 trainingloss 0.6931471805599453
iteration 7 batch 13780 trainingloss 0.6931471805599453
iteration 7 batch 13790 trainingloss 0.6931471805599453
iteration 7 batch 13800 trainingloss 0.6931471805599453
iteration 7 batch 13810 trainingloss 0.6931471805599453
iteration 7 batch 13820 trainingloss 0.6931471805599453
iteration 7 batch 13830 trainingloss 0.6916632528527511
iteration 7 batch 13840 trainingloss 0.6931471805599453
iteration 7 batch 13850 trainingloss 0.6916632528527511
iteration 7 batch 13860 trainingloss 0.6931471805599453
iteration 7 batch 13870 trainingloss 0.6916632528527511
iteration 7 batch 13880 trainingloss 0.6931471805599453
iteration 7 batch 13890 trainingloss 0.6931471805599453
iteration 7 batch 13900 trainingloss 0.6931471805599453
iteration 7 batch 13910 trainingloss 0.6931471805599453
iteration 7 batch 13920 trainingloss 0.6931471805599453
iteration 7 batch 13930 trainingloss 0.6916632528527511
iteration 7 batch 13940 trainingloss 0.6931471805599453
iteration 7 batch 13950 trainingloss 0.6931471805599453
iteration 7 batch 13960 trainingloss 0.6931471805599453
iteration 7 batch 13970 trainingloss 0.6931471805599453
iteration 7 batch 13980 trainingloss 0.6931471805599453
iteration 7 batch 13990 trainingloss 0.6931471805599453
iteration 7 batch 14000 trainingloss 0.6931471805599453
iteration 7 batch 14010 trainingloss 0.6931471805599453
iteration 7 batch 14020 trainingloss 0.6931471805599453
iteration 7 batch 14030 trainingloss 0.6931471805599453
iteration 7 batch 14040 trainingloss 0.6931471805599453
iteration 7 batch 14050 trainingloss 0.6916632528527511
iteration 7 batch 14060 trainingloss 0.6916632528527511
iteration 7 batch 14070 trainingloss 0.6931471805599453
iteration 7 batch 14080 trainingloss 0.6931471805599453
iteration 7 batch 14090 trainingloss 0.6931471805599453
iteration 7 batch 14100 trainingloss 0.6931471805599453
iteration 7 batch 14110 trainingloss 0.6931471805599453
iteration 7 batch 14120 trainingloss 0.6931471805599453
iteration 7 batch 14130 trainingloss 0.6931471805599453
iteration 7 batch 14140 trainingloss 0.6931471805599453
iteration 7 batch 14150 trainingloss 0.6931471805599453
iteration 7 batch 14160 trainingloss 0.6931471805599453
iteration 7 batch 14170 trainingloss 0.6931471805599453
iteration 7 batch 14180 trainingloss 0.6931471805599453
iteration 7 batch 14190 trainingloss 0.6931471805599453
iteration 7 batch 14200 trainingloss 0.6931471805599453
iteration 7 batch 14210 trainingloss 0.6931471805599453
iteration 7 batch 14220 trainingloss 0.6931471805599453
iteration 7 batch 14230 trainingloss 0.6931471805599453
iteration 7 batch 14240 trainingloss 0.6931471805599453
iteration 7 batch 14250 trainingloss 0.6931471805599453
iteration 7 batch 14260 trainingloss 0.6931471805599453
iteration 7 batch 14270 trainingloss 0.6931471805599453
iteration 7 batch 14280 trainingloss 0.6931471805599453
iteration 7 batch 14290 trainingloss 0.6931471805599453
iteration 7 batch 14300 trainingloss 0.6931471805599453
iteration 7 batch 14310 trainingloss 0.6931471805599453
iteration 7 batch 14320 trainingloss 0.6931471805599453
iteration 7 batch 14330 trainingloss 0.6931471805599453
iteration 7 batch 14340 trainingloss 0.6931471805599453
iteration 7 batch 14350 trainingloss 0.6931471805599453
iteration 7 batch 14360 trainingloss 0.6931471805599453
iteration 7 batch 14370 trainingloss 0.6931471805599453
iteration 7 batch 14380 trainingloss 0.6931471805599453
iteration 7 batch 14390 trainingloss 0.6931471805599453
iteration 7 batch 14400 trainingloss 0.6931471805599453
iteration 7 batch 14410 trainingloss 0.6931471805599453
iteration 7 batch 14420 trainingloss 0.6931471805599453
iteration 7 batch 14430 trainingloss 0.6931471805599453
iteration 7 batch 14440 trainingloss 0.6931471805599453
iteration 7 batch 14450 trainingloss 0.6931471805599453
iteration 7 batch 14460 trainingloss 0.6931471805599453
iteration 7 batch 14470 trainingloss 0.6931471805599453
iteration 7 batch 14480 trainingloss 0.6931471805599453
iteration 7 batch 14490 trainingloss 0.6931471805599453
iteration 7 batch 14500 trainingloss 0.6931471805599453
iteration 7 batch 14510 trainingloss 0.6931471805599453
iteration 7 batch 14520 trainingloss 0.6931471805599453
iteration 7 batch 14530 trainingloss 0.6931471805599453
iteration 7 batch 14540 trainingloss 0.6931471805599453
iteration 7 batch 14550 trainingloss 0.6931471805599453
iteration 7 batch 14560 trainingloss 0.6931471805599453
iteration 7 batch 14570 trainingloss 0.6931471805599453
iteration 7 batch 14580 trainingloss 0.6931471805599453
iteration 7 batch 14590 trainingloss 0.6931471805599453
iteration 7 batch 14600 trainingloss 0.6931471805599453
iteration 7 batch 14610 trainingloss 0.6931471805599453
iteration 7 batch 14620 trainingloss 0.6931471805599453
iteration 7 batch 14630 trainingloss 0.6931471805599453
iteration 7 batch 14640 trainingloss 0.6931471805599453
iteration 7 batch 14650 trainingloss 0.6931471805599453
iteration 7 batch 14660 trainingloss 0.6931471805599453
iteration 7 batch 14670 trainingloss 0.6931471805599453
iteration 7 batch 14680 trainingloss 0.6931471805599453
iteration 7 batch 14690 trainingloss 0.6931471805599453
iteration 7 batch 14700 trainingloss 0.6931471805599453
iteration 7 batch 14710 trainingloss 0.6931471805599453
iteration 7 batch 14720 trainingloss 0.6931471805599453
iteration 7 batch 14730 trainingloss 0.6931471805599453
iteration 7 batch 14740 trainingloss 0.6931471805599453
iteration 7 batch 14750 trainingloss 0.6931471805599453
iteration 7 batch 14760 trainingloss 0.6931471805599453
iteration 7 batch 14770 trainingloss 0.6931471805599453
iteration 7 batch 14780 trainingloss 0.6931471805599453
iteration 7 batch 14790 trainingloss 0.6931471805599453
iteration 7 batch 14800 trainingloss 0.6931471805599453
iteration 7 batch 14810 trainingloss 0.6931471805599453
iteration 7 batch 14820 trainingloss 0.6931471805599453
iteration 7 batch 14830 trainingloss 0.6931471805599453
iteration 7 batch 14840 trainingloss 0.6931471805599453
iteration 7 batch 14850 trainingloss 0.6931471805599453
iteration 7 batch 14860 trainingloss 0.6931471805599453
iteration 7 batch 14870 trainingloss 0.6931471805599453
iteration 7 batch 14880 trainingloss 0.6931471805599453
iteration 7 batch 14890 trainingloss 0.6931471805599453
iteration 7 batch 14900 trainingloss 0.6931471805599453
iteration 7 batch 14910 trainingloss 0.6931471805599453
iteration 7 batch 14920 trainingloss 0.6931471805599453
iteration 7 batch 14930 trainingloss 0.6916632528527511
iteration 7 batch 14940 trainingloss 0.6931471805599453
iteration 7 batch 14950 trainingloss 0.6931471805599453
iteration 7 batch 14960 trainingloss 0.6931471805599453
iteration 7 batch 14970 trainingloss 0.6931471805599453
iteration 7 batch 14980 trainingloss 0.6916632528527511
iteration 7 batch 14990 trainingloss 0.6931471805599453
iteration 7 batch 15000 trainingloss 0.6931471805599453
iteration 7 batch 15010 trainingloss 0.6931471805599453
iteration 7 batch 15020 trainingloss 0.6931471805599453
iteration 7 batch 15030 trainingloss 0.6931471805599453
iteration 7 batch 15040 trainingloss 0.6931471805599453
iteration 7 batch 15050 trainingloss 0.6931471805599453
iteration 7 batch 15060 trainingloss 0.6931471805599453
iteration 7 batch 15070 trainingloss 0.6931471805599453
iteration 7 batch 15080 trainingloss 0.6931471805599453
iteration 7 batch 15090 trainingloss 0.6931471805599453
iteration 7 batch 15100 trainingloss 0.6916632528527511
iteration 7 batch 15110 trainingloss 0.6931471805599453
iteration 7 batch 15120 trainingloss 0.6931471805599453
iteration 7 batch 15130 trainingloss 0.6931471805599453
iteration 7 batch 15140 trainingloss 0.6931471805599453
iteration 7 batch 15150 trainingloss 0.6916632528527511
iteration 7 batch 15160 trainingloss 0.6931471805599453
iteration 7 batch 15170 trainingloss 0.6931471805599453
iteration 7 batch 15180 trainingloss 0.6931471805599453
iteration 7 batch 15190 trainingloss 0.6931471805599453
iteration 7 batch 15200 trainingloss 0.6931471805599453
iteration 7 batch 15210 trainingloss 0.6931471805599453
iteration 7 batch 15220 trainingloss 0.6931471805599453
iteration 7 batch 15230 trainingloss 0.6931471805599453
iteration 7 batch 15240 trainingloss 0.6931471805599453
iteration 7 batch 15250 trainingloss 0.6931471805599453
iteration 7 batch 15260 trainingloss 0.6931471805599453
iteration 7 batch 15270 trainingloss 0.6931471805599453
iteration 7 batch 15280 trainingloss 0.6931471805599453
iteration 7 batch 15290 trainingloss 0.6931471805599453
iteration 7 batch 15300 trainingloss 0.6931471805599453
iteration 7 batch 15310 trainingloss 0.6931471805599453
iteration 7 batch 15320 trainingloss 0.6931471805599453
iteration 7 batch 15330 trainingloss 0.6931471805599453
iteration 7 batch 15340 trainingloss 0.6931471805599453
iteration 7 batch 15350 trainingloss 0.6931471805599453
iteration 7 batch 15360 trainingloss 0.6931471805599453
iteration 7 batch 15370 trainingloss 0.6916632528527511
iteration 7 batch 15380 trainingloss 0.6931471805599453
iteration 7 batch 15390 trainingloss 0.6931471805599453
iteration 7 batch 15400 trainingloss 0.6931471805599453
iteration 7 batch 15410 trainingloss 0.6931471805599453
iteration 7 batch 15420 trainingloss 0.6931471805599453
iteration 7 batch 15430 trainingloss 0.6931471805599453
iteration 7 batch 15440 trainingloss 0.6931471805599453
iteration 7 batch 15450 trainingloss 0.6931471805599453
iteration 7 batch 15460 trainingloss 0.6931471805599453
iteration 7 batch 15470 trainingloss 0.6931471805599453
iteration 7 batch 15480 trainingloss 0.6931471805599453
iteration 7 batch 15490 trainingloss 0.6931471805599453
iteration 7 batch 15500 trainingloss 0.6931471805599453
iteration 7 batch 15510 trainingloss 0.6931471805599453
iteration 7 batch 15520 trainingloss 0.6931471805599453
iteration 7 batch 15530 trainingloss 0.6931471805599453
iteration 7 batch 15540 trainingloss 0.6931471805599453
iteration 7 batch 15550 trainingloss 0.6931471805599453
iteration 7 batch 15560 trainingloss 0.6931471805599453
iteration 7 batch 15570 trainingloss 0.6931471805599453
iteration 7 batch 15580 trainingloss 0.6931471805599453
iteration 7 batch 15590 trainingloss 0.6931471805599453
iteration 7 batch 15600 trainingloss 0.6931471805599453
iteration 7 batch 15610 trainingloss 0.6931471805599453
iteration 7 batch 15620 trainingloss 0.6931471805599453
iteration 7 batch 15630 trainingloss 0.6931471805599453
iteration 7 batch 15640 trainingloss 0.6931471805599453
iteration 7 batch 15650 trainingloss 0.6931471805599453
iteration 7 batch 15660 trainingloss 0.6931471805599453
iteration 7 batch 15670 trainingloss 0.6916632528527511
iteration 7 batch 15680 trainingloss 0.6931471805599453
iteration 7 batch 15690 trainingloss 0.6931471805599453
iteration 7 batch 15700 trainingloss 0.6931471805599453
iteration 7 batch 15710 trainingloss 0.6931471805599453
iteration 7 batch 15720 trainingloss 0.6931471805599453
iteration 7 batch 15730 trainingloss 0.6931471805599453
iteration 7 batch 15740 trainingloss 0.6931471805599453
iteration 7 batch 15750 trainingloss 0.6931471805599453
iteration 7 batch 15760 trainingloss 0.6931471805599453
iteration 7 batch 15770 trainingloss 0.6931471805599453
iteration 7 batch 15780 trainingloss 0.6931471805599453
iteration 7 batch 15790 trainingloss 0.6931471805599453
iteration 7 batch 15800 trainingloss 0.6931471805599453
iteration 7 batch 15810 trainingloss 0.6931471805599453
iteration 7 batch 15820 trainingloss 0.6931471805599453
iteration 7 batch 15830 trainingloss 0.6931471805599453
iteration 7 batch 15840 trainingloss 0.6931471805599453
iteration 7 batch 15850 trainingloss 0.6931471805599453
iteration 7 batch 15860 trainingloss 0.6931471805599453
iteration 7 batch 15870 trainingloss 0.6931471805599453
iteration 7 batch 15880 trainingloss 0.6931471805599453
iteration 7 batch 15890 trainingloss 0.6931471805599453
iteration 7 batch 15900 trainingloss 0.6931471805599453
iteration 7 batch 15910 trainingloss 0.6931471805599453
iteration 7 batch 15920 trainingloss 0.6931471805599453
iteration 7 batch 15930 trainingloss 0.6931471805599453
iteration 7 batch 15940 trainingloss 0.6931471805599453
iteration 7 batch 15950 trainingloss 0.6931471805599453
iteration 7 batch 15960 trainingloss 0.6931471805599453
iteration 7 batch 15970 trainingloss 0.6931471805599453
iteration 7 batch 15980 trainingloss 0.6931471805599453
iteration 7 batch 15990 trainingloss 0.6931471805599453
iteration 7 batch 16000 trainingloss 0.6931471805599453
iteration 7 batch 16010 trainingloss 0.6931471805599453
iteration 7 batch 16020 trainingloss 0.6931471805599453
iteration 7 batch 16030 trainingloss 0.6931471805599453
iteration 7 batch 16040 trainingloss 0.6931471805599453
iteration 7 batch 16050 trainingloss 0.6931471805599453
iteration 7 batch 16060 trainingloss 0.6931471805599453
iteration 7 batch 16070 trainingloss 0.6931471805599453
iteration 7 batch 16080 trainingloss 0.6931471805599453
iteration 7 batch 16090 trainingloss 0.6931471805599453
iteration 7 batch 16100 trainingloss 0.6931471805599453
iteration 7 batch 16110 trainingloss 0.6916632528527511
iteration 7 batch 16120 trainingloss 0.6916632528527511
iteration 7 batch 16130 trainingloss 0.6931471805599453
iteration 7 batch 16140 trainingloss 0.6931471805599453
iteration 7 batch 16150 trainingloss 0.6931471805599453
iteration 7 batch 16160 trainingloss 0.6931471805599453
iteration 7 batch 16170 trainingloss 0.6931471805599453
iteration 7 batch 16180 trainingloss 0.6931471805599453
iteration 7 batch 16190 trainingloss 0.6931471805599453
iteration 7 batch 16200 trainingloss 0.6931471805599453
iteration 7 batch 16210 trainingloss 0.6916632528527511
iteration 7 batch 16220 trainingloss 0.6931471805599453
iteration 7 batch 16230 trainingloss 0.6931471805599453
iteration 7 batch 16240 trainingloss 0.6931471805599453
iteration 7 batch 16250 trainingloss 0.6931471805599453
iteration 7 batch 16260 trainingloss 0.6931471805599453
iteration 7 batch 16270 trainingloss 0.6931471805599453
iteration 7 batch 16280 trainingloss 0.6931471805599453
iteration 7 batch 16290 trainingloss 0.6931471805599453
iteration 7 batch 16300 trainingloss 0.6931471805599453
iteration 7 batch 16310 trainingloss 0.6931471805599453
iteration 7 batch 16320 trainingloss 0.6931471805599453
iteration 7 batch 16330 trainingloss 0.6931471805599453
iteration 7 batch 16340 trainingloss 0.6931471805599453
iteration 7 batch 16350 trainingloss 0.6931471805599453
iteration 7 batch 16360 trainingloss 0.6931471805599453
iteration 7 batch 16370 trainingloss 0.6931471805599453
iteration 7 batch 16380 trainingloss 0.6931471805599453
iteration 7 batch 16390 trainingloss 0.6916632528527511
iteration 7 batch 16400 trainingloss 0.6931471805599453
iteration 7 batch 16410 trainingloss 0.6931471805599453
iteration 7 batch 16420 trainingloss 0.6931471805599453
iteration 7 batch 16430 trainingloss 0.6931471805599453
iteration 7 batch 16440 trainingloss 0.6931471805599453
iteration 7 batch 16450 trainingloss 0.6931471805599453
iteration 7 batch 16460 trainingloss 0.6931471805599453
iteration 7 batch 16470 trainingloss 0.6931471805599453
iteration 7 batch 16480 trainingloss 0.6931471805599453
iteration 7 batch 16490 trainingloss 0.6931471805599453
iteration 7 batch 16500 trainingloss 0.6931471805599453
iteration 7 batch 16510 trainingloss 0.6931471805599453
iteration 7 batch 16520 trainingloss 0.6931471805599453
iteration 7 batch 16530 trainingloss 0.6931471805599453
iteration 7 batch 16540 trainingloss 0.6931471805599453
iteration 7 batch 16550 trainingloss 0.6931471805599453
iteration 7 batch 16560 trainingloss 0.6931471805599453
iteration 7 batch 16570 trainingloss 0.6931471805599453
iteration 7 batch 16580 trainingloss 0.6931471805599453
iteration 7 batch 16590 trainingloss 0.6931471805599453
iteration 7 batch 16600 trainingloss 0.6931471805599453
iteration 7 batch 16610 trainingloss 0.6931471805599453
iteration 7 batch 16620 trainingloss 0.6931471805599453
iteration 7 batch 16630 trainingloss 0.6931471805599453
iteration 7 batch 16640 trainingloss 0.6916632528527511
iteration 7 batch 16650 trainingloss 0.6931471805599453
iteration 7 batch 16660 trainingloss 0.6931471805599453
iteration 7 batch 16670 trainingloss 0.6931471805599453
iteration 7 batch 16680 trainingloss 0.6931471805599453
iteration 7 batch 16690 trainingloss 0.6931471805599453
iteration 7 batch 16700 trainingloss 0.6931471805599453
iteration 7 batch 16710 trainingloss 0.6931471805599453
iteration 7 batch 16720 trainingloss 0.6931471805599453
iteration 7 batch 16730 trainingloss 0.6931471805599453
iteration 7 batch 16740 trainingloss 0.6931471805599453
iteration 7 batch 16750 trainingloss 0.6931471805599453
iteration 7 batch 16760 trainingloss 0.6931471805599453
iteration 7 batch 16770 trainingloss 0.6931471805599453
iteration 7 batch 16780 trainingloss 0.6931471805599453
iteration 7 batch 16790 trainingloss 0.6931471805599453
iteration 7 batch 16800 trainingloss 0.6931471805599453
iteration 7 batch 16810 trainingloss 0.6931471805599453
iteration 7 batch 16820 trainingloss 0.6931471805599453
iteration 7 batch 16830 trainingloss 0.6931471805599453
iteration 7 batch 16840 trainingloss 0.6931471805599453
iteration 7 batch 16850 trainingloss 0.6931471805599453
iteration 7 batch 16860 trainingloss 0.6931471805599453
iteration 7 batch 16870 trainingloss 0.6931471805599453
iteration 7 batch 16880 trainingloss 0.6931471805599453
iteration 7 batch 16890 trainingloss 0.6931471805599453
iteration 7 batch 16900 trainingloss 0.6916632528527511
iteration 7 batch 16910 trainingloss 0.6931471805599453
iteration 7 batch 16920 trainingloss 0.6931471805599453
iteration 7 batch 16930 trainingloss 0.6931471805599453
iteration 7 batch 16940 trainingloss 0.6931471805599453
iteration 7 batch 16950 trainingloss 0.6931471805599453
iteration 7 batch 16960 trainingloss 0.6931471805599453
iteration 7 batch 16970 trainingloss 0.6931471805599453
iteration 7 batch 16980 trainingloss 0.6931471805599453
iteration 7 batch 16990 trainingloss 0.6916632528527511
iteration 7 batch 17000 trainingloss 0.6931471805599453
iteration 7 batch 17010 trainingloss 0.6931471805599453
iteration 7 batch 17020 trainingloss 0.6931471805599453
iteration 7 batch 17030 trainingloss 0.6931471805599453
iteration 7 batch 17040 trainingloss 0.6931471805599453
iteration 7 batch 17050 trainingloss 0.6916632528527511
iteration 7 batch 17060 trainingloss 0.6931471805599453
iteration 7 batch 17070 trainingloss 0.6931471805599453
iteration 7 batch 17080 trainingloss 0.6931471805599453
iteration 7 batch 17090 trainingloss 0.6931471805599453
iteration 7 batch 17100 trainingloss 0.6931471805599453
iteration 7 batch 17110 trainingloss 0.6931471805599453
iteration 7 batch 17120 trainingloss 0.6931471805599453
iteration 7 batch 17130 trainingloss 0.6931471805599453
iteration 7 batch 17140 trainingloss 0.6931471805599453
iteration 7 batch 17150 trainingloss 0.6931471805599453
iteration 7 batch 17160 trainingloss 0.6931471805599453
iteration 7 batch 17170 trainingloss 0.6931471805599453
iteration 7 batch 17180 trainingloss 0.6931471805599453
iteration 7 batch 17190 trainingloss 0.6931471805599453
iteration 7 batch 17200 trainingloss 0.6931471805599453
iteration 7 batch 17210 trainingloss 0.6931471805599453
iteration 7 batch 17220 trainingloss 0.6931471805599453
iteration 7 batch 17230 trainingloss 0.6931471805599453
iteration 7 batch 17240 trainingloss 0.6931471805599453
iteration 7 batch 17250 trainingloss 0.6916632528527511
iteration 7 batch 17260 trainingloss 0.6931471805599453
iteration 7 batch 17270 trainingloss 0.6931471805599453
iteration 7 batch 17280 trainingloss 0.6931471805599453
iteration 7 batch 17290 trainingloss 0.6931471805599453
iteration 7 batch 17300 trainingloss 0.6931471805599453
iteration 7 batch 17310 trainingloss 0.6931471805599453
iteration 7 batch 17320 trainingloss 0.6931471805599453
iteration 7 batch 17330 trainingloss 0.6916632528527511
iteration 7 batch 17340 trainingloss 0.6931471805599453
iteration 7 batch 17350 trainingloss 0.6931471805599453
iteration 7 batch 17360 trainingloss 0.6931471805599453
iteration 7 batch 17370 trainingloss 0.6931471805599453
iteration 7 batch 17380 trainingloss 0.6931471805599453
iteration 7 batch 17390 trainingloss 0.6931471805599453
iteration 7 batch 17400 trainingloss 0.6931471805599453
iteration 7 batch 17410 trainingloss 0.6931471805599453
iteration 7 batch 17420 trainingloss 0.6931471805599453
iteration 7 batch 17430 trainingloss 0.6931471805599453
iteration 7 batch 17440 trainingloss 0.6931471805599453
iteration 7 batch 17450 trainingloss 0.6916632528527511
iteration 7 batch 17460 trainingloss 0.6931471805599453
iteration 7 batch 17470 trainingloss 0.6931471805599453
iteration 7 batch 17480 trainingloss 0.6931471805599453
iteration 7 batch 17490 trainingloss 0.6931471805599453
iteration 7 batch 17500 trainingloss 0.6916632528527511
iteration 7 batch 17510 trainingloss 0.6931471805599453
iteration 7 batch 17520 trainingloss 0.6931471805599453
iteration 7 batch 17530 trainingloss 0.6931471805599453
iteration 7 batch 17540 trainingloss 0.6931471805599453
iteration 7 batch 17550 trainingloss 0.6931471805599453
iteration 7 batch 17560 trainingloss 0.6931471805599453
iteration 7 batch 17570 trainingloss 0.6931471805599453
iteration 7 batch 17580 trainingloss 0.6931471805599453
iteration 7 batch 17590 trainingloss 0.6931471805599453
iteration 7 batch 17600 trainingloss 0.6931471805599453
iteration 7 batch 17610 trainingloss 0.6931471805599453
iteration 7 batch 17620 trainingloss 0.6931471805599453
iteration 7 batch 17630 trainingloss 0.6931471805599453
iteration 7 batch 17640 trainingloss 0.6931471805599453
iteration 7 batch 17650 trainingloss 0.6931471805599453
iteration 7 batch 17660 trainingloss 0.6931471805599453
iteration 7 batch 17670 trainingloss 0.6931471805599453
iteration 7 batch 17680 trainingloss 0.6931471805599453
iteration 7 batch 17690 trainingloss 0.6931471805599453
iteration 7 batch 17700 trainingloss 0.6931471805599453
iteration 7 batch 17710 trainingloss 0.6931471805599453
iteration 7 batch 17720 trainingloss 0.6931471805599453
iteration 7 batch 17730 trainingloss 0.6916632528527511
iteration 7 batch 17740 trainingloss 0.6931471805599453
iteration 7 batch 17750 trainingloss 0.6931471805599453
iteration 7 batch 17760 trainingloss 0.6931471805599453
iteration 7 batch 17770 trainingloss 0.6931471805599453
iteration 7 batch 17780 trainingloss 0.6931471805599453
iteration 7 batch 17790 trainingloss 0.6931471805599453
iteration 7 batch 17800 trainingloss 0.6931471805599453
iteration 7 batch 17810 trainingloss 0.6931471805599453
iteration 7 batch 17820 trainingloss 0.6931471805599453
iteration 7 batch 17830 trainingloss 0.6931471805599453
iteration 7 batch 17840 trainingloss 0.6931471805599453
iteration 7 batch 17850 trainingloss 0.6931471805599453
iteration 7 batch 17860 trainingloss 0.6931471805599453
iteration 7 batch 17870 trainingloss 0.6931471805599453
iteration 7 batch 17880 trainingloss 0.6931471805599453
iteration 7 batch 17890 trainingloss 0.6931471805599453
iteration 7 batch 17900 trainingloss 0.6931471805599453
iteration 7 batch 17910 trainingloss 0.6931471805599453
iteration 7 batch 17920 trainingloss 0.6931471805599453
iteration 7 batch 17930 trainingloss 0.6931471805599453
iteration 7 batch 17940 trainingloss 0.6931471805599453
iteration 7 batch 17950 trainingloss 0.6931471805599453
iteration 7 batch 17960 trainingloss 0.6931471805599453
iteration 7 batch 17970 trainingloss 0.6931471805599453
iteration 7 batch 17980 trainingloss 0.6931471805599453
iteration 7 batch 17990 trainingloss 0.6931471805599453
iteration 7 batch 18000 trainingloss 0.6931471805599453
iteration 7 batch 18010 trainingloss 0.6931471805599453
iteration 7 batch 18020 trainingloss 0.6931471805599453
iteration 7 batch 18030 trainingloss 0.6931471805599453
iteration 7 batch 18040 trainingloss 0.6931471805599453
iteration 7 batch 18050 trainingloss 0.6931471805599453
iteration 7 batch 18060 trainingloss 0.6931471805599453
iteration 7 batch 18070 trainingloss 0.6931471805599453
iteration 7 batch 18080 trainingloss 0.6931471805599453
iteration 7 batch 18090 trainingloss 0.6931471805599453
iteration 7 batch 18100 trainingloss 0.6931471805599453
iteration 7 batch 18110 trainingloss 0.6931471805599453
iteration 7 batch 18120 trainingloss 0.6931471805599453
iteration 7 batch 18130 trainingloss 0.6931471805599453
iteration 7 batch 18140 trainingloss 0.6931471805599453
iteration 7 batch 18150 trainingloss 0.6931471805599453
iteration 7 batch 18160 trainingloss 0.6931471805599453
iteration 7 batch 18170 trainingloss 0.6931471805599453
iteration 7 batch 18180 trainingloss 0.6916632528527511
iteration 7 batch 18190 trainingloss 0.6931471805599453
iteration 7 batch 18200 trainingloss 0.6931471805599453
iteration 7 batch 18210 trainingloss 0.6931471805599453
iteration 7 batch 18220 trainingloss 0.6931471805599453
iteration 7 batch 18230 trainingloss 0.6931471805599453
iteration 7 batch 18240 trainingloss 0.6931471805599453
iteration 7 batch 18250 trainingloss 0.6931471805599453
iteration 7 batch 18260 trainingloss 0.6931471805599453
iteration 7 batch 18270 trainingloss 0.6931471805599453
iteration 7 batch 18280 trainingloss 0.6931471805599453
iteration 7 batch 18290 trainingloss 0.6931471805599453
iteration 7 batch 18300 trainingloss 0.6931471805599453
iteration 7 batch 18310 trainingloss 0.6931471805599453
iteration 7 batch 18320 trainingloss 0.6931471805599453
iteration 7 batch 18330 trainingloss 0.6931471805599453
iteration 7 batch 18340 trainingloss 0.6931471805599453
iteration 7 batch 18350 trainingloss 0.6931471805599453
iteration 7 batch 18360 trainingloss 0.6931471805599453
iteration 7 batch 18370 trainingloss 0.6931471805599453
iteration 7 batch 18380 trainingloss 0.6931471805599453
iteration 7 batch 18390 trainingloss 0.6916632528527511
iteration 7 batch 18400 trainingloss 0.6931471805599453
iteration 7 batch 18410 trainingloss 0.6916632528527511
iteration 7 batch 18420 trainingloss 0.6931471805599453
iteration 7 batch 18430 trainingloss 0.6931471805599453
iteration 7 batch 18440 trainingloss 0.6931471805599453
iteration 7 batch 18450 trainingloss 0.6931471805599453
iteration 7 batch 18460 trainingloss 0.6931471805599453
iteration 7 batch 18470 trainingloss 0.6931471805599453
iteration 7 batch 18480 trainingloss 0.6931471805599453
iteration 7 batch 18490 trainingloss 0.6931471805599453
iteration 7 batch 18500 trainingloss 0.6931471805599453
iteration 7 batch 18510 trainingloss 0.6931471805599453
iteration 7 batch 18520 trainingloss 0.6931471805599453
iteration 7 batch 18530 trainingloss 0.6931471805599453
iteration 7 batch 18540 trainingloss 0.6931471805599453
iteration 7 batch 18550 trainingloss 0.6931471805599453
iteration 7 batch 18560 trainingloss 0.6931471805599453
iteration 7 batch 18570 trainingloss 0.6931471805599453
iteration 7 batch 18580 trainingloss 0.6931471805599453
iteration 7 batch 18590 trainingloss 0.6931471805599453
iteration 7 batch 18600 trainingloss 0.6931471805599453
iteration 7 batch 18610 trainingloss 0.6931471805599453
iteration 8 batch 0 trainingloss 0.6931471805599453
iteration 8 batch 10 trainingloss 0.6931471805599453
iteration 8 batch 20 trainingloss 0.6931471805599453
iteration 8 batch 30 trainingloss 0.6931471805599453
iteration 8 batch 40 trainingloss 0.6931471805599453
iteration 8 batch 50 trainingloss 0.6916632528527511
iteration 8 batch 60 trainingloss 0.6931471805599453
iteration 8 batch 70 trainingloss 0.6931471805599453
iteration 8 batch 80 trainingloss 0.6931471805599453
iteration 8 batch 90 trainingloss 0.6931471805599453
iteration 8 batch 100 trainingloss 0.6931471805599453
iteration 8 batch 110 trainingloss 0.6931471805599453
iteration 8 batch 120 trainingloss 0.6931471805599453
iteration 8 batch 130 trainingloss 0.6931471805599453
iteration 8 batch 140 trainingloss 0.6931471805599453
iteration 8 batch 150 trainingloss 0.6931471805599453
iteration 8 batch 160 trainingloss 0.6931471805599453
iteration 8 batch 170 trainingloss 0.6931471805599453
iteration 8 batch 180 trainingloss 0.6931471805599453
iteration 8 batch 190 trainingloss 0.6931471805599453
iteration 8 batch 200 trainingloss 0.6931471805599453
iteration 8 batch 210 trainingloss 0.6931471805599453
iteration 8 batch 220 trainingloss 0.6931471805599453
iteration 8 batch 230 trainingloss 0.6931471805599453
iteration 8 batch 240 trainingloss 0.6931471805599453
iteration 8 batch 250 trainingloss 0.6931471805599453
iteration 8 batch 260 trainingloss 0.6931471805599453
iteration 8 batch 270 trainingloss 0.6931471805599453
iteration 8 batch 280 trainingloss 0.6931471805599453
iteration 8 batch 290 trainingloss 0.6931471805599453
iteration 8 batch 300 trainingloss 0.6931471805599453
iteration 8 batch 310 trainingloss 0.6931471805599453
iteration 8 batch 320 trainingloss 0.6931471805599453
iteration 8 batch 330 trainingloss 0.6931471805599453
iteration 8 batch 340 trainingloss 0.6931471805599453
iteration 8 batch 350 trainingloss 0.6931471805599453
iteration 8 batch 360 trainingloss 0.6931471805599453
iteration 8 batch 370 trainingloss 0.6931471805599453
iteration 8 batch 380 trainingloss 0.6931471805599453
iteration 8 batch 390 trainingloss 0.6931471805599453
iteration 8 batch 400 trainingloss 0.6916632528527511
iteration 8 batch 410 trainingloss 0.6931471805599453
iteration 8 batch 420 trainingloss 0.6931471805599453
iteration 8 batch 430 trainingloss 0.6916632528527511
iteration 8 batch 440 trainingloss 0.6931471805599453
iteration 8 batch 450 trainingloss 0.6931471805599453
iteration 8 batch 460 trainingloss 0.6931471805599453
iteration 8 batch 470 trainingloss 0.6931471805599453
iteration 8 batch 480 trainingloss 0.6931471805599453
iteration 8 batch 490 trainingloss 0.6931471805599453
iteration 8 batch 500 trainingloss 0.6931471805599453
iteration 8 batch 510 trainingloss 0.6931471805599453
iteration 8 batch 520 trainingloss 0.6931471805599453
iteration 8 batch 530 trainingloss 0.6931471805599453
iteration 8 batch 540 trainingloss 0.6931471805599453
iteration 8 batch 550 trainingloss 0.6931471805599453
iteration 8 batch 560 trainingloss 0.6931471805599453
iteration 8 batch 570 trainingloss 0.6931471805599453
iteration 8 batch 580 trainingloss 0.6931471805599453
iteration 8 batch 590 trainingloss 0.6931471805599453
iteration 8 batch 600 trainingloss 0.6931471805599453
iteration 8 batch 610 trainingloss 0.6931471805599453
iteration 8 batch 620 trainingloss 0.6931471805599453
iteration 8 batch 630 trainingloss 0.6931471805599453
iteration 8 batch 640 trainingloss 0.6931471805599453
iteration 8 batch 650 trainingloss 0.6931471805599453
iteration 8 batch 660 trainingloss 0.6931471805599453
iteration 8 batch 670 trainingloss 0.6931471805599453
iteration 8 batch 680 trainingloss 0.6931471805599453
iteration 8 batch 690 trainingloss 0.6916632528527511
iteration 8 batch 700 trainingloss 0.6931471805599453
iteration 8 batch 710 trainingloss 0.6931471805599453
iteration 8 batch 720 trainingloss 0.6931471805599453
iteration 8 batch 730 trainingloss 0.6931471805599453
iteration 8 batch 740 trainingloss 0.6931471805599453
iteration 8 batch 750 trainingloss 0.6931471805599453
iteration 8 batch 760 trainingloss 0.6931471805599453
iteration 8 batch 770 trainingloss 0.6931471805599453
iteration 8 batch 780 trainingloss 0.6931471805599453
iteration 8 batch 790 trainingloss 0.6931471805599453
iteration 8 batch 800 trainingloss 0.6931471805599453
iteration 8 batch 810 trainingloss 0.6931471805599453
iteration 8 batch 820 trainingloss 0.6916632528527511
iteration 8 batch 830 trainingloss 0.6916632528527511
iteration 8 batch 840 trainingloss 0.6931471805599453
iteration 8 batch 850 trainingloss 0.6931471805599453
iteration 8 batch 860 trainingloss 0.6931471805599453
iteration 8 batch 870 trainingloss 0.6931471805599453
iteration 8 batch 880 trainingloss 0.6916632528527511
iteration 8 batch 890 trainingloss 0.6931471805599453
iteration 8 batch 900 trainingloss 0.6931471805599453
iteration 8 batch 910 trainingloss 0.6931471805599453
iteration 8 batch 920 trainingloss 0.6931471805599453
iteration 8 batch 930 trainingloss 0.6931471805599453
iteration 8 batch 940 trainingloss 0.6931471805599453
iteration 8 batch 950 trainingloss 0.6931471805599453
iteration 8 batch 960 trainingloss 0.6931471805599453
iteration 8 batch 970 trainingloss 0.6931471805599453
iteration 8 batch 980 trainingloss 0.6916632528527511
iteration 8 batch 990 trainingloss 0.6931471805599453
iteration 8 batch 1000 trainingloss 0.6931471805599453
iteration 8 batch 1010 trainingloss 0.6931471805599453
iteration 8 batch 1020 trainingloss 0.6931471805599453
iteration 8 batch 1030 trainingloss 0.6931471805599453
iteration 8 batch 1040 trainingloss 0.6931471805599453
iteration 8 batch 1050 trainingloss 0.6931471805599453
iteration 8 batch 1060 trainingloss 0.6931471805599453
iteration 8 batch 1070 trainingloss 0.6931471805599453
iteration 8 batch 1080 trainingloss 0.6931471805599453
iteration 8 batch 1090 trainingloss 0.6931471805599453
iteration 8 batch 1100 trainingloss 0.6931471805599453
iteration 8 batch 1110 trainingloss 0.6931471805599453
iteration 8 batch 1120 trainingloss 0.6931471805599453
iteration 8 batch 1130 trainingloss 0.6931471805599453
iteration 8 batch 1140 trainingloss 0.6931471805599453
iteration 8 batch 1150 trainingloss 0.6931471805599453
iteration 8 batch 1160 trainingloss 0.6931471805599453
iteration 8 batch 1170 trainingloss 0.6931471805599453
iteration 8 batch 1180 trainingloss 0.6931471805599453
iteration 8 batch 1190 trainingloss 0.6931471805599453
iteration 8 batch 1200 trainingloss 0.6931471805599453
iteration 8 batch 1210 trainingloss 0.6931471805599453
iteration 8 batch 1220 trainingloss 0.6931471805599453
iteration 8 batch 1230 trainingloss 0.6916632528527511
iteration 8 batch 1240 trainingloss 0.6931471805599453
iteration 8 batch 1250 trainingloss 0.6931471805599453
iteration 8 batch 1260 trainingloss 0.6931471805599453
iteration 8 batch 1270 trainingloss 0.6931471805599453
iteration 8 batch 1280 trainingloss 0.6931471805599453
iteration 8 batch 1290 trainingloss 0.6931471805599453
iteration 8 batch 1300 trainingloss 0.6931471805599453
iteration 8 batch 1310 trainingloss 0.6931471805599453
iteration 8 batch 1320 trainingloss 0.6931471805599453
iteration 8 batch 1330 trainingloss 0.6931471805599453
iteration 8 batch 1340 trainingloss 0.6931471805599453
iteration 8 batch 1350 trainingloss 0.6931471805599453
iteration 8 batch 1360 trainingloss 0.6931471805599453
iteration 8 batch 1370 trainingloss 0.6931471805599453
iteration 8 batch 1380 trainingloss 0.6931471805599453
iteration 8 batch 1390 trainingloss 0.6931471805599453
iteration 8 batch 1400 trainingloss 0.6931471805599453
iteration 8 batch 1410 trainingloss 0.6931471805599453
iteration 8 batch 1420 trainingloss 0.6931471805599453
iteration 8 batch 1430 trainingloss 0.6931471805599453
iteration 8 batch 1440 trainingloss 0.6931471805599453
iteration 8 batch 1450 trainingloss 0.6931471805599453
iteration 8 batch 1460 trainingloss 0.6931471805599453
iteration 8 batch 1470 trainingloss 0.6931471805599453
iteration 8 batch 1480 trainingloss 0.6931471805599453
iteration 8 batch 1490 trainingloss 0.6931471805599453
iteration 8 batch 1500 trainingloss 0.6931471805599453
iteration 8 batch 1510 trainingloss 0.6931471805599453
iteration 8 batch 1520 trainingloss 0.6931471805599453
iteration 8 batch 1530 trainingloss 0.6931471805599453
iteration 8 batch 1540 trainingloss 0.6931471805599453
iteration 8 batch 1550 trainingloss 0.6931471805599453
iteration 8 batch 1560 trainingloss 0.6931471805599453
iteration 8 batch 1570 trainingloss 0.6931471805599453
iteration 8 batch 1580 trainingloss 0.6931471805599453
iteration 8 batch 1590 trainingloss 0.6931471805599453
iteration 8 batch 1600 trainingloss 0.6931471805599453
iteration 8 batch 1610 trainingloss 0.6931471805599453
iteration 8 batch 1620 trainingloss 0.6931471805599453
iteration 8 batch 1630 trainingloss 0.6931471805599453
iteration 8 batch 1640 trainingloss 0.6931471805599453
iteration 8 batch 1650 trainingloss 0.6931471805599453
iteration 8 batch 1660 trainingloss 0.6931471805599453
iteration 8 batch 1670 trainingloss 0.6931471805599453
iteration 8 batch 1680 trainingloss 0.6931471805599453
iteration 8 batch 1690 trainingloss 0.6931471805599453
iteration 8 batch 1700 trainingloss 0.6931471805599453
iteration 8 batch 1710 trainingloss 0.6931471805599453
iteration 8 batch 1720 trainingloss 0.6931471805599453
iteration 8 batch 1730 trainingloss 0.6931471805599453
iteration 8 batch 1740 trainingloss 0.6916632528527511
iteration 8 batch 1750 trainingloss 0.6931471805599453
iteration 8 batch 1760 trainingloss 0.6931471805599453
iteration 8 batch 1770 trainingloss 0.6931471805599453
iteration 8 batch 1780 trainingloss 0.6931471805599453
iteration 8 batch 1790 trainingloss 0.6931471805599453
iteration 8 batch 1800 trainingloss 0.6931471805599453
iteration 8 batch 1810 trainingloss 0.6931471805599453
iteration 8 batch 1820 trainingloss 0.6931471805599453
iteration 8 batch 1830 trainingloss 0.6931471805599453
iteration 8 batch 1840 trainingloss 0.6931471805599453
iteration 8 batch 1850 trainingloss 0.6916632528527511
iteration 8 batch 1860 trainingloss 0.6931471805599453
iteration 8 batch 1870 trainingloss 0.6931471805599453
iteration 8 batch 1880 trainingloss 0.6931471805599453
iteration 8 batch 1890 trainingloss 0.6931471805599453
iteration 8 batch 1900 trainingloss 0.6931471805599453
iteration 8 batch 1910 trainingloss 0.6931471805599453
iteration 8 batch 1920 trainingloss 0.6931471805599453
iteration 8 batch 1930 trainingloss 0.6931471805599453
iteration 8 batch 1940 trainingloss 0.6931471805599453
iteration 8 batch 1950 trainingloss 0.6931471805599453
iteration 8 batch 1960 trainingloss 0.6931471805599453
iteration 8 batch 1970 trainingloss 0.6931471805599453
iteration 8 batch 1980 trainingloss 0.6931471805599453
iteration 8 batch 1990 trainingloss 0.6931471805599453
iteration 8 batch 2000 trainingloss 0.6931471805599453
iteration 8 batch 2010 trainingloss 0.6931471805599453
iteration 8 batch 2020 trainingloss 0.6931471805599453
iteration 8 batch 2030 trainingloss 0.6931471805599453
iteration 8 batch 2040 trainingloss 0.6931471805599453
iteration 8 batch 2050 trainingloss 0.6931471805599453
iteration 8 batch 2060 trainingloss 0.6931471805599453
iteration 8 batch 2070 trainingloss 0.6931471805599453
iteration 8 batch 2080 trainingloss 0.6931471805599453
iteration 8 batch 2090 trainingloss 0.6931471805599453
iteration 8 batch 2100 trainingloss 0.6931471805599453
iteration 8 batch 2110 trainingloss 0.6931471805599453
iteration 8 batch 2120 trainingloss 0.6931471805599453
iteration 8 batch 2130 trainingloss 0.6931471805599453
iteration 8 batch 2140 trainingloss 0.6931471805599453
iteration 8 batch 2150 trainingloss 0.6931471805599453
iteration 8 batch 2160 trainingloss 0.6931471805599453
iteration 8 batch 2170 trainingloss 0.6931471805599453
iteration 8 batch 2180 trainingloss 0.6931471805599453
iteration 8 batch 2190 trainingloss 0.6931471805599453
iteration 8 batch 2200 trainingloss 0.6931471805599453
iteration 8 batch 2210 trainingloss 0.6931471805599453
iteration 8 batch 2220 trainingloss 0.6931471805599453
iteration 8 batch 2230 trainingloss 0.6931471805599453
iteration 8 batch 2240 trainingloss 0.6931471805599453
iteration 8 batch 2250 trainingloss 0.6931471805599453
iteration 8 batch 2260 trainingloss 0.6931471805599453
iteration 8 batch 2270 trainingloss 0.6931471805599453
iteration 8 batch 2280 trainingloss 0.6931471805599453
iteration 8 batch 2290 trainingloss 0.6931471805599453
iteration 8 batch 2300 trainingloss 0.6931471805599453
iteration 8 batch 2310 trainingloss 0.6931471805599453
iteration 8 batch 2320 trainingloss 0.6931471805599453
iteration 8 batch 2330 trainingloss 0.6931471805599453
iteration 8 batch 2340 trainingloss 0.6931471805599453
iteration 8 batch 2350 trainingloss 0.6931471805599453
iteration 8 batch 2360 trainingloss 0.6931471805599453
iteration 8 batch 2370 trainingloss 0.6931471805599453
iteration 8 batch 2380 trainingloss 0.6931471805599453
iteration 8 batch 2390 trainingloss 0.6931471805599453
iteration 8 batch 2400 trainingloss 0.6931471805599453
iteration 8 batch 2410 trainingloss 0.6931471805599453
iteration 8 batch 2420 trainingloss 0.6931471805599453
iteration 8 batch 2430 trainingloss 0.6931471805599453
iteration 8 batch 2440 trainingloss 0.6931471805599453
iteration 8 batch 2450 trainingloss 0.6931471805599453
iteration 8 batch 2460 trainingloss 0.6931471805599453
iteration 8 batch 2470 trainingloss 0.6931471805599453
iteration 8 batch 2480 trainingloss 0.6931471805599453
iteration 8 batch 2490 trainingloss 0.6931471805599453
iteration 8 batch 2500 trainingloss 0.6931471805599453
iteration 8 batch 2510 trainingloss 0.6931471805599453
iteration 8 batch 2520 trainingloss 0.6931471805599453
iteration 8 batch 2530 trainingloss 0.6931471805599453
iteration 8 batch 2540 trainingloss 0.6931471805599453
iteration 8 batch 2550 trainingloss 0.6931471805599453
iteration 8 batch 2560 trainingloss 0.6931471805599453
iteration 8 batch 2570 trainingloss 0.6931471805599453
iteration 8 batch 2580 trainingloss 0.6931471805599453
iteration 8 batch 2590 trainingloss 0.6931471805599453
iteration 8 batch 2600 trainingloss 0.6931471805599453
iteration 8 batch 2610 trainingloss 0.6931471805599453
iteration 8 batch 2620 trainingloss 0.6931471805599453
iteration 8 batch 2630 trainingloss 0.6931471805599453
iteration 8 batch 2640 trainingloss 0.6931471805599453
iteration 8 batch 2650 trainingloss 0.6931471805599453
iteration 8 batch 2660 trainingloss 0.6931471805599453
iteration 8 batch 2670 trainingloss 0.6931471805599453
iteration 8 batch 2680 trainingloss 0.6931471805599453
iteration 8 batch 2690 trainingloss 0.6931471805599453
iteration 8 batch 2700 trainingloss 0.6931471805599453
iteration 8 batch 2710 trainingloss 0.6931471805599453
iteration 8 batch 2720 trainingloss 0.6931471805599453
iteration 8 batch 2730 trainingloss 0.6916632528527511
iteration 8 batch 2740 trainingloss 0.6931471805599453
iteration 8 batch 2750 trainingloss 0.6931471805599453
iteration 8 batch 2760 trainingloss 0.6931471805599453
iteration 8 batch 2770 trainingloss 0.6931471805599453
iteration 8 batch 2780 trainingloss 0.6931471805599453
iteration 8 batch 2790 trainingloss 0.6916632528527511
iteration 8 batch 2800 trainingloss 0.6931471805599453
iteration 8 batch 2810 trainingloss 0.6931471805599453
iteration 8 batch 2820 trainingloss 0.6931471805599453
iteration 8 batch 2830 trainingloss 0.6931471805599453
iteration 8 batch 2840 trainingloss 0.6931471805599453
iteration 8 batch 2850 trainingloss 0.6931471805599453
iteration 8 batch 2860 trainingloss 0.6931471805599453
iteration 8 batch 2870 trainingloss 0.6931471805599453
iteration 8 batch 2880 trainingloss 0.6931471805599453
iteration 8 batch 2890 trainingloss 0.6931471805599453
iteration 8 batch 2900 trainingloss 0.6931471805599453
iteration 8 batch 2910 trainingloss 0.6931471805599453
iteration 8 batch 2920 trainingloss 0.6916632528527511
iteration 8 batch 2930 trainingloss 0.6931471805599453
iteration 8 batch 2940 trainingloss 0.6931471805599453
iteration 8 batch 2950 trainingloss 0.6931471805599453
iteration 8 batch 2960 trainingloss 0.6931471805599453
iteration 8 batch 2970 trainingloss 0.6931471805599453
iteration 8 batch 2980 trainingloss 0.6931471805599453
iteration 8 batch 2990 trainingloss 0.6931471805599453
iteration 8 batch 3000 trainingloss 0.6931471805599453
iteration 8 batch 3010 trainingloss 0.6931471805599453
iteration 8 batch 3020 trainingloss 0.6931471805599453
iteration 8 batch 3030 trainingloss 0.6931471805599453
iteration 8 batch 3040 trainingloss 0.6931471805599453
iteration 8 batch 3050 trainingloss 0.6931471805599453
iteration 8 batch 3060 trainingloss 0.6931471805599453
iteration 8 batch 3070 trainingloss 0.6931471805599453
iteration 8 batch 3080 trainingloss 0.6931471805599453
iteration 8 batch 3090 trainingloss 0.6931471805599453
iteration 8 batch 3100 trainingloss 0.6931471805599453
iteration 8 batch 3110 trainingloss 0.6931471805599453
iteration 8 batch 3120 trainingloss 0.6931471805599453
iteration 8 batch 3130 trainingloss 0.6931471805599453
iteration 8 batch 3140 trainingloss 0.6931471805599453
iteration 8 batch 3150 trainingloss 0.6931471805599453
iteration 8 batch 3160 trainingloss 0.6931471805599453
iteration 8 batch 3170 trainingloss 0.6931471805599453
iteration 8 batch 3180 trainingloss 0.6931471805599453
iteration 8 batch 3190 trainingloss 0.6931471805599453
iteration 8 batch 3200 trainingloss 0.6931471805599453
iteration 8 batch 3210 trainingloss 0.6931471805599453
iteration 8 batch 3220 trainingloss 0.6931471805599453
iteration 8 batch 3230 trainingloss 0.6931471805599453
iteration 8 batch 3240 trainingloss 0.6916632528527511
iteration 8 batch 3250 trainingloss 0.6931471805599453
iteration 8 batch 3260 trainingloss 0.6931471805599453
iteration 8 batch 3270 trainingloss 0.6931471805599453
iteration 8 batch 3280 trainingloss 0.6931471805599453
iteration 8 batch 3290 trainingloss 0.6931471805599453
iteration 8 batch 3300 trainingloss 0.6931471805599453
iteration 8 batch 3310 trainingloss 0.6931471805599453
iteration 8 batch 3320 trainingloss 0.6931471805599453
iteration 8 batch 3330 trainingloss 0.6931471805599453
iteration 8 batch 3340 trainingloss 0.6931471805599453
iteration 8 batch 3350 trainingloss 0.6931471805599453
iteration 8 batch 3360 trainingloss 0.6931471805599453
iteration 8 batch 3370 trainingloss 0.6931471805599453
iteration 8 batch 3380 trainingloss 0.6931471805599453
iteration 8 batch 3390 trainingloss 0.6931471805599453
iteration 8 batch 3400 trainingloss 0.6931471805599453
iteration 8 batch 3410 trainingloss 0.6931471805599453
iteration 8 batch 3420 trainingloss 0.6931471805599453
iteration 8 batch 3430 trainingloss 0.6931471805599453
iteration 8 batch 3440 trainingloss 0.6931471805599453
iteration 8 batch 3450 trainingloss 0.6931471805599453
iteration 8 batch 3460 trainingloss 0.6931471805599453
iteration 8 batch 3470 trainingloss 0.6931471805599453
iteration 8 batch 3480 trainingloss 0.6931471805599453
iteration 8 batch 3490 trainingloss 0.6931471805599453
iteration 8 batch 3500 trainingloss 0.6931471805599453
iteration 8 batch 3510 trainingloss 0.6931471805599453
iteration 8 batch 3520 trainingloss 0.6931471805599453
iteration 8 batch 3530 trainingloss 0.6931471805599453
iteration 8 batch 3540 trainingloss 0.6931471805599453
iteration 8 batch 3550 trainingloss 0.6931471805599453
iteration 8 batch 3560 trainingloss 0.6931471805599453
iteration 8 batch 3570 trainingloss 0.6931471805599453
iteration 8 batch 3580 trainingloss 0.6931471805599453
iteration 8 batch 3590 trainingloss 0.6916632528527511
iteration 8 batch 3600 trainingloss 0.6931471805599453
iteration 8 batch 3610 trainingloss 0.6931471805599453
iteration 8 batch 3620 trainingloss 0.6931471805599453
iteration 8 batch 3630 trainingloss 0.6916632528527511
iteration 8 batch 3640 trainingloss 0.6931471805599453
iteration 8 batch 3650 trainingloss 0.6931471805599453
iteration 8 batch 3660 trainingloss 0.6931471805599453
iteration 8 batch 3670 trainingloss 0.6931471805599453
iteration 8 batch 3680 trainingloss 0.6931471805599453
iteration 8 batch 3690 trainingloss 0.6931471805599453
iteration 8 batch 3700 trainingloss 0.6931471805599453
iteration 8 batch 3710 trainingloss 0.6916632528527511
iteration 8 batch 3720 trainingloss 0.6931471805599453
iteration 8 batch 3730 trainingloss 0.6931471805599453
iteration 8 batch 3740 trainingloss 0.6931471805599453
iteration 8 batch 3750 trainingloss 0.6931471805599453
iteration 8 batch 3760 trainingloss 0.6916632528527511
iteration 8 batch 3770 trainingloss 0.6931471805599453
iteration 8 batch 3780 trainingloss 0.6931471805599453
iteration 8 batch 3790 trainingloss 0.6916632528527511
iteration 8 batch 3800 trainingloss 0.6931471805599453
iteration 8 batch 3810 trainingloss 0.6931471805599453
iteration 8 batch 3820 trainingloss 0.6931471805599453
iteration 8 batch 3830 trainingloss 0.6931471805599453
iteration 8 batch 3840 trainingloss 0.6931471805599453
iteration 8 batch 3850 trainingloss 0.6931471805599453
iteration 8 batch 3860 trainingloss 0.6931471805599453
iteration 8 batch 3870 trainingloss 0.6931471805599453
iteration 8 batch 3880 trainingloss 0.6931471805599453
iteration 8 batch 3890 trainingloss 0.6931471805599453
iteration 8 batch 3900 trainingloss 0.6931471805599453
iteration 8 batch 3910 trainingloss 0.6931471805599453
iteration 8 batch 3920 trainingloss 0.6931471805599453
iteration 8 batch 3930 trainingloss 0.6931471805599453
iteration 8 batch 3940 trainingloss 0.6931471805599453
iteration 8 batch 3950 trainingloss 0.6931471805599453
iteration 8 batch 3960 trainingloss 0.6931471805599453
iteration 8 batch 3970 trainingloss 0.6931471805599453
iteration 8 batch 3980 trainingloss 0.6931471805599453
iteration 8 batch 3990 trainingloss 0.6931471805599453
iteration 8 batch 4000 trainingloss 0.6931471805599453
iteration 8 batch 4010 trainingloss 0.6931471805599453
iteration 8 batch 4020 trainingloss 0.6931471805599453
iteration 8 batch 4030 trainingloss 0.6931471805599453
iteration 8 batch 4040 trainingloss 0.6931471805599453
iteration 8 batch 4050 trainingloss 0.6931471805599453
iteration 8 batch 4060 trainingloss 0.6931471805599453
iteration 8 batch 4070 trainingloss 0.6931471805599453
iteration 8 batch 4080 trainingloss 0.6931471805599453
iteration 8 batch 4090 trainingloss 0.6931471805599453
iteration 8 batch 4100 trainingloss 0.6931471805599453
iteration 8 batch 4110 trainingloss 0.6931471805599453
iteration 8 batch 4120 trainingloss 0.6931471805599453
iteration 8 batch 4130 trainingloss 0.6931471805599453
iteration 8 batch 4140 trainingloss 0.6931471805599453
iteration 8 batch 4150 trainingloss 0.6931471805599453
iteration 8 batch 4160 trainingloss 0.6931471805599453
iteration 8 batch 4170 trainingloss 0.6931471805599453
iteration 8 batch 4180 trainingloss 0.6931471805599453
iteration 8 batch 4190 trainingloss 0.6931471805599453
iteration 8 batch 4200 trainingloss 0.6931471805599453
iteration 8 batch 4210 trainingloss 0.6931471805599453
iteration 8 batch 4220 trainingloss 0.6931471805599453
iteration 8 batch 4230 trainingloss 0.6931471805599453
iteration 8 batch 4240 trainingloss 0.6931471805599453
iteration 8 batch 4250 trainingloss 0.6931471805599453
iteration 8 batch 4260 trainingloss 0.6931471805599453
iteration 8 batch 4270 trainingloss 0.6931471805599453
iteration 8 batch 4280 trainingloss 0.6931471805599453
iteration 8 batch 4290 trainingloss 0.6916632528527511
iteration 8 batch 4300 trainingloss 0.6931471805599453
iteration 8 batch 4310 trainingloss 0.6931471805599453
iteration 8 batch 4320 trainingloss 0.6931471805599453
iteration 8 batch 4330 trainingloss 0.6931471805599453
iteration 8 batch 4340 trainingloss 0.6931471805599453
iteration 8 batch 4350 trainingloss 0.6931471805599453
iteration 8 batch 4360 trainingloss 0.6931471805599453
iteration 8 batch 4370 trainingloss 0.6931471805599453
iteration 8 batch 4380 trainingloss 0.6931471805599453
iteration 8 batch 4390 trainingloss 0.6931471805599453
iteration 8 batch 4400 trainingloss 0.6931471805599453
iteration 8 batch 4410 trainingloss 0.6931471805599453
iteration 8 batch 4420 trainingloss 0.6931471805599453
iteration 8 batch 4430 trainingloss 0.6931471805599453
iteration 8 batch 4440 trainingloss 0.6931471805599453
iteration 8 batch 4450 trainingloss 0.6931471805599453
iteration 8 batch 4460 trainingloss 0.6931471805599453
iteration 8 batch 4470 trainingloss 0.6931471805599453
iteration 8 batch 4480 trainingloss 0.6931471805599453
iteration 8 batch 4490 trainingloss 0.6931471805599453
iteration 8 batch 4500 trainingloss 0.6931471805599453
iteration 8 batch 4510 trainingloss 0.6931471805599453
iteration 8 batch 4520 trainingloss 0.6931471805599453
iteration 8 batch 4530 trainingloss 0.6931471805599453
iteration 8 batch 4540 trainingloss 0.6931471805599453
iteration 8 batch 4550 trainingloss 0.6916632528527511
iteration 8 batch 4560 trainingloss 0.6931471805599453
iteration 8 batch 4570 trainingloss 0.6931471805599453
iteration 8 batch 4580 trainingloss 0.6931471805599453
iteration 8 batch 4590 trainingloss 0.6931471805599453
iteration 8 batch 4600 trainingloss 0.6931471805599453
iteration 8 batch 4610 trainingloss 0.6916632528527511
iteration 8 batch 4620 trainingloss 0.6931471805599453
iteration 8 batch 4630 trainingloss 0.6931471805599453
iteration 8 batch 4640 trainingloss 0.6931471805599453
iteration 8 batch 4650 trainingloss 0.6931471805599453
iteration 8 batch 4660 trainingloss 0.6931471805599453
iteration 8 batch 4670 trainingloss 0.6931471805599453
iteration 8 batch 4680 trainingloss 0.6931471805599453
iteration 8 batch 4690 trainingloss 0.6931471805599453
iteration 8 batch 4700 trainingloss 0.6931471805599453
iteration 8 batch 4710 trainingloss 0.6931471805599453
iteration 8 batch 4720 trainingloss 0.6931471805599453
iteration 8 batch 4730 trainingloss 0.6931471805599453
iteration 8 batch 4740 trainingloss 0.6931471805599453
iteration 8 batch 4750 trainingloss 0.6931471805599453
iteration 8 batch 4760 trainingloss 0.6931471805599453
iteration 8 batch 4770 trainingloss 0.6931471805599453
iteration 8 batch 4780 trainingloss 0.6931471805599453
iteration 8 batch 4790 trainingloss 0.6916632528527511
iteration 8 batch 4800 trainingloss 0.6931471805599453
iteration 8 batch 4810 trainingloss 0.6931471805599453
iteration 8 batch 4820 trainingloss 0.6931471805599453
iteration 8 batch 4830 trainingloss 0.6931471805599453
iteration 8 batch 4840 trainingloss 0.6931471805599453
iteration 8 batch 4850 trainingloss 0.6931471805599453
iteration 8 batch 4860 trainingloss 0.6931471805599453
iteration 8 batch 4870 trainingloss 0.6931471805599453
iteration 8 batch 4880 trainingloss 0.6931471805599453
iteration 8 batch 4890 trainingloss 0.6931471805599453
iteration 8 batch 4900 trainingloss 0.6931471805599453
iteration 8 batch 4910 trainingloss 0.6931471805599453
iteration 8 batch 4920 trainingloss 0.6931471805599453
iteration 8 batch 4930 trainingloss 0.6931471805599453
iteration 8 batch 4940 trainingloss 0.6931471805599453
iteration 8 batch 4950 trainingloss 0.6931471805599453
iteration 8 batch 4960 trainingloss 0.6931471805599453
iteration 8 batch 4970 trainingloss 0.6931471805599453
iteration 8 batch 4980 trainingloss 0.6931471805599453
iteration 8 batch 4990 trainingloss 0.6931471805599453
iteration 8 batch 5000 trainingloss 0.6931471805599453
iteration 8 batch 5010 trainingloss 0.6931471805599453
iteration 8 batch 5020 trainingloss 0.6931471805599453
iteration 8 batch 5030 trainingloss 0.6931471805599453
iteration 8 batch 5040 trainingloss 0.6931471805599453
iteration 8 batch 5050 trainingloss 0.6916632528527511
iteration 8 batch 5060 trainingloss 0.6931471805599453
iteration 8 batch 5070 trainingloss 0.6931471805599453
iteration 8 batch 5080 trainingloss 0.6931471805599453
iteration 8 batch 5090 trainingloss 0.6916632528527511
iteration 8 batch 5100 trainingloss 0.6931471805599453
iteration 8 batch 5110 trainingloss 0.6931471805599453
iteration 8 batch 5120 trainingloss 0.6931471805599453
iteration 8 batch 5130 trainingloss 0.6931471805599453
iteration 8 batch 5140 trainingloss 0.6931471805599453
iteration 8 batch 5150 trainingloss 0.6931471805599453
iteration 8 batch 5160 trainingloss 0.6931471805599453
iteration 8 batch 5170 trainingloss 0.6931471805599453
iteration 8 batch 5180 trainingloss 0.6931471805599453
iteration 8 batch 5190 trainingloss 0.6931471805599453
iteration 8 batch 5200 trainingloss 0.6931471805599453
iteration 8 batch 5210 trainingloss 0.6916632528527511
iteration 8 batch 5220 trainingloss 0.6931471805599453
iteration 8 batch 5230 trainingloss 0.6931471805599453
iteration 8 batch 5240 trainingloss 0.6931471805599453
iteration 8 batch 5250 trainingloss 0.6931471805599453
iteration 8 batch 5260 trainingloss 0.6916632528527511
iteration 8 batch 5270 trainingloss 0.6931471805599453
iteration 8 batch 5280 trainingloss 0.6931471805599453
iteration 8 batch 5290 trainingloss 0.6931471805599453
iteration 8 batch 5300 trainingloss 0.6931471805599453
iteration 8 batch 5310 trainingloss 0.6931471805599453
iteration 8 batch 5320 trainingloss 0.6931471805599453
iteration 8 batch 5330 trainingloss 0.6931471805599453
iteration 8 batch 5340 trainingloss 0.6931471805599453
iteration 8 batch 5350 trainingloss 0.6931471805599453
iteration 8 batch 5360 trainingloss 0.6931471805599453
iteration 8 batch 5370 trainingloss 0.6931471805599453
iteration 8 batch 5380 trainingloss 0.6931471805599453
iteration 8 batch 5390 trainingloss 0.6931471805599453
iteration 8 batch 5400 trainingloss 0.6931471805599453
iteration 8 batch 5410 trainingloss 0.6931471805599453
iteration 8 batch 5420 trainingloss 0.6931471805599453
iteration 8 batch 5430 trainingloss 0.6931471805599453
iteration 8 batch 5440 trainingloss 0.6931471805599453
iteration 8 batch 5450 trainingloss 0.6931471805599453
iteration 8 batch 5460 trainingloss 0.6931471805599453
iteration 8 batch 5470 trainingloss 0.6931471805599453
iteration 8 batch 5480 trainingloss 0.6931471805599453
iteration 8 batch 5490 trainingloss 0.6916632528527511
iteration 8 batch 5500 trainingloss 0.6931471805599453
iteration 8 batch 5510 trainingloss 0.6931471805599453
iteration 8 batch 5520 trainingloss 0.6931471805599453
iteration 8 batch 5530 trainingloss 0.6931471805599453
iteration 8 batch 5540 trainingloss 0.6931471805599453
iteration 8 batch 5550 trainingloss 0.6931471805599453
iteration 8 batch 5560 trainingloss 0.6931471805599453
iteration 8 batch 5570 trainingloss 0.6931471805599453
iteration 8 batch 5580 trainingloss 0.6916632528527511
iteration 8 batch 5590 trainingloss 0.6931471805599453
iteration 8 batch 5600 trainingloss 0.6931471805599453
iteration 8 batch 5610 trainingloss 0.6931471805599453
iteration 8 batch 5620 trainingloss 0.6931471805599453
iteration 8 batch 5630 trainingloss 0.6931471805599453
iteration 8 batch 5640 trainingloss 0.6931471805599453
iteration 8 batch 5650 trainingloss 0.6931471805599453
iteration 8 batch 5660 trainingloss 0.6931471805599453
iteration 8 batch 5670 trainingloss 0.6931471805599453
iteration 8 batch 5680 trainingloss 0.6931471805599453
iteration 8 batch 5690 trainingloss 0.6931471805599453
iteration 8 batch 5700 trainingloss 0.6931471805599453
iteration 8 batch 5710 trainingloss 0.6931471805599453
iteration 8 batch 5720 trainingloss 0.6931471805599453
iteration 8 batch 5730 trainingloss 0.6931471805599453
iteration 8 batch 5740 trainingloss 0.6931471805599453
iteration 8 batch 5750 trainingloss 0.6931471805599453
iteration 8 batch 5760 trainingloss 0.6931471805599453
iteration 8 batch 5770 trainingloss 0.6931471805599453
iteration 8 batch 5780 trainingloss 0.6916632528527511
iteration 8 batch 5790 trainingloss 0.6931471805599453
iteration 8 batch 5800 trainingloss 0.6931471805599453
iteration 8 batch 5810 trainingloss 0.6916632528527511
iteration 8 batch 5820 trainingloss 0.6931471805599453
iteration 8 batch 5830 trainingloss 0.6931471805599453
iteration 8 batch 5840 trainingloss 0.6931471805599453
iteration 8 batch 5850 trainingloss 0.6931471805599453
iteration 8 batch 5860 trainingloss 0.6931471805599453
iteration 8 batch 5870 trainingloss 0.6931471805599453
iteration 8 batch 5880 trainingloss 0.6931471805599453
iteration 8 batch 5890 trainingloss 0.6931471805599453
iteration 8 batch 5900 trainingloss 0.6931471805599453
iteration 8 batch 5910 trainingloss 0.6931471805599453
iteration 8 batch 5920 trainingloss 0.6931471805599453
iteration 8 batch 5930 trainingloss 0.6931471805599453
iteration 8 batch 5940 trainingloss 0.6931471805599453
iteration 8 batch 5950 trainingloss 0.6931471805599453
iteration 8 batch 5960 trainingloss 0.6931471805599453
iteration 8 batch 5970 trainingloss 0.6931471805599453
iteration 8 batch 5980 trainingloss 0.6931471805599453
iteration 8 batch 5990 trainingloss 0.6931471805599453
iteration 8 batch 6000 trainingloss 0.6931471805599453
iteration 8 batch 6010 trainingloss 0.6931471805599453
iteration 8 batch 6020 trainingloss 0.6931471805599453
iteration 8 batch 6030 trainingloss 0.6931471805599453
iteration 8 batch 6040 trainingloss 0.6931471805599453
iteration 8 batch 6050 trainingloss 0.6931471805599453
iteration 8 batch 6060 trainingloss 0.6931471805599453
iteration 8 batch 6070 trainingloss 0.6931471805599453
iteration 8 batch 6080 trainingloss 0.6931471805599453
iteration 8 batch 6090 trainingloss 0.6931471805599453
iteration 8 batch 6100 trainingloss 0.6931471805599453
iteration 8 batch 6110 trainingloss 0.6931471805599453
iteration 8 batch 6120 trainingloss 0.6931471805599453
iteration 8 batch 6130 trainingloss 0.6931471805599453
iteration 8 batch 6140 trainingloss 0.6931471805599453
iteration 8 batch 6150 trainingloss 0.6931471805599453
iteration 8 batch 6160 trainingloss 0.6931471805599453
iteration 8 batch 6170 trainingloss 0.6931471805599453
iteration 8 batch 6180 trainingloss 0.6916632528527511
iteration 8 batch 6190 trainingloss 0.6931471805599453
iteration 8 batch 6200 trainingloss 0.6931471805599453
iteration 8 batch 6210 trainingloss 0.6931471805599453
iteration 8 batch 6220 trainingloss 0.6931471805599453
iteration 8 batch 6230 trainingloss 0.6916632528527511
iteration 8 batch 6240 trainingloss 0.6916632528527511
iteration 8 batch 6250 trainingloss 0.6931471805599453
iteration 8 batch 6260 trainingloss 0.6931471805599453
iteration 8 batch 6270 trainingloss 0.6931471805599453
iteration 8 batch 6280 trainingloss 0.6931471805599453
iteration 8 batch 6290 trainingloss 0.6901793251455568
iteration 8 batch 6300 trainingloss 0.6931471805599453
iteration 8 batch 6310 trainingloss 0.6931471805599453
iteration 8 batch 6320 trainingloss 0.6931471805599453
iteration 8 batch 6330 trainingloss 0.6931471805599453
iteration 8 batch 6340 trainingloss 0.6931471805599453
iteration 8 batch 6350 trainingloss 0.6931471805599453
iteration 8 batch 6360 trainingloss 0.6931471805599453
iteration 8 batch 6370 trainingloss 0.6931471805599453
iteration 8 batch 6380 trainingloss 0.6931471805599453
iteration 8 batch 6390 trainingloss 0.6931471805599453
iteration 8 batch 6400 trainingloss 0.6931471805599453
iteration 8 batch 6410 trainingloss 0.6931471805599453
iteration 8 batch 6420 trainingloss 0.6931471805599453
iteration 8 batch 6430 trainingloss 0.6931471805599453
iteration 8 batch 6440 trainingloss 0.6931471805599453
iteration 8 batch 6450 trainingloss 0.6931471805599453
iteration 8 batch 6460 trainingloss 0.6931471805599453
iteration 8 batch 6470 trainingloss 0.6931471805599453
iteration 8 batch 6480 trainingloss 0.6931471805599453
iteration 8 batch 6490 trainingloss 0.6931471805599453
iteration 8 batch 6500 trainingloss 0.6916632528527511
iteration 8 batch 6510 trainingloss 0.6931471805599453
iteration 8 batch 6520 trainingloss 0.6931471805599453
iteration 8 batch 6530 trainingloss 0.6931471805599453
iteration 8 batch 6540 trainingloss 0.6931471805599453
iteration 8 batch 6550 trainingloss 0.6931471805599453
iteration 8 batch 6560 trainingloss 0.6931471805599453
iteration 8 batch 6570 trainingloss 0.6931471805599453
iteration 8 batch 6580 trainingloss 0.6931471805599453
iteration 8 batch 6590 trainingloss 0.6931471805599453
iteration 8 batch 6600 trainingloss 0.6931471805599453
iteration 8 batch 6610 trainingloss 0.6931471805599453
iteration 8 batch 6620 trainingloss 0.6931471805599453
iteration 8 batch 6630 trainingloss 0.6931471805599453
iteration 8 batch 6640 trainingloss 0.6931471805599453
iteration 8 batch 6650 trainingloss 0.6931471805599453
iteration 8 batch 6660 trainingloss 0.6931471805599453
iteration 8 batch 6670 trainingloss 0.6931471805599453
iteration 8 batch 6680 trainingloss 0.6931471805599453
iteration 8 batch 6690 trainingloss 0.6931471805599453
iteration 8 batch 6700 trainingloss 0.6931471805599453
iteration 8 batch 6710 trainingloss 0.6931471805599453
iteration 8 batch 6720 trainingloss 0.6931471805599453
iteration 8 batch 6730 trainingloss 0.6931471805599453
iteration 8 batch 6740 trainingloss 0.6931471805599453
iteration 8 batch 6750 trainingloss 0.6931471805599453
iteration 8 batch 6760 trainingloss 0.6931471805599453
iteration 8 batch 6770 trainingloss 0.6931471805599453
iteration 8 batch 6780 trainingloss 0.6931471805599453
iteration 8 batch 6790 trainingloss 0.6931471805599453
iteration 8 batch 6800 trainingloss 0.6931471805599453
iteration 8 batch 6810 trainingloss 0.6931471805599453
iteration 8 batch 6820 trainingloss 0.6931471805599453
iteration 8 batch 6830 trainingloss 0.6931471805599453
iteration 8 batch 6840 trainingloss 0.6931471805599453
iteration 8 batch 6850 trainingloss 0.6931471805599453
iteration 8 batch 6860 trainingloss 0.6931471805599453
iteration 8 batch 6870 trainingloss 0.6931471805599453
iteration 8 batch 6880 trainingloss 0.6931471805599453
iteration 8 batch 6890 trainingloss 0.6931471805599453
iteration 8 batch 6900 trainingloss 0.6931471805599453
iteration 8 batch 6910 trainingloss 0.6931471805599453
iteration 8 batch 6920 trainingloss 0.6931471805599453
iteration 8 batch 6930 trainingloss 0.6931471805599453
iteration 8 batch 6940 trainingloss 0.6931471805599453
iteration 8 batch 6950 trainingloss 0.6931471805599453
iteration 8 batch 6960 trainingloss 0.6931471805599453
iteration 8 batch 6970 trainingloss 0.6931471805599453
iteration 8 batch 6980 trainingloss 0.6931471805599453
iteration 8 batch 6990 trainingloss 0.6931471805599453
iteration 8 batch 7000 trainingloss 0.6931471805599453
iteration 8 batch 7010 trainingloss 0.6931471805599453
iteration 8 batch 7020 trainingloss 0.6931471805599453
iteration 8 batch 7030 trainingloss 0.6931471805599453
iteration 8 batch 7040 trainingloss 0.6931471805599453
iteration 8 batch 7050 trainingloss 0.6931471805599453
iteration 8 batch 7060 trainingloss 0.6931471805599453
iteration 8 batch 7070 trainingloss 0.6931471805599453
iteration 8 batch 7080 trainingloss 0.6931471805599453
iteration 8 batch 7090 trainingloss 0.6931471805599453
iteration 8 batch 7100 trainingloss 0.6931471805599453
iteration 8 batch 7110 trainingloss 0.6931471805599453
iteration 8 batch 7120 trainingloss 0.6931471805599453
iteration 8 batch 7130 trainingloss 0.6931471805599453
iteration 8 batch 7140 trainingloss 0.6931471805599453
iteration 8 batch 7150 trainingloss 0.6931471805599453
iteration 8 batch 7160 trainingloss 0.6931471805599453
iteration 8 batch 7170 trainingloss 0.6931471805599453
iteration 8 batch 7180 trainingloss 0.6931471805599453
iteration 8 batch 7190 trainingloss 0.6931471805599453
iteration 8 batch 7200 trainingloss 0.6931471805599453
iteration 8 batch 7210 trainingloss 0.6931471805599453
iteration 8 batch 7220 trainingloss 0.6931471805599453
iteration 8 batch 7230 trainingloss 0.6931471805599453
iteration 8 batch 7240 trainingloss 0.6931471805599453
iteration 8 batch 7250 trainingloss 0.6931471805599453
iteration 8 batch 7260 trainingloss 0.6931471805599453
iteration 8 batch 7270 trainingloss 0.6931471805599453
iteration 8 batch 7280 trainingloss 0.6931471805599453
iteration 8 batch 7290 trainingloss 0.6931471805599453
iteration 8 batch 7300 trainingloss 0.6931471805599453
iteration 8 batch 7310 trainingloss 0.6931471805599453
iteration 8 batch 7320 trainingloss 0.6931471805599453
iteration 8 batch 7330 trainingloss 0.6931471805599453
iteration 8 batch 7340 trainingloss 0.6931471805599453
iteration 8 batch 7350 trainingloss 0.6931471805599453
iteration 8 batch 7360 trainingloss 0.6931471805599453
iteration 8 batch 7370 trainingloss 0.6916632528527511
iteration 8 batch 7380 trainingloss 0.6931471805599453
iteration 8 batch 7390 trainingloss 0.6931471805599453
iteration 8 batch 7400 trainingloss 0.6931471805599453
iteration 8 batch 7410 trainingloss 0.6931471805599453
iteration 8 batch 7420 trainingloss 0.6931471805599453
iteration 8 batch 7430 trainingloss 0.6931471805599453
iteration 8 batch 7440 trainingloss 0.6931471805599453
iteration 8 batch 7450 trainingloss 0.6931471805599453
iteration 8 batch 7460 trainingloss 0.6916632528527511
iteration 8 batch 7470 trainingloss 0.6931471805599453
iteration 8 batch 7480 trainingloss 0.6931471805599453
iteration 8 batch 7490 trainingloss 0.6931471805599453
iteration 8 batch 7500 trainingloss 0.6931471805599453
iteration 8 batch 7510 trainingloss 0.6931471805599453
iteration 8 batch 7520 trainingloss 0.6931471805599453
iteration 8 batch 7530 trainingloss 0.6931471805599453
iteration 8 batch 7540 trainingloss 0.6931471805599453
iteration 8 batch 7550 trainingloss 0.6931471805599453
iteration 8 batch 7560 trainingloss 0.6931471805599453
iteration 8 batch 7570 trainingloss 0.6931471805599453
iteration 8 batch 7580 trainingloss 0.6931471805599453
iteration 8 batch 7590 trainingloss 0.6931471805599453
iteration 8 batch 7600 trainingloss 0.6931471805599453
iteration 8 batch 7610 trainingloss 0.6931471805599453
iteration 8 batch 7620 trainingloss 0.6931471805599453
iteration 8 batch 7630 trainingloss 0.6931471805599453
iteration 8 batch 7640 trainingloss 0.6931471805599453
iteration 8 batch 7650 trainingloss 0.6931471805599453
iteration 8 batch 7660 trainingloss 0.6931471805599453
iteration 8 batch 7670 trainingloss 0.6931471805599453
iteration 8 batch 7680 trainingloss 0.6931471805599453
iteration 8 batch 7690 trainingloss 0.6916632528527511
iteration 8 batch 7700 trainingloss 0.6931471805599453
iteration 8 batch 7710 trainingloss 0.6931471805599453
iteration 8 batch 7720 trainingloss 0.6916632528527511
iteration 8 batch 7730 trainingloss 0.6931471805599453
iteration 8 batch 7740 trainingloss 0.6931471805599453
iteration 8 batch 7750 trainingloss 0.6931471805599453
iteration 8 batch 7760 trainingloss 0.6916632528527511
iteration 8 batch 7770 trainingloss 0.6931471805599453
iteration 8 batch 7780 trainingloss 0.6931471805599453
iteration 8 batch 7790 trainingloss 0.6931471805599453
iteration 8 batch 7800 trainingloss 0.6931471805599453
iteration 8 batch 7810 trainingloss 0.6931471805599453
iteration 8 batch 7820 trainingloss 0.6931471805599453
iteration 8 batch 7830 trainingloss 0.6931471805599453
iteration 8 batch 7840 trainingloss 0.6931471805599453
iteration 8 batch 7850 trainingloss 0.6931471805599453
iteration 8 batch 7860 trainingloss 0.6931471805599453
iteration 8 batch 7870 trainingloss 0.6931471805599453
iteration 8 batch 7880 trainingloss 0.6931471805599453
iteration 8 batch 7890 trainingloss 0.6931471805599453
iteration 8 batch 7900 trainingloss 0.6931471805599453
iteration 8 batch 7910 trainingloss 0.6931471805599453
iteration 8 batch 7920 trainingloss 0.6931471805599453
iteration 8 batch 7930 trainingloss 0.6931471805599453
iteration 8 batch 7940 trainingloss 0.6931471805599453
iteration 8 batch 7950 trainingloss 0.6931471805599453
iteration 8 batch 7960 trainingloss 0.6931471805599453
iteration 8 batch 7970 trainingloss 0.6931471805599453
iteration 8 batch 7980 trainingloss 0.6931471805599453
iteration 8 batch 7990 trainingloss 0.6931471805599453
iteration 8 batch 8000 trainingloss 0.6931471805599453
iteration 8 batch 8010 trainingloss 0.6931471805599453
iteration 8 batch 8020 trainingloss 0.6931471805599453
iteration 8 batch 8030 trainingloss 0.6931471805599453
iteration 8 batch 8040 trainingloss 0.6931471805599453
iteration 8 batch 8050 trainingloss 0.6931471805599453
iteration 8 batch 8060 trainingloss 0.6931471805599453
iteration 8 batch 8070 trainingloss 0.6931471805599453
iteration 8 batch 8080 trainingloss 0.6916632528527511
iteration 8 batch 8090 trainingloss 0.6931471805599453
iteration 8 batch 8100 trainingloss 0.6931471805599453
iteration 8 batch 8110 trainingloss 0.6931471805599453
iteration 8 batch 8120 trainingloss 0.6931471805599453
iteration 8 batch 8130 trainingloss 0.6931471805599453
iteration 8 batch 8140 trainingloss 0.6931471805599453
iteration 8 batch 8150 trainingloss 0.6931471805599453
iteration 8 batch 8160 trainingloss 0.6931471805599453
iteration 8 batch 8170 trainingloss 0.6931471805599453
iteration 8 batch 8180 trainingloss 0.6931471805599453
iteration 8 batch 8190 trainingloss 0.6931471805599453
iteration 8 batch 8200 trainingloss 0.6931471805599453
iteration 8 batch 8210 trainingloss 0.6931471805599453
iteration 8 batch 8220 trainingloss 0.6931471805599453
iteration 8 batch 8230 trainingloss 0.6931471805599453
iteration 8 batch 8240 trainingloss 0.6931471805599453
iteration 8 batch 8250 trainingloss 0.6931471805599453
iteration 8 batch 8260 trainingloss 0.6931471805599453
iteration 8 batch 8270 trainingloss 0.6931471805599453
iteration 8 batch 8280 trainingloss 0.6931471805599453
iteration 8 batch 8290 trainingloss 0.6916632528527511
iteration 8 batch 8300 trainingloss 0.6931471805599453
iteration 8 batch 8310 trainingloss 0.6931471805599453
iteration 8 batch 8320 trainingloss 0.6931471805599453
iteration 8 batch 8330 trainingloss 0.6931471805599453
iteration 8 batch 8340 trainingloss 0.6931471805599453
iteration 8 batch 8350 trainingloss 0.6931471805599453
iteration 8 batch 8360 trainingloss 0.6931471805599453
iteration 8 batch 8370 trainingloss 0.6931471805599453
iteration 8 batch 8380 trainingloss 0.6931471805599453
iteration 8 batch 8390 trainingloss 0.6931471805599453
iteration 8 batch 8400 trainingloss 0.6931471805599453
iteration 8 batch 8410 trainingloss 0.6931471805599453
iteration 8 batch 8420 trainingloss 0.6931471805599453
iteration 8 batch 8430 trainingloss 0.6931471805599453
iteration 8 batch 8440 trainingloss 0.6931471805599453
iteration 8 batch 8450 trainingloss 0.6931471805599453
iteration 8 batch 8460 trainingloss 0.6931471805599453
iteration 8 batch 8470 trainingloss 0.6931471805599453
iteration 8 batch 8480 trainingloss 0.6931471805599453
iteration 8 batch 8490 trainingloss 0.6931471805599453
iteration 8 batch 8500 trainingloss 0.6931471805599453
iteration 8 batch 8510 trainingloss 0.6931471805599453
iteration 8 batch 8520 trainingloss 0.6916632528527511
iteration 8 batch 8530 trainingloss 0.6931471805599453
iteration 8 batch 8540 trainingloss 0.6931471805599453
iteration 8 batch 8550 trainingloss 0.6931471805599453
iteration 8 batch 8560 trainingloss 0.6931471805599453
iteration 8 batch 8570 trainingloss 0.6931471805599453
iteration 8 batch 8580 trainingloss 0.6931471805599453
iteration 8 batch 8590 trainingloss 0.6931471805599453
iteration 8 batch 8600 trainingloss 0.6931471805599453
iteration 8 batch 8610 trainingloss 0.6931471805599453
iteration 8 batch 8620 trainingloss 0.6931471805599453
iteration 8 batch 8630 trainingloss 0.6931471805599453
iteration 8 batch 8640 trainingloss 0.6931471805599453
iteration 8 batch 8650 trainingloss 0.6931471805599453
iteration 8 batch 8660 trainingloss 0.6931471805599453
iteration 8 batch 8670 trainingloss 0.6931471805599453
iteration 8 batch 8680 trainingloss 0.6931471805599453
iteration 8 batch 8690 trainingloss 0.6931471805599453
iteration 8 batch 8700 trainingloss 0.6931471805599453
iteration 8 batch 8710 trainingloss 0.6931471805599453
iteration 8 batch 8720 trainingloss 0.6931471805599453
iteration 8 batch 8730 trainingloss 0.6931471805599453
iteration 8 batch 8740 trainingloss 0.6931471805599453
iteration 8 batch 8750 trainingloss 0.6931471805599453
iteration 8 batch 8760 trainingloss 0.6931471805599453
iteration 8 batch 8770 trainingloss 0.6931471805599453
iteration 8 batch 8780 trainingloss 0.6931471805599453
iteration 8 batch 8790 trainingloss 0.6931471805599453
iteration 8 batch 8800 trainingloss 0.6931471805599453
iteration 8 batch 8810 trainingloss 0.6931471805599453
iteration 8 batch 8820 trainingloss 0.6931471805599453
iteration 8 batch 8830 trainingloss 0.6931471805599453
iteration 8 batch 8840 trainingloss 0.6931471805599453
iteration 8 batch 8850 trainingloss 0.6931471805599453
iteration 8 batch 8860 trainingloss 0.6931471805599453
iteration 8 batch 8870 trainingloss 0.6931471805599453
iteration 8 batch 8880 trainingloss 0.6931471805599453
iteration 8 batch 8890 trainingloss 0.6931471805599453
iteration 8 batch 8900 trainingloss 0.6931471805599453
iteration 8 batch 8910 trainingloss 0.6931471805599453
iteration 8 batch 8920 trainingloss 0.6931471805599453
iteration 8 batch 8930 trainingloss 0.6931471805599453
iteration 8 batch 8940 trainingloss 0.6931471805599453
iteration 8 batch 8950 trainingloss 0.6931471805599453
iteration 8 batch 8960 trainingloss 0.6931471805599453
iteration 8 batch 8970 trainingloss 0.6931471805599453
iteration 8 batch 8980 trainingloss 0.6931471805599453
iteration 8 batch 8990 trainingloss 0.6931471805599453
iteration 8 batch 9000 trainingloss 0.6931471805599453
iteration 8 batch 9010 trainingloss 0.6931471805599453
iteration 8 batch 9020 trainingloss 0.6931471805599453
iteration 8 batch 9030 trainingloss 0.6931471805599453
iteration 8 batch 9040 trainingloss 0.6931471805599453
iteration 8 batch 9050 trainingloss 0.6931471805599453
iteration 8 batch 9060 trainingloss 0.6931471805599453
iteration 8 batch 9070 trainingloss 0.6931471805599453
iteration 8 batch 9080 trainingloss 0.6931471805599453
iteration 8 batch 9090 trainingloss 0.6931471805599453
iteration 8 batch 9100 trainingloss 0.6931471805599453
iteration 8 batch 9110 trainingloss 0.6931471805599453
iteration 8 batch 9120 trainingloss 0.6931471805599453
iteration 8 batch 9130 trainingloss 0.6931471805599453
iteration 8 batch 9140 trainingloss 0.6931471805599453
iteration 8 batch 9150 trainingloss 0.6931471805599453
iteration 8 batch 9160 trainingloss 0.6931471805599453
iteration 8 batch 9170 trainingloss 0.6931471805599453
iteration 8 batch 9180 trainingloss 0.6931471805599453
iteration 8 batch 9190 trainingloss 0.6916632528527511
iteration 8 batch 9200 trainingloss 0.6931471805599453
iteration 8 batch 9210 trainingloss 0.6931471805599453
iteration 8 batch 9220 trainingloss 0.6916632528527511
iteration 8 batch 9230 trainingloss 0.6916632528527511
iteration 8 batch 9240 trainingloss 0.6931471805599453
iteration 8 batch 9250 trainingloss 0.6931471805599453
iteration 8 batch 9260 trainingloss 0.6931471805599453
iteration 8 batch 9270 trainingloss 0.6931471805599453
iteration 8 batch 9280 trainingloss 0.6931471805599453
iteration 8 batch 9290 trainingloss 0.6931471805599453
iteration 8 batch 9300 trainingloss 0.6931471805599453
iteration 8 batch 9310 trainingloss 0.6931471805599453
iteration 8 batch 9320 trainingloss 0.6931471805599453
iteration 8 batch 9330 trainingloss 0.6931471805599453
iteration 8 batch 9340 trainingloss 0.6931471805599453
iteration 8 batch 9350 trainingloss 0.6931471805599453
iteration 8 batch 9360 trainingloss 0.6931471805599453
iteration 8 batch 9370 trainingloss 0.6916632528527511
iteration 8 batch 9380 trainingloss 0.6931471805599453
iteration 8 batch 9390 trainingloss 0.6931471805599453
iteration 8 batch 9400 trainingloss 0.6931471805599453
iteration 8 batch 9410 trainingloss 0.6931471805599453
iteration 8 batch 9420 trainingloss 0.6931471805599453
iteration 8 batch 9430 trainingloss 0.6931471805599453
iteration 8 batch 9440 trainingloss 0.6916632528527511
iteration 8 batch 9450 trainingloss 0.6931471805599453
iteration 8 batch 9460 trainingloss 0.6931471805599453
iteration 8 batch 9470 trainingloss 0.6931471805599453
iteration 8 batch 9480 trainingloss 0.6931471805599453
iteration 8 batch 9490 trainingloss 0.6931471805599453
iteration 8 batch 9500 trainingloss 0.6931471805599453
iteration 8 batch 9510 trainingloss 0.6931471805599453
iteration 8 batch 9520 trainingloss 0.6931471805599453
iteration 8 batch 9530 trainingloss 0.6931471805599453
iteration 8 batch 9540 trainingloss 0.6931471805599453
iteration 8 batch 9550 trainingloss 0.6916632528527511
iteration 8 batch 9560 trainingloss 0.6931471805599453
iteration 8 batch 9570 trainingloss 0.6916632528527511
iteration 8 batch 9580 trainingloss 0.6931471805599453
iteration 8 batch 9590 trainingloss 0.6931471805599453
iteration 8 batch 9600 trainingloss 0.6931471805599453
iteration 8 batch 9610 trainingloss 0.6931471805599453
iteration 8 batch 9620 trainingloss 0.6931471805599453
iteration 8 batch 9630 trainingloss 0.6931471805599453
iteration 8 batch 9640 trainingloss 0.6931471805599453
iteration 8 batch 9650 trainingloss 0.6931471805599453
iteration 8 batch 9660 trainingloss 0.6931471805599453
iteration 8 batch 9670 trainingloss 0.6931471805599453
iteration 8 batch 9680 trainingloss 0.6931471805599453
iteration 8 batch 9690 trainingloss 0.6916632528527511
iteration 8 batch 9700 trainingloss 0.6931471805599453
iteration 8 batch 9710 trainingloss 0.6931471805599453
iteration 8 batch 9720 trainingloss 0.6931471805599453
iteration 8 batch 9730 trainingloss 0.6931471805599453
iteration 8 batch 9740 trainingloss 0.6931471805599453
iteration 8 batch 9750 trainingloss 0.6931471805599453
iteration 8 batch 9760 trainingloss 0.6931471805599453
iteration 8 batch 9770 trainingloss 0.6931471805599453
iteration 8 batch 9780 trainingloss 0.6931471805599453
iteration 8 batch 9790 trainingloss 0.6931471805599453
iteration 8 batch 9800 trainingloss 0.6931471805599453
iteration 8 batch 9810 trainingloss 0.6931471805599453
iteration 8 batch 9820 trainingloss 0.6931471805599453
iteration 8 batch 9830 trainingloss 0.6931471805599453
iteration 8 batch 9840 trainingloss 0.6931471805599453
iteration 8 batch 9850 trainingloss 0.6931471805599453
iteration 8 batch 9860 trainingloss 0.6931471805599453
iteration 8 batch 9870 trainingloss 0.6931471805599453
iteration 8 batch 9880 trainingloss 0.6916632528527511
iteration 8 batch 9890 trainingloss 0.6931471805599453
iteration 8 batch 9900 trainingloss 0.6931471805599453
iteration 8 batch 9910 trainingloss 0.6931471805599453
iteration 8 batch 9920 trainingloss 0.6931471805599453
iteration 8 batch 9930 trainingloss 0.6931471805599453
iteration 8 batch 9940 trainingloss 0.6931471805599453
iteration 8 batch 9950 trainingloss 0.6931471805599453
iteration 8 batch 9960 trainingloss 0.6931471805599453
iteration 8 batch 9970 trainingloss 0.6916632528527511
iteration 8 batch 9980 trainingloss 0.6931471805599453
iteration 8 batch 9990 trainingloss 0.6931471805599453
iteration 8 batch 10000 trainingloss 0.6931471805599453
iteration 8 batch 10010 trainingloss 0.6931471805599453
iteration 8 batch 10020 trainingloss 0.6931471805599453
iteration 8 batch 10030 trainingloss 0.6931471805599453
iteration 8 batch 10040 trainingloss 0.6931471805599453
iteration 8 batch 10050 trainingloss 0.6931471805599453
iteration 8 batch 10060 trainingloss 0.6931471805599453
iteration 8 batch 10070 trainingloss 0.6931471805599453
iteration 8 batch 10080 trainingloss 0.6931471805599453
iteration 8 batch 10090 trainingloss 0.6931471805599453
iteration 8 batch 10100 trainingloss 0.6931471805599453
iteration 8 batch 10110 trainingloss 0.6931471805599453
iteration 8 batch 10120 trainingloss 0.6931471805599453
iteration 8 batch 10130 trainingloss 0.6931471805599453
iteration 8 batch 10140 trainingloss 0.6931471805599453
iteration 8 batch 10150 trainingloss 0.6931471805599453
iteration 8 batch 10160 trainingloss 0.6931471805599453
iteration 8 batch 10170 trainingloss 0.6931471805599453
iteration 8 batch 10180 trainingloss 0.6931471805599453
iteration 8 batch 10190 trainingloss 0.6931471805599453
iteration 8 batch 10200 trainingloss 0.6931471805599453
iteration 8 batch 10210 trainingloss 0.6931471805599453
iteration 8 batch 10220 trainingloss 0.6916632528527511
iteration 8 batch 10230 trainingloss 0.6916632528527511
iteration 8 batch 10240 trainingloss 0.6931471805599453
iteration 8 batch 10250 trainingloss 0.6931471805599453
iteration 8 batch 10260 trainingloss 0.6931471805599453
iteration 8 batch 10270 trainingloss 0.6931471805599453
iteration 8 batch 10280 trainingloss 0.6931471805599453
iteration 8 batch 10290 trainingloss 0.6931471805599453
iteration 8 batch 10300 trainingloss 0.6931471805599453
iteration 8 batch 10310 trainingloss 0.6931471805599453
iteration 8 batch 10320 trainingloss 0.6931471805599453
iteration 8 batch 10330 trainingloss 0.6931471805599453
iteration 8 batch 10340 trainingloss 0.6931471805599453
iteration 8 batch 10350 trainingloss 0.6916632528527511
iteration 8 batch 10360 trainingloss 0.6931471805599453
iteration 8 batch 10370 trainingloss 0.6931471805599453
iteration 8 batch 10380 trainingloss 0.6931471805599453
iteration 8 batch 10390 trainingloss 0.6931471805599453
iteration 8 batch 10400 trainingloss 0.6931471805599453
iteration 8 batch 10410 trainingloss 0.6931471805599453
iteration 8 batch 10420 trainingloss 0.6931471805599453
iteration 8 batch 10430 trainingloss 0.6931471805599453
iteration 8 batch 10440 trainingloss 0.6931471805599453
iteration 8 batch 10450 trainingloss 0.6931471805599453
iteration 8 batch 10460 trainingloss 0.6931471805599453
iteration 8 batch 10470 trainingloss 0.6931471805599453
iteration 8 batch 10480 trainingloss 0.6931471805599453
iteration 8 batch 10490 trainingloss 0.6931471805599453
iteration 8 batch 10500 trainingloss 0.6931471805599453
iteration 8 batch 10510 trainingloss 0.6931471805599453
iteration 8 batch 10520 trainingloss 0.6931471805599453
iteration 8 batch 10530 trainingloss 0.6931471805599453
iteration 8 batch 10540 trainingloss 0.6931471805599453
iteration 8 batch 10550 trainingloss 0.6931471805599453
iteration 8 batch 10560 trainingloss 0.6931471805599453
iteration 8 batch 10570 trainingloss 0.6931471805599453
iteration 8 batch 10580 trainingloss 0.6931471805599453
iteration 8 batch 10590 trainingloss 0.6931471805599453
iteration 8 batch 10600 trainingloss 0.6931471805599453
iteration 8 batch 10610 trainingloss 0.6931471805599453
iteration 8 batch 10620 trainingloss 0.6931471805599453
iteration 8 batch 10630 trainingloss 0.6931471805599453
iteration 8 batch 10640 trainingloss 0.6931471805599453
iteration 8 batch 10650 trainingloss 0.6931471805599453
iteration 8 batch 10660 trainingloss 0.6916632528527511
iteration 8 batch 10670 trainingloss 0.6931471805599453
iteration 8 batch 10680 trainingloss 0.6931471805599453
iteration 8 batch 10690 trainingloss 0.6931471805599453
iteration 8 batch 10700 trainingloss 0.6931471805599453
iteration 8 batch 10710 trainingloss 0.6931471805599453
iteration 8 batch 10720 trainingloss 0.6916632528527511
iteration 8 batch 10730 trainingloss 0.6931471805599453
iteration 8 batch 10740 trainingloss 0.6931471805599453
iteration 8 batch 10750 trainingloss 0.6931471805599453
iteration 8 batch 10760 trainingloss 0.6931471805599453
iteration 8 batch 10770 trainingloss 0.6931471805599453
iteration 8 batch 10780 trainingloss 0.6931471805599453
iteration 8 batch 10790 trainingloss 0.6931471805599453
iteration 8 batch 10800 trainingloss 0.6931471805599453
iteration 8 batch 10810 trainingloss 0.6931471805599453
iteration 8 batch 10820 trainingloss 0.6931471805599453
iteration 8 batch 10830 trainingloss 0.6931471805599453
iteration 8 batch 10840 trainingloss 0.6931471805599453
iteration 8 batch 10850 trainingloss 0.6931471805599453
iteration 8 batch 10860 trainingloss 0.6916632528527511
iteration 8 batch 10870 trainingloss 0.6931471805599453
iteration 8 batch 10880 trainingloss 0.6931471805599453
iteration 8 batch 10890 trainingloss 0.6931471805599453
iteration 8 batch 10900 trainingloss 0.6931471805599453
iteration 8 batch 10910 trainingloss 0.6931471805599453
iteration 8 batch 10920 trainingloss 0.6931471805599453
iteration 8 batch 10930 trainingloss 0.6931471805599453
iteration 8 batch 10940 trainingloss 0.6931471805599453
iteration 8 batch 10950 trainingloss 0.6931471805599453
iteration 8 batch 10960 trainingloss 0.6931471805599453
iteration 8 batch 10970 trainingloss 0.6931471805599453
iteration 8 batch 10980 trainingloss 0.6916632528527511
iteration 8 batch 10990 trainingloss 0.6931471805599453
iteration 8 batch 11000 trainingloss 0.6931471805599453
iteration 8 batch 11010 trainingloss 0.6931471805599453
iteration 8 batch 11020 trainingloss 0.6931471805599453
iteration 8 batch 11030 trainingloss 0.6931471805599453
iteration 8 batch 11040 trainingloss 0.6931471805599453
iteration 8 batch 11050 trainingloss 0.6931471805599453
iteration 8 batch 11060 trainingloss 0.6931471805599453
iteration 8 batch 11070 trainingloss 0.6931471805599453
iteration 8 batch 11080 trainingloss 0.6916632528527511
iteration 8 batch 11090 trainingloss 0.6931471805599453
iteration 8 batch 11100 trainingloss 0.6931471805599453
iteration 8 batch 11110 trainingloss 0.6931471805599453
iteration 8 batch 11120 trainingloss 0.6931471805599453
iteration 8 batch 11130 trainingloss 0.6931471805599453
iteration 8 batch 11140 trainingloss 0.6931471805599453
iteration 8 batch 11150 trainingloss 0.6931471805599453
iteration 8 batch 11160 trainingloss 0.6931471805599453
iteration 8 batch 11170 trainingloss 0.6931471805599453
iteration 8 batch 11180 trainingloss 0.6916632528527511
iteration 8 batch 11190 trainingloss 0.6931471805599453
iteration 8 batch 11200 trainingloss 0.6931471805599453
iteration 8 batch 11210 trainingloss 0.6931471805599453
iteration 8 batch 11220 trainingloss 0.6931471805599453
iteration 8 batch 11230 trainingloss 0.6931471805599453
iteration 8 batch 11240 trainingloss 0.6931471805599453
iteration 8 batch 11250 trainingloss 0.6931471805599453
iteration 8 batch 11260 trainingloss 0.6931471805599453
iteration 8 batch 11270 trainingloss 0.6931471805599453
iteration 8 batch 11280 trainingloss 0.6931471805599453
iteration 8 batch 11290 trainingloss 0.6931471805599453
iteration 8 batch 11300 trainingloss 0.6931471805599453
iteration 8 batch 11310 trainingloss 0.6931471805599453
iteration 8 batch 11320 trainingloss 0.6931471805599453
iteration 8 batch 11330 trainingloss 0.6931471805599453
iteration 8 batch 11340 trainingloss 0.6931471805599453
iteration 8 batch 11350 trainingloss 0.6931471805599453
iteration 8 batch 11360 trainingloss 0.6931471805599453
iteration 8 batch 11370 trainingloss 0.6931471805599453
iteration 8 batch 11380 trainingloss 0.6931471805599453
iteration 8 batch 11390 trainingloss 0.6916632528527511
iteration 8 batch 11400 trainingloss 0.6931471805599453
iteration 8 batch 11410 trainingloss 0.6931471805599453
iteration 8 batch 11420 trainingloss 0.6931471805599453
iteration 8 batch 11430 trainingloss 0.6931471805599453
iteration 8 batch 11440 trainingloss 0.6931471805599453
iteration 8 batch 11450 trainingloss 0.6931471805599453
iteration 8 batch 11460 trainingloss 0.6931471805599453
iteration 8 batch 11470 trainingloss 0.6931471805599453
iteration 8 batch 11480 trainingloss 0.6931471805599453
iteration 8 batch 11490 trainingloss 0.6931471805599453
iteration 8 batch 11500 trainingloss 0.6931471805599453
iteration 8 batch 11510 trainingloss 0.6931471805599453
iteration 8 batch 11520 trainingloss 0.6931471805599453
iteration 8 batch 11530 trainingloss 0.6931471805599453
iteration 8 batch 11540 trainingloss 0.6931471805599453
iteration 8 batch 11550 trainingloss 0.6931471805599453
iteration 8 batch 11560 trainingloss 0.6931471805599453
iteration 8 batch 11570 trainingloss 0.6931471805599453
iteration 8 batch 11580 trainingloss 0.6931471805599453
iteration 8 batch 11590 trainingloss 0.6931471805599453
iteration 8 batch 11600 trainingloss 0.6931471805599453
iteration 8 batch 11610 trainingloss 0.6931471805599453
iteration 8 batch 11620 trainingloss 0.6931471805599453
iteration 8 batch 11630 trainingloss 0.6931471805599453
iteration 8 batch 11640 trainingloss 0.6931471805599453
iteration 8 batch 11650 trainingloss 0.6931471805599453
iteration 8 batch 11660 trainingloss 0.6931471805599453
iteration 8 batch 11670 trainingloss 0.6931471805599453
iteration 8 batch 11680 trainingloss 0.6931471805599453
iteration 8 batch 11690 trainingloss 0.6931471805599453
iteration 8 batch 11700 trainingloss 0.6931471805599453
iteration 8 batch 11710 trainingloss 0.6931471805599453
iteration 8 batch 11720 trainingloss 0.6931471805599453
iteration 8 batch 11730 trainingloss 0.6916632528527511
iteration 8 batch 11740 trainingloss 0.6931471805599453
iteration 8 batch 11750 trainingloss 0.6916632528527511
iteration 8 batch 11760 trainingloss 0.6931471805599453
iteration 8 batch 11770 trainingloss 0.6931471805599453
iteration 8 batch 11780 trainingloss 0.6916632528527511
iteration 8 batch 11790 trainingloss 0.6931471805599453
iteration 8 batch 11800 trainingloss 0.6931471805599453
iteration 8 batch 11810 trainingloss 0.6931471805599453
iteration 8 batch 11820 trainingloss 0.6931471805599453
iteration 8 batch 11830 trainingloss 0.6931471805599453
iteration 8 batch 11840 trainingloss 0.6931471805599453
iteration 8 batch 11850 trainingloss 0.6931471805599453
iteration 8 batch 11860 trainingloss 0.6931471805599453
iteration 8 batch 11870 trainingloss 0.6931471805599453
iteration 8 batch 11880 trainingloss 0.6931471805599453
iteration 8 batch 11890 trainingloss 0.6931471805599453
iteration 8 batch 11900 trainingloss 0.6931471805599453
iteration 8 batch 11910 trainingloss 0.6931471805599453
iteration 8 batch 11920 trainingloss 0.6931471805599453
iteration 8 batch 11930 trainingloss 0.6931471805599453
iteration 8 batch 11940 trainingloss 0.6931471805599453
iteration 8 batch 11950 trainingloss 0.6931471805599453
iteration 8 batch 11960 trainingloss 0.6931471805599453
iteration 8 batch 11970 trainingloss 0.6916632528527511
iteration 8 batch 11980 trainingloss 0.6931471805599453
iteration 8 batch 11990 trainingloss 0.6931471805599453
iteration 8 batch 12000 trainingloss 0.6931471805599453
iteration 8 batch 12010 trainingloss 0.6916632528527511
iteration 8 batch 12020 trainingloss 0.6931471805599453
iteration 8 batch 12030 trainingloss 0.6931471805599453
iteration 8 batch 12040 trainingloss 0.6931471805599453
iteration 8 batch 12050 trainingloss 0.6931471805599453
iteration 8 batch 12060 trainingloss 0.6931471805599453
iteration 8 batch 12070 trainingloss 0.6931471805599453
iteration 8 batch 12080 trainingloss 0.6931471805599453
iteration 8 batch 12090 trainingloss 0.6931471805599453
iteration 8 batch 12100 trainingloss 0.6931471805599453
iteration 8 batch 12110 trainingloss 0.6931471805599453
iteration 8 batch 12120 trainingloss 0.6931471805599453
iteration 8 batch 12130 trainingloss 0.6931471805599453
iteration 8 batch 12140 trainingloss 0.6931471805599453
iteration 8 batch 12150 trainingloss 0.6931471805599453
iteration 8 batch 12160 trainingloss 0.6931471805599453
iteration 8 batch 12170 trainingloss 0.6931471805599453
iteration 8 batch 12180 trainingloss 0.6931471805599453
iteration 8 batch 12190 trainingloss 0.6931471805599453
iteration 8 batch 12200 trainingloss 0.6931471805599453
iteration 8 batch 12210 trainingloss 0.6931471805599453
iteration 8 batch 12220 trainingloss 0.6931471805599453
iteration 8 batch 12230 trainingloss 0.6931471805599453
iteration 8 batch 12240 trainingloss 0.6931471805599453
iteration 8 batch 12250 trainingloss 0.6931471805599453
iteration 8 batch 12260 trainingloss 0.6931471805599453
iteration 8 batch 12270 trainingloss 0.6931471805599453
iteration 8 batch 12280 trainingloss 0.6931471805599453
iteration 8 batch 12290 trainingloss 0.6931471805599453
iteration 8 batch 12300 trainingloss 0.6931471805599453
iteration 8 batch 12310 trainingloss 0.6931471805599453
iteration 8 batch 12320 trainingloss 0.6931471805599453
iteration 8 batch 12330 trainingloss 0.6931471805599453
iteration 8 batch 12340 trainingloss 0.6931471805599453
iteration 8 batch 12350 trainingloss 0.6931471805599453
iteration 8 batch 12360 trainingloss 0.6931471805599453
iteration 8 batch 12370 trainingloss 0.6931471805599453
iteration 8 batch 12380 trainingloss 0.6931471805599453
iteration 8 batch 12390 trainingloss 0.6931471805599453
iteration 8 batch 12400 trainingloss 0.6931471805599453
iteration 8 batch 12410 trainingloss 0.6931471805599453
iteration 8 batch 12420 trainingloss 0.6931471805599453
iteration 8 batch 12430 trainingloss 0.6931471805599453
iteration 8 batch 12440 trainingloss 0.6931471805599453
iteration 8 batch 12450 trainingloss 0.6931471805599453
iteration 8 batch 12460 trainingloss 0.6931471805599453
iteration 8 batch 12470 trainingloss 0.6931471805599453
iteration 8 batch 12480 trainingloss 0.6931471805599453
iteration 8 batch 12490 trainingloss 0.6931471805599453
iteration 8 batch 12500 trainingloss 0.6931471805599453
iteration 8 batch 12510 trainingloss 0.6931471805599453
iteration 8 batch 12520 trainingloss 0.6931471805599453
iteration 8 batch 12530 trainingloss 0.6931471805599453
iteration 8 batch 12540 trainingloss 0.6931471805599453
iteration 8 batch 12550 trainingloss 0.6931471805599453
iteration 8 batch 12560 trainingloss 0.6931471805599453
iteration 8 batch 12570 trainingloss 0.6931471805599453
iteration 8 batch 12580 trainingloss 0.6931471805599453
iteration 8 batch 12590 trainingloss 0.6931471805599453
iteration 8 batch 12600 trainingloss 0.6931471805599453
iteration 8 batch 12610 trainingloss 0.6931471805599453
iteration 8 batch 12620 trainingloss 0.6931471805599453
iteration 8 batch 12630 trainingloss 0.6931471805599453
iteration 8 batch 12640 trainingloss 0.6931471805599453
iteration 8 batch 12650 trainingloss 0.6931471805599453
iteration 8 batch 12660 trainingloss 0.6931471805599453
iteration 8 batch 12670 trainingloss 0.6931471805599453
iteration 8 batch 12680 trainingloss 0.6916632528527511
iteration 8 batch 12690 trainingloss 0.6931471805599453
iteration 8 batch 12700 trainingloss 0.6931471805599453
iteration 8 batch 12710 trainingloss 0.6931471805599453
iteration 8 batch 12720 trainingloss 0.6931471805599453
iteration 8 batch 12730 trainingloss 0.6931471805599453
iteration 8 batch 12740 trainingloss 0.6931471805599453
iteration 8 batch 12750 trainingloss 0.6931471805599453
iteration 8 batch 12760 trainingloss 0.6931471805599453
iteration 8 batch 12770 trainingloss 0.6931471805599453
iteration 8 batch 12780 trainingloss 0.6931471805599453
iteration 8 batch 12790 trainingloss 0.6931471805599453
iteration 8 batch 12800 trainingloss 0.6931471805599453
iteration 8 batch 12810 trainingloss 0.6931471805599453
iteration 8 batch 12820 trainingloss 0.6931471805599453
iteration 8 batch 12830 trainingloss 0.6931471805599453
iteration 8 batch 12840 trainingloss 0.6931471805599453
iteration 8 batch 12850 trainingloss 0.6931471805599453
iteration 8 batch 12860 trainingloss 0.6931471805599453
iteration 8 batch 12870 trainingloss 0.6931471805599453
iteration 8 batch 12880 trainingloss 0.6931471805599453
iteration 8 batch 12890 trainingloss 0.6931471805599453
iteration 8 batch 12900 trainingloss 0.6931471805599453
iteration 8 batch 12910 trainingloss 0.6931471805599453
iteration 8 batch 12920 trainingloss 0.6931471805599453
iteration 8 batch 12930 trainingloss 0.6931471805599453
iteration 8 batch 12940 trainingloss 0.6931471805599453
iteration 8 batch 12950 trainingloss 0.6916632528527511
iteration 8 batch 12960 trainingloss 0.6931471805599453
iteration 8 batch 12970 trainingloss 0.6931471805599453
iteration 8 batch 12980 trainingloss 0.6931471805599453
iteration 8 batch 12990 trainingloss 0.6931471805599453
iteration 8 batch 13000 trainingloss 0.6931471805599453
iteration 8 batch 13010 trainingloss 0.6931471805599453
iteration 8 batch 13020 trainingloss 0.6931471805599453
iteration 8 batch 13030 trainingloss 0.6931471805599453
iteration 8 batch 13040 trainingloss 0.6931471805599453
iteration 8 batch 13050 trainingloss 0.6931471805599453
iteration 8 batch 13060 trainingloss 0.6931471805599453
iteration 8 batch 13070 trainingloss 0.6931471805599453
iteration 8 batch 13080 trainingloss 0.6931471805599453
iteration 8 batch 13090 trainingloss 0.6931471805599453
iteration 8 batch 13100 trainingloss 0.6931471805599453
iteration 8 batch 13110 trainingloss 0.6931471805599453
iteration 8 batch 13120 trainingloss 0.6931471805599453
iteration 8 batch 13130 trainingloss 0.6931471805599453
iteration 8 batch 13140 trainingloss 0.6931471805599453
iteration 8 batch 13150 trainingloss 0.6931471805599453
iteration 8 batch 13160 trainingloss 0.6931471805599453
iteration 8 batch 13170 trainingloss 0.6931471805599453
iteration 8 batch 13180 trainingloss 0.6931471805599453
iteration 8 batch 13190 trainingloss 0.6931471805599453
iteration 8 batch 13200 trainingloss 0.6931471805599453
iteration 8 batch 13210 trainingloss 0.6901793251455568
iteration 8 batch 13220 trainingloss 0.6931471805599453
iteration 8 batch 13230 trainingloss 0.6931471805599453
iteration 8 batch 13240 trainingloss 0.6931471805599453
iteration 8 batch 13250 trainingloss 0.6931471805599453
iteration 8 batch 13260 trainingloss 0.6931471805599453
iteration 8 batch 13270 trainingloss 0.6931471805599453
iteration 8 batch 13280 trainingloss 0.6931471805599453
iteration 8 batch 13290 trainingloss 0.6931471805599453
iteration 8 batch 13300 trainingloss 0.6931471805599453
iteration 8 batch 13310 trainingloss 0.6931471805599453
iteration 8 batch 13320 trainingloss 0.6916632528527511
iteration 8 batch 13330 trainingloss 0.6931471805599453
iteration 8 batch 13340 trainingloss 0.6931471805599453
iteration 8 batch 13350 trainingloss 0.6931471805599453
iteration 8 batch 13360 trainingloss 0.6931471805599453
iteration 8 batch 13370 trainingloss 0.6931471805599453
iteration 8 batch 13380 trainingloss 0.6931471805599453
iteration 8 batch 13390 trainingloss 0.6931471805599453
iteration 8 batch 13400 trainingloss 0.6931471805599453
iteration 8 batch 13410 trainingloss 0.6931471805599453
iteration 8 batch 13420 trainingloss 0.6931471805599453
iteration 8 batch 13430 trainingloss 0.6931471805599453
iteration 8 batch 13440 trainingloss 0.6916632528527511
iteration 8 batch 13450 trainingloss 0.6931471805599453
iteration 8 batch 13460 trainingloss 0.6931471805599453
iteration 8 batch 13470 trainingloss 0.6931471805599453
iteration 8 batch 13480 trainingloss 0.6931471805599453
iteration 8 batch 13490 trainingloss 0.6931471805599453
iteration 8 batch 13500 trainingloss 0.6931471805599453
iteration 8 batch 13510 trainingloss 0.6931471805599453
iteration 8 batch 13520 trainingloss 0.6931471805599453
iteration 8 batch 13530 trainingloss 0.6931471805599453
iteration 8 batch 13540 trainingloss 0.6931471805599453
iteration 8 batch 13550 trainingloss 0.6931471805599453
iteration 8 batch 13560 trainingloss 0.6931471805599453
iteration 8 batch 13570 trainingloss 0.6931471805599453
iteration 8 batch 13580 trainingloss 0.6931471805599453
iteration 8 batch 13590 trainingloss 0.6931471805599453
iteration 8 batch 13600 trainingloss 0.6931471805599453
iteration 8 batch 13610 trainingloss 0.6931471805599453
iteration 8 batch 13620 trainingloss 0.6916632528527511
iteration 8 batch 13630 trainingloss 0.6931471805599453
iteration 8 batch 13640 trainingloss 0.6931471805599453
iteration 8 batch 13650 trainingloss 0.6931471805599453
iteration 8 batch 13660 trainingloss 0.6931471805599453
iteration 8 batch 13670 trainingloss 0.6931471805599453
iteration 8 batch 13680 trainingloss 0.6931471805599453
iteration 8 batch 13690 trainingloss 0.6931471805599453
iteration 8 batch 13700 trainingloss 0.6931471805599453
iteration 8 batch 13710 trainingloss 0.6931471805599453
iteration 8 batch 13720 trainingloss 0.6931471805599453
iteration 8 batch 13730 trainingloss 0.6931471805599453
iteration 8 batch 13740 trainingloss 0.6931471805599453
iteration 8 batch 13750 trainingloss 0.6931471805599453
iteration 8 batch 13760 trainingloss 0.6931471805599453
iteration 8 batch 13770 trainingloss 0.6931471805599453
iteration 8 batch 13780 trainingloss 0.6931471805599453
iteration 8 batch 13790 trainingloss 0.6931471805599453
iteration 8 batch 13800 trainingloss 0.6931471805599453
iteration 8 batch 13810 trainingloss 0.6931471805599453
iteration 8 batch 13820 trainingloss 0.6931471805599453
iteration 8 batch 13830 trainingloss 0.6916632528527511
iteration 8 batch 13840 trainingloss 0.6931471805599453
iteration 8 batch 13850 trainingloss 0.6916632528527511
iteration 8 batch 13860 trainingloss 0.6931471805599453
iteration 8 batch 13870 trainingloss 0.6916632528527511
iteration 8 batch 13880 trainingloss 0.6931471805599453
iteration 8 batch 13890 trainingloss 0.6931471805599453
iteration 8 batch 13900 trainingloss 0.6931471805599453
iteration 8 batch 13910 trainingloss 0.6931471805599453
iteration 8 batch 13920 trainingloss 0.6931471805599453
iteration 8 batch 13930 trainingloss 0.6916632528527511
iteration 8 batch 13940 trainingloss 0.6931471805599453
iteration 8 batch 13950 trainingloss 0.6931471805599453
iteration 8 batch 13960 trainingloss 0.6931471805599453
iteration 8 batch 13970 trainingloss 0.6931471805599453
iteration 8 batch 13980 trainingloss 0.6931471805599453
iteration 8 batch 13990 trainingloss 0.6931471805599453
iteration 8 batch 14000 trainingloss 0.6931471805599453
iteration 8 batch 14010 trainingloss 0.6931471805599453
iteration 8 batch 14020 trainingloss 0.6931471805599453
iteration 8 batch 14030 trainingloss 0.6931471805599453
iteration 8 batch 14040 trainingloss 0.6931471805599453
iteration 8 batch 14050 trainingloss 0.6916632528527511
iteration 8 batch 14060 trainingloss 0.6916632528527511
iteration 8 batch 14070 trainingloss 0.6931471805599453
iteration 8 batch 14080 trainingloss 0.6931471805599453
iteration 8 batch 14090 trainingloss 0.6931471805599453
iteration 8 batch 14100 trainingloss 0.6931471805599453
iteration 8 batch 14110 trainingloss 0.6931471805599453
iteration 8 batch 14120 trainingloss 0.6931471805599453
iteration 8 batch 14130 trainingloss 0.6931471805599453
iteration 8 batch 14140 trainingloss 0.6931471805599453
iteration 8 batch 14150 trainingloss 0.6931471805599453
iteration 8 batch 14160 trainingloss 0.6931471805599453
iteration 8 batch 14170 trainingloss 0.6931471805599453
iteration 8 batch 14180 trainingloss 0.6931471805599453
iteration 8 batch 14190 trainingloss 0.6931471805599453
iteration 8 batch 14200 trainingloss 0.6931471805599453
iteration 8 batch 14210 trainingloss 0.6931471805599453
iteration 8 batch 14220 trainingloss 0.6931471805599453
iteration 8 batch 14230 trainingloss 0.6931471805599453
iteration 8 batch 14240 trainingloss 0.6931471805599453
iteration 8 batch 14250 trainingloss 0.6931471805599453
iteration 8 batch 14260 trainingloss 0.6931471805599453
iteration 8 batch 14270 trainingloss 0.6931471805599453
iteration 8 batch 14280 trainingloss 0.6931471805599453
iteration 8 batch 14290 trainingloss 0.6931471805599453
iteration 8 batch 14300 trainingloss 0.6931471805599453
iteration 8 batch 14310 trainingloss 0.6931471805599453
iteration 8 batch 14320 trainingloss 0.6931471805599453
iteration 8 batch 14330 trainingloss 0.6931471805599453
iteration 8 batch 14340 trainingloss 0.6931471805599453
iteration 8 batch 14350 trainingloss 0.6931471805599453
iteration 8 batch 14360 trainingloss 0.6931471805599453
iteration 8 batch 14370 trainingloss 0.6931471805599453
iteration 8 batch 14380 trainingloss 0.6931471805599453
iteration 8 batch 14390 trainingloss 0.6931471805599453
iteration 8 batch 14400 trainingloss 0.6931471805599453
iteration 8 batch 14410 trainingloss 0.6931471805599453
iteration 8 batch 14420 trainingloss 0.6931471805599453
iteration 8 batch 14430 trainingloss 0.6931471805599453
iteration 8 batch 14440 trainingloss 0.6931471805599453
iteration 8 batch 14450 trainingloss 0.6931471805599453
iteration 8 batch 14460 trainingloss 0.6931471805599453
iteration 8 batch 14470 trainingloss 0.6931471805599453
iteration 8 batch 14480 trainingloss 0.6931471805599453
iteration 8 batch 14490 trainingloss 0.6931471805599453
iteration 8 batch 14500 trainingloss 0.6931471805599453
iteration 8 batch 14510 trainingloss 0.6931471805599453
iteration 8 batch 14520 trainingloss 0.6931471805599453
iteration 8 batch 14530 trainingloss 0.6931471805599453
iteration 8 batch 14540 trainingloss 0.6931471805599453
iteration 8 batch 14550 trainingloss 0.6931471805599453
iteration 8 batch 14560 trainingloss 0.6931471805599453
iteration 8 batch 14570 trainingloss 0.6931471805599453
iteration 8 batch 14580 trainingloss 0.6931471805599453
iteration 8 batch 14590 trainingloss 0.6931471805599453
iteration 8 batch 14600 trainingloss 0.6931471805599453
iteration 8 batch 14610 trainingloss 0.6931471805599453
iteration 8 batch 14620 trainingloss 0.6931471805599453
iteration 8 batch 14630 trainingloss 0.6931471805599453
iteration 8 batch 14640 trainingloss 0.6931471805599453
iteration 8 batch 14650 trainingloss 0.6931471805599453
iteration 8 batch 14660 trainingloss 0.6931471805599453
iteration 8 batch 14670 trainingloss 0.6931471805599453
iteration 8 batch 14680 trainingloss 0.6931471805599453
iteration 8 batch 14690 trainingloss 0.6931471805599453
iteration 8 batch 14700 trainingloss 0.6931471805599453
iteration 8 batch 14710 trainingloss 0.6931471805599453
iteration 8 batch 14720 trainingloss 0.6931471805599453
iteration 8 batch 14730 trainingloss 0.6931471805599453
iteration 8 batch 14740 trainingloss 0.6931471805599453
iteration 8 batch 14750 trainingloss 0.6931471805599453
iteration 8 batch 14760 trainingloss 0.6931471805599453
iteration 8 batch 14770 trainingloss 0.6931471805599453
iteration 8 batch 14780 trainingloss 0.6931471805599453
iteration 8 batch 14790 trainingloss 0.6931471805599453
iteration 8 batch 14800 trainingloss 0.6931471805599453
iteration 8 batch 14810 trainingloss 0.6931471805599453
iteration 8 batch 14820 trainingloss 0.6931471805599453
iteration 8 batch 14830 trainingloss 0.6931471805599453
iteration 8 batch 14840 trainingloss 0.6931471805599453
iteration 8 batch 14850 trainingloss 0.6931471805599453
iteration 8 batch 14860 trainingloss 0.6931471805599453
iteration 8 batch 14870 trainingloss 0.6931471805599453
iteration 8 batch 14880 trainingloss 0.6931471805599453
iteration 8 batch 14890 trainingloss 0.6931471805599453
iteration 8 batch 14900 trainingloss 0.6931471805599453
iteration 8 batch 14910 trainingloss 0.6931471805599453
iteration 8 batch 14920 trainingloss 0.6931471805599453
iteration 8 batch 14930 trainingloss 0.6916632528527511
iteration 8 batch 14940 trainingloss 0.6931471805599453
iteration 8 batch 14950 trainingloss 0.6931471805599453
iteration 8 batch 14960 trainingloss 0.6931471805599453
iteration 8 batch 14970 trainingloss 0.6931471805599453
iteration 8 batch 14980 trainingloss 0.6916632528527511
iteration 8 batch 14990 trainingloss 0.6931471805599453
iteration 8 batch 15000 trainingloss 0.6931471805599453
iteration 8 batch 15010 trainingloss 0.6931471805599453
iteration 8 batch 15020 trainingloss 0.6931471805599453
iteration 8 batch 15030 trainingloss 0.6931471805599453
iteration 8 batch 15040 trainingloss 0.6931471805599453
iteration 8 batch 15050 trainingloss 0.6931471805599453
iteration 8 batch 15060 trainingloss 0.6931471805599453
iteration 8 batch 15070 trainingloss 0.6931471805599453
iteration 8 batch 15080 trainingloss 0.6931471805599453
iteration 8 batch 15090 trainingloss 0.6931471805599453
iteration 8 batch 15100 trainingloss 0.6916632528527511
iteration 8 batch 15110 trainingloss 0.6931471805599453
iteration 8 batch 15120 trainingloss 0.6931471805599453
iteration 8 batch 15130 trainingloss 0.6931471805599453
iteration 8 batch 15140 trainingloss 0.6931471805599453
iteration 8 batch 15150 trainingloss 0.6916632528527511
iteration 8 batch 15160 trainingloss 0.6931471805599453
iteration 8 batch 15170 trainingloss 0.6931471805599453
iteration 8 batch 15180 trainingloss 0.6931471805599453
iteration 8 batch 15190 trainingloss 0.6931471805599453
iteration 8 batch 15200 trainingloss 0.6931471805599453
iteration 8 batch 15210 trainingloss 0.6931471805599453
iteration 8 batch 15220 trainingloss 0.6931471805599453
iteration 8 batch 15230 trainingloss 0.6931471805599453
iteration 8 batch 15240 trainingloss 0.6931471805599453
iteration 8 batch 15250 trainingloss 0.6931471805599453
iteration 8 batch 15260 trainingloss 0.6931471805599453
iteration 8 batch 15270 trainingloss 0.6931471805599453
iteration 8 batch 15280 trainingloss 0.6931471805599453
iteration 8 batch 15290 trainingloss 0.6931471805599453
iteration 8 batch 15300 trainingloss 0.6931471805599453
iteration 8 batch 15310 trainingloss 0.6931471805599453
iteration 8 batch 15320 trainingloss 0.6931471805599453
iteration 8 batch 15330 trainingloss 0.6931471805599453
iteration 8 batch 15340 trainingloss 0.6931471805599453
iteration 8 batch 15350 trainingloss 0.6931471805599453
iteration 8 batch 15360 trainingloss 0.6931471805599453
iteration 8 batch 15370 trainingloss 0.6916632528527511
iteration 8 batch 15380 trainingloss 0.6931471805599453
iteration 8 batch 15390 trainingloss 0.6931471805599453
iteration 8 batch 15400 trainingloss 0.6931471805599453
iteration 8 batch 15410 trainingloss 0.6931471805599453
iteration 8 batch 15420 trainingloss 0.6931471805599453
iteration 8 batch 15430 trainingloss 0.6931471805599453
iteration 8 batch 15440 trainingloss 0.6931471805599453
iteration 8 batch 15450 trainingloss 0.6931471805599453
iteration 8 batch 15460 trainingloss 0.6931471805599453
iteration 8 batch 15470 trainingloss 0.6931471805599453
iteration 8 batch 15480 trainingloss 0.6931471805599453
iteration 8 batch 15490 trainingloss 0.6931471805599453
iteration 8 batch 15500 trainingloss 0.6931471805599453
iteration 8 batch 15510 trainingloss 0.6931471805599453
iteration 8 batch 15520 trainingloss 0.6931471805599453
iteration 8 batch 15530 trainingloss 0.6931471805599453
iteration 8 batch 15540 trainingloss 0.6931471805599453
iteration 8 batch 15550 trainingloss 0.6931471805599453
iteration 8 batch 15560 trainingloss 0.6931471805599453
iteration 8 batch 15570 trainingloss 0.6931471805599453
iteration 8 batch 15580 trainingloss 0.6931471805599453
iteration 8 batch 15590 trainingloss 0.6931471805599453
iteration 8 batch 15600 trainingloss 0.6931471805599453
iteration 8 batch 15610 trainingloss 0.6931471805599453
iteration 8 batch 15620 trainingloss 0.6931471805599453
iteration 8 batch 15630 trainingloss 0.6931471805599453
iteration 8 batch 15640 trainingloss 0.6931471805599453
iteration 8 batch 15650 trainingloss 0.6931471805599453
iteration 8 batch 15660 trainingloss 0.6931471805599453
iteration 8 batch 15670 trainingloss 0.6916632528527511
iteration 8 batch 15680 trainingloss 0.6931471805599453
iteration 8 batch 15690 trainingloss 0.6931471805599453
iteration 8 batch 15700 trainingloss 0.6931471805599453
iteration 8 batch 15710 trainingloss 0.6931471805599453
iteration 8 batch 15720 trainingloss 0.6931471805599453
iteration 8 batch 15730 trainingloss 0.6931471805599453
iteration 8 batch 15740 trainingloss 0.6931471805599453
iteration 8 batch 15750 trainingloss 0.6931471805599453
iteration 8 batch 15760 trainingloss 0.6931471805599453
iteration 8 batch 15770 trainingloss 0.6931471805599453
iteration 8 batch 15780 trainingloss 0.6931471805599453
iteration 8 batch 15790 trainingloss 0.6931471805599453
iteration 8 batch 15800 trainingloss 0.6931471805599453
iteration 8 batch 15810 trainingloss 0.6931471805599453
iteration 8 batch 15820 trainingloss 0.6931471805599453
iteration 8 batch 15830 trainingloss 0.6931471805599453
iteration 8 batch 15840 trainingloss 0.6931471805599453
iteration 8 batch 15850 trainingloss 0.6931471805599453
iteration 8 batch 15860 trainingloss 0.6931471805599453
iteration 8 batch 15870 trainingloss 0.6931471805599453
iteration 8 batch 15880 trainingloss 0.6931471805599453
iteration 8 batch 15890 trainingloss 0.6931471805599453
iteration 8 batch 15900 trainingloss 0.6931471805599453
iteration 8 batch 15910 trainingloss 0.6931471805599453
iteration 8 batch 15920 trainingloss 0.6931471805599453
iteration 8 batch 15930 trainingloss 0.6931471805599453
iteration 8 batch 15940 trainingloss 0.6931471805599453
iteration 8 batch 15950 trainingloss 0.6931471805599453
iteration 8 batch 15960 trainingloss 0.6931471805599453
iteration 8 batch 15970 trainingloss 0.6931471805599453
iteration 8 batch 15980 trainingloss 0.6931471805599453
iteration 8 batch 15990 trainingloss 0.6931471805599453
iteration 8 batch 16000 trainingloss 0.6931471805599453
iteration 8 batch 16010 trainingloss 0.6931471805599453
iteration 8 batch 16020 trainingloss 0.6931471805599453
iteration 8 batch 16030 trainingloss 0.6931471805599453
iteration 8 batch 16040 trainingloss 0.6931471805599453
iteration 8 batch 16050 trainingloss 0.6931471805599453
iteration 8 batch 16060 trainingloss 0.6931471805599453
iteration 8 batch 16070 trainingloss 0.6931471805599453
iteration 8 batch 16080 trainingloss 0.6931471805599453
iteration 8 batch 16090 trainingloss 0.6931471805599453
iteration 8 batch 16100 trainingloss 0.6931471805599453
iteration 8 batch 16110 trainingloss 0.6916632528527511
iteration 8 batch 16120 trainingloss 0.6916632528527511
iteration 8 batch 16130 trainingloss 0.6931471805599453
iteration 8 batch 16140 trainingloss 0.6931471805599453
iteration 8 batch 16150 trainingloss 0.6931471805599453
iteration 8 batch 16160 trainingloss 0.6931471805599453
iteration 8 batch 16170 trainingloss 0.6931471805599453
iteration 8 batch 16180 trainingloss 0.6931471805599453
iteration 8 batch 16190 trainingloss 0.6931471805599453
iteration 8 batch 16200 trainingloss 0.6931471805599453
iteration 8 batch 16210 trainingloss 0.6916632528527511
iteration 8 batch 16220 trainingloss 0.6931471805599453
iteration 8 batch 16230 trainingloss 0.6931471805599453
iteration 8 batch 16240 trainingloss 0.6931471805599453
iteration 8 batch 16250 trainingloss 0.6931471805599453
iteration 8 batch 16260 trainingloss 0.6931471805599453
iteration 8 batch 16270 trainingloss 0.6931471805599453
iteration 8 batch 16280 trainingloss 0.6931471805599453
iteration 8 batch 16290 trainingloss 0.6931471805599453
iteration 8 batch 16300 trainingloss 0.6931471805599453
iteration 8 batch 16310 trainingloss 0.6931471805599453
iteration 8 batch 16320 trainingloss 0.6931471805599453
iteration 8 batch 16330 trainingloss 0.6931471805599453
iteration 8 batch 16340 trainingloss 0.6931471805599453
iteration 8 batch 16350 trainingloss 0.6931471805599453
iteration 8 batch 16360 trainingloss 0.6931471805599453
iteration 8 batch 16370 trainingloss 0.6931471805599453
iteration 8 batch 16380 trainingloss 0.6931471805599453
iteration 8 batch 16390 trainingloss 0.6916632528527511
iteration 8 batch 16400 trainingloss 0.6931471805599453
iteration 8 batch 16410 trainingloss 0.6931471805599453
iteration 8 batch 16420 trainingloss 0.6931471805599453
iteration 8 batch 16430 trainingloss 0.6931471805599453
iteration 8 batch 16440 trainingloss 0.6931471805599453
iteration 8 batch 16450 trainingloss 0.6931471805599453
iteration 8 batch 16460 trainingloss 0.6931471805599453
iteration 8 batch 16470 trainingloss 0.6931471805599453
iteration 8 batch 16480 trainingloss 0.6931471805599453
iteration 8 batch 16490 trainingloss 0.6931471805599453
iteration 8 batch 16500 trainingloss 0.6931471805599453
iteration 8 batch 16510 trainingloss 0.6931471805599453
iteration 8 batch 16520 trainingloss 0.6931471805599453
iteration 8 batch 16530 trainingloss 0.6931471805599453
iteration 8 batch 16540 trainingloss 0.6931471805599453
iteration 8 batch 16550 trainingloss 0.6931471805599453
iteration 8 batch 16560 trainingloss 0.6931471805599453
iteration 8 batch 16570 trainingloss 0.6931471805599453
iteration 8 batch 16580 trainingloss 0.6931471805599453
iteration 8 batch 16590 trainingloss 0.6931471805599453
iteration 8 batch 16600 trainingloss 0.6931471805599453
iteration 8 batch 16610 trainingloss 0.6931471805599453
iteration 8 batch 16620 trainingloss 0.6931471805599453
iteration 8 batch 16630 trainingloss 0.6931471805599453
iteration 8 batch 16640 trainingloss 0.6916632528527511
iteration 8 batch 16650 trainingloss 0.6931471805599453
iteration 8 batch 16660 trainingloss 0.6931471805599453
iteration 8 batch 16670 trainingloss 0.6931471805599453
iteration 8 batch 16680 trainingloss 0.6931471805599453
iteration 8 batch 16690 trainingloss 0.6931471805599453
iteration 8 batch 16700 trainingloss 0.6931471805599453
iteration 8 batch 16710 trainingloss 0.6931471805599453
iteration 8 batch 16720 trainingloss 0.6931471805599453
iteration 8 batch 16730 trainingloss 0.6931471805599453
iteration 8 batch 16740 trainingloss 0.6931471805599453
iteration 8 batch 16750 trainingloss 0.6931471805599453
iteration 8 batch 16760 trainingloss 0.6931471805599453
iteration 8 batch 16770 trainingloss 0.6931471805599453
iteration 8 batch 16780 trainingloss 0.6931471805599453
iteration 8 batch 16790 trainingloss 0.6931471805599453
iteration 8 batch 16800 trainingloss 0.6931471805599453
iteration 8 batch 16810 trainingloss 0.6931471805599453
iteration 8 batch 16820 trainingloss 0.6931471805599453
iteration 8 batch 16830 trainingloss 0.6931471805599453
iteration 8 batch 16840 trainingloss 0.6931471805599453
iteration 8 batch 16850 trainingloss 0.6931471805599453
iteration 8 batch 16860 trainingloss 0.6931471805599453
iteration 8 batch 16870 trainingloss 0.6931471805599453
iteration 8 batch 16880 trainingloss 0.6931471805599453
iteration 8 batch 16890 trainingloss 0.6931471805599453
iteration 8 batch 16900 trainingloss 0.6916632528527511
iteration 8 batch 16910 trainingloss 0.6931471805599453
iteration 8 batch 16920 trainingloss 0.6931471805599453
iteration 8 batch 16930 trainingloss 0.6931471805599453
iteration 8 batch 16940 trainingloss 0.6931471805599453
iteration 8 batch 16950 trainingloss 0.6931471805599453
iteration 8 batch 16960 trainingloss 0.6931471805599453
iteration 8 batch 16970 trainingloss 0.6931471805599453
iteration 8 batch 16980 trainingloss 0.6931471805599453
iteration 8 batch 16990 trainingloss 0.6916632528527511
iteration 8 batch 17000 trainingloss 0.6931471805599453
iteration 8 batch 17010 trainingloss 0.6931471805599453
iteration 8 batch 17020 trainingloss 0.6931471805599453
iteration 8 batch 17030 trainingloss 0.6931471805599453
iteration 8 batch 17040 trainingloss 0.6931471805599453
iteration 8 batch 17050 trainingloss 0.6916632528527511
iteration 8 batch 17060 trainingloss 0.6931471805599453
iteration 8 batch 17070 trainingloss 0.6931471805599453
iteration 8 batch 17080 trainingloss 0.6931471805599453
iteration 8 batch 17090 trainingloss 0.6931471805599453
iteration 8 batch 17100 trainingloss 0.6931471805599453
iteration 8 batch 17110 trainingloss 0.6931471805599453
iteration 8 batch 17120 trainingloss 0.6931471805599453
iteration 8 batch 17130 trainingloss 0.6931471805599453
iteration 8 batch 17140 trainingloss 0.6931471805599453
iteration 8 batch 17150 trainingloss 0.6931471805599453
iteration 8 batch 17160 trainingloss 0.6931471805599453
iteration 8 batch 17170 trainingloss 0.6931471805599453
iteration 8 batch 17180 trainingloss 0.6931471805599453
iteration 8 batch 17190 trainingloss 0.6931471805599453
iteration 8 batch 17200 trainingloss 0.6931471805599453
iteration 8 batch 17210 trainingloss 0.6931471805599453
iteration 8 batch 17220 trainingloss 0.6931471805599453
iteration 8 batch 17230 trainingloss 0.6931471805599453
iteration 8 batch 17240 trainingloss 0.6931471805599453
iteration 8 batch 17250 trainingloss 0.6916632528527511
iteration 8 batch 17260 trainingloss 0.6931471805599453
iteration 8 batch 17270 trainingloss 0.6931471805599453
iteration 8 batch 17280 trainingloss 0.6931471805599453
iteration 8 batch 17290 trainingloss 0.6931471805599453
iteration 8 batch 17300 trainingloss 0.6931471805599453
iteration 8 batch 17310 trainingloss 0.6931471805599453
iteration 8 batch 17320 trainingloss 0.6931471805599453
iteration 8 batch 17330 trainingloss 0.6916632528527511
iteration 8 batch 17340 trainingloss 0.6931471805599453
iteration 8 batch 17350 trainingloss 0.6931471805599453
iteration 8 batch 17360 trainingloss 0.6931471805599453
iteration 8 batch 17370 trainingloss 0.6931471805599453
iteration 8 batch 17380 trainingloss 0.6931471805599453
iteration 8 batch 17390 trainingloss 0.6931471805599453
iteration 8 batch 17400 trainingloss 0.6931471805599453
iteration 8 batch 17410 trainingloss 0.6931471805599453
iteration 8 batch 17420 trainingloss 0.6931471805599453
iteration 8 batch 17430 trainingloss 0.6931471805599453
iteration 8 batch 17440 trainingloss 0.6931471805599453
iteration 8 batch 17450 trainingloss 0.6916632528527511
iteration 8 batch 17460 trainingloss 0.6931471805599453
iteration 8 batch 17470 trainingloss 0.6931471805599453
iteration 8 batch 17480 trainingloss 0.6931471805599453
iteration 8 batch 17490 trainingloss 0.6931471805599453
iteration 8 batch 17500 trainingloss 0.6916632528527511
iteration 8 batch 17510 trainingloss 0.6931471805599453
iteration 8 batch 17520 trainingloss 0.6931471805599453
iteration 8 batch 17530 trainingloss 0.6931471805599453
iteration 8 batch 17540 trainingloss 0.6931471805599453
iteration 8 batch 17550 trainingloss 0.6931471805599453
iteration 8 batch 17560 trainingloss 0.6931471805599453
iteration 8 batch 17570 trainingloss 0.6931471805599453
iteration 8 batch 17580 trainingloss 0.6931471805599453
iteration 8 batch 17590 trainingloss 0.6931471805599453
iteration 8 batch 17600 trainingloss 0.6931471805599453
iteration 8 batch 17610 trainingloss 0.6931471805599453
iteration 8 batch 17620 trainingloss 0.6931471805599453
iteration 8 batch 17630 trainingloss 0.6931471805599453
iteration 8 batch 17640 trainingloss 0.6931471805599453
iteration 8 batch 17650 trainingloss 0.6931471805599453
iteration 8 batch 17660 trainingloss 0.6931471805599453
iteration 8 batch 17670 trainingloss 0.6931471805599453
iteration 8 batch 17680 trainingloss 0.6931471805599453
iteration 8 batch 17690 trainingloss 0.6931471805599453
iteration 8 batch 17700 trainingloss 0.6931471805599453
iteration 8 batch 17710 trainingloss 0.6931471805599453
iteration 8 batch 17720 trainingloss 0.6931471805599453
iteration 8 batch 17730 trainingloss 0.6916632528527511
iteration 8 batch 17740 trainingloss 0.6931471805599453
iteration 8 batch 17750 trainingloss 0.6931471805599453
iteration 8 batch 17760 trainingloss 0.6931471805599453
iteration 8 batch 17770 trainingloss 0.6931471805599453
iteration 8 batch 17780 trainingloss 0.6931471805599453
iteration 8 batch 17790 trainingloss 0.6931471805599453
iteration 8 batch 17800 trainingloss 0.6931471805599453
iteration 8 batch 17810 trainingloss 0.6931471805599453
iteration 8 batch 17820 trainingloss 0.6931471805599453
iteration 8 batch 17830 trainingloss 0.6931471805599453
iteration 8 batch 17840 trainingloss 0.6931471805599453
iteration 8 batch 17850 trainingloss 0.6931471805599453
iteration 8 batch 17860 trainingloss 0.6931471805599453
iteration 8 batch 17870 trainingloss 0.6931471805599453
iteration 8 batch 17880 trainingloss 0.6931471805599453
iteration 8 batch 17890 trainingloss 0.6931471805599453
iteration 8 batch 17900 trainingloss 0.6931471805599453
iteration 8 batch 17910 trainingloss 0.6931471805599453
iteration 8 batch 17920 trainingloss 0.6931471805599453
iteration 8 batch 17930 trainingloss 0.6931471805599453
iteration 8 batch 17940 trainingloss 0.6931471805599453
iteration 8 batch 17950 trainingloss 0.6931471805599453
iteration 8 batch 17960 trainingloss 0.6931471805599453
iteration 8 batch 17970 trainingloss 0.6931471805599453
iteration 8 batch 17980 trainingloss 0.6931471805599453
iteration 8 batch 17990 trainingloss 0.6931471805599453
iteration 8 batch 18000 trainingloss 0.6931471805599453
iteration 8 batch 18010 trainingloss 0.6931471805599453
iteration 8 batch 18020 trainingloss 0.6931471805599453
iteration 8 batch 18030 trainingloss 0.6931471805599453
iteration 8 batch 18040 trainingloss 0.6931471805599453
iteration 8 batch 18050 trainingloss 0.6931471805599453
iteration 8 batch 18060 trainingloss 0.6931471805599453
iteration 8 batch 18070 trainingloss 0.6931471805599453
iteration 8 batch 18080 trainingloss 0.6931471805599453
iteration 8 batch 18090 trainingloss 0.6931471805599453
iteration 8 batch 18100 trainingloss 0.6931471805599453
iteration 8 batch 18110 trainingloss 0.6931471805599453
iteration 8 batch 18120 trainingloss 0.6931471805599453
iteration 8 batch 18130 trainingloss 0.6931471805599453
iteration 8 batch 18140 trainingloss 0.6931471805599453
iteration 8 batch 18150 trainingloss 0.6931471805599453
iteration 8 batch 18160 trainingloss 0.6931471805599453
iteration 8 batch 18170 trainingloss 0.6931471805599453
iteration 8 batch 18180 trainingloss 0.6916632528527511
iteration 8 batch 18190 trainingloss 0.6931471805599453
iteration 8 batch 18200 trainingloss 0.6931471805599453
iteration 8 batch 18210 trainingloss 0.6931471805599453
iteration 8 batch 18220 trainingloss 0.6931471805599453
iteration 8 batch 18230 trainingloss 0.6931471805599453
iteration 8 batch 18240 trainingloss 0.6931471805599453
iteration 8 batch 18250 trainingloss 0.6931471805599453
iteration 8 batch 18260 trainingloss 0.6931471805599453
iteration 8 batch 18270 trainingloss 0.6931471805599453
iteration 8 batch 18280 trainingloss 0.6931471805599453
iteration 8 batch 18290 trainingloss 0.6931471805599453
iteration 8 batch 18300 trainingloss 0.6931471805599453
iteration 8 batch 18310 trainingloss 0.6931471805599453
iteration 8 batch 18320 trainingloss 0.6931471805599453
iteration 8 batch 18330 trainingloss 0.6931471805599453
iteration 8 batch 18340 trainingloss 0.6931471805599453
iteration 8 batch 18350 trainingloss 0.6931471805599453
iteration 8 batch 18360 trainingloss 0.6931471805599453
iteration 8 batch 18370 trainingloss 0.6931471805599453
iteration 8 batch 18380 trainingloss 0.6931471805599453
iteration 8 batch 18390 trainingloss 0.6916632528527511
iteration 8 batch 18400 trainingloss 0.6931471805599453
iteration 8 batch 18410 trainingloss 0.6916632528527511
iteration 8 batch 18420 trainingloss 0.6931471805599453
iteration 8 batch 18430 trainingloss 0.6931471805599453
iteration 8 batch 18440 trainingloss 0.6931471805599453
iteration 8 batch 18450 trainingloss 0.6931471805599453
iteration 8 batch 18460 trainingloss 0.6931471805599453
iteration 8 batch 18470 trainingloss 0.6931471805599453
iteration 8 batch 18480 trainingloss 0.6931471805599453
iteration 8 batch 18490 trainingloss 0.6931471805599453
iteration 8 batch 18500 trainingloss 0.6931471805599453
iteration 8 batch 18510 trainingloss 0.6931471805599453
iteration 8 batch 18520 trainingloss 0.6931471805599453
iteration 8 batch 18530 trainingloss 0.6931471805599453
iteration 8 batch 18540 trainingloss 0.6931471805599453
iteration 8 batch 18550 trainingloss 0.6931471805599453
iteration 8 batch 18560 trainingloss 0.6931471805599453
iteration 8 batch 18570 trainingloss 0.6931471805599453
iteration 8 batch 18580 trainingloss 0.6931471805599453
iteration 8 batch 18590 trainingloss 0.6931471805599453
iteration 8 batch 18600 trainingloss 0.6931471805599453
iteration 8 batch 18610 trainingloss 0.6931471805599453
iteration 9 batch 0 trainingloss 0.6931471805599453
iteration 9 batch 10 trainingloss 0.6931471805599453
iteration 9 batch 20 trainingloss 0.6931471805599453
iteration 9 batch 30 trainingloss 0.6931471805599453
iteration 9 batch 40 trainingloss 0.6931471805599453
iteration 9 batch 50 trainingloss 0.6916632528527511
iteration 9 batch 60 trainingloss 0.6931471805599453
iteration 9 batch 70 trainingloss 0.6931471805599453
iteration 9 batch 80 trainingloss 0.6931471805599453
iteration 9 batch 90 trainingloss 0.6931471805599453
iteration 9 batch 100 trainingloss 0.6931471805599453
iteration 9 batch 110 trainingloss 0.6931471805599453
iteration 9 batch 120 trainingloss 0.6931471805599453
iteration 9 batch 130 trainingloss 0.6931471805599453
iteration 9 batch 140 trainingloss 0.6931471805599453
iteration 9 batch 150 trainingloss 0.6931471805599453
iteration 9 batch 160 trainingloss 0.6931471805599453
iteration 9 batch 170 trainingloss 0.6931471805599453
iteration 9 batch 180 trainingloss 0.6931471805599453
iteration 9 batch 190 trainingloss 0.6931471805599453
iteration 9 batch 200 trainingloss 0.6931471805599453
iteration 9 batch 210 trainingloss 0.6931471805599453
iteration 9 batch 220 trainingloss 0.6931471805599453
iteration 9 batch 230 trainingloss 0.6931471805599453
iteration 9 batch 240 trainingloss 0.6931471805599453
iteration 9 batch 250 trainingloss 0.6931471805599453
iteration 9 batch 260 trainingloss 0.6931471805599453
iteration 9 batch 270 trainingloss 0.6931471805599453
iteration 9 batch 280 trainingloss 0.6931471805599453
iteration 9 batch 290 trainingloss 0.6931471805599453
iteration 9 batch 300 trainingloss 0.6931471805599453
iteration 9 batch 310 trainingloss 0.6931471805599453
iteration 9 batch 320 trainingloss 0.6931471805599453
iteration 9 batch 330 trainingloss 0.6931471805599453
iteration 9 batch 340 trainingloss 0.6931471805599453
iteration 9 batch 350 trainingloss 0.6931471805599453
iteration 9 batch 360 trainingloss 0.6931471805599453
iteration 9 batch 370 trainingloss 0.6931471805599453
iteration 9 batch 380 trainingloss 0.6931471805599453
iteration 9 batch 390 trainingloss 0.6931471805599453
iteration 9 batch 400 trainingloss 0.6916632528527511
iteration 9 batch 410 trainingloss 0.6931471805599453
iteration 9 batch 420 trainingloss 0.6931471805599453
iteration 9 batch 430 trainingloss 0.6916632528527511
iteration 9 batch 440 trainingloss 0.6931471805599453
iteration 9 batch 450 trainingloss 0.6931471805599453
iteration 9 batch 460 trainingloss 0.6931471805599453
iteration 9 batch 470 trainingloss 0.6931471805599453
iteration 9 batch 480 trainingloss 0.6931471805599453
iteration 9 batch 490 trainingloss 0.6931471805599453
iteration 9 batch 500 trainingloss 0.6931471805599453
iteration 9 batch 510 trainingloss 0.6931471805599453
iteration 9 batch 520 trainingloss 0.6931471805599453
iteration 9 batch 530 trainingloss 0.6931471805599453
iteration 9 batch 540 trainingloss 0.6931471805599453
iteration 9 batch 550 trainingloss 0.6931471805599453
iteration 9 batch 560 trainingloss 0.6931471805599453
iteration 9 batch 570 trainingloss 0.6931471805599453
iteration 9 batch 580 trainingloss 0.6931471805599453
iteration 9 batch 590 trainingloss 0.6931471805599453
iteration 9 batch 600 trainingloss 0.6931471805599453
iteration 9 batch 610 trainingloss 0.6931471805599453
iteration 9 batch 620 trainingloss 0.6931471805599453
iteration 9 batch 630 trainingloss 0.6931471805599453
iteration 9 batch 640 trainingloss 0.6931471805599453
iteration 9 batch 650 trainingloss 0.6931471805599453
iteration 9 batch 660 trainingloss 0.6931471805599453
iteration 9 batch 670 trainingloss 0.6931471805599453
iteration 9 batch 680 trainingloss 0.6931471805599453
iteration 9 batch 690 trainingloss 0.6916632528527511
iteration 9 batch 700 trainingloss 0.6931471805599453
iteration 9 batch 710 trainingloss 0.6931471805599453
iteration 9 batch 720 trainingloss 0.6931471805599453
iteration 9 batch 730 trainingloss 0.6931471805599453
iteration 9 batch 740 trainingloss 0.6931471805599453
iteration 9 batch 750 trainingloss 0.6931471805599453
iteration 9 batch 760 trainingloss 0.6931471805599453
iteration 9 batch 770 trainingloss 0.6931471805599453
iteration 9 batch 780 trainingloss 0.6931471805599453
iteration 9 batch 790 trainingloss 0.6931471805599453
iteration 9 batch 800 trainingloss 0.6931471805599453
iteration 9 batch 810 trainingloss 0.6931471805599453
iteration 9 batch 820 trainingloss 0.6916632528527511
iteration 9 batch 830 trainingloss 0.6916632528527511
iteration 9 batch 840 trainingloss 0.6931471805599453
iteration 9 batch 850 trainingloss 0.6931471805599453
iteration 9 batch 860 trainingloss 0.6931471805599453
iteration 9 batch 870 trainingloss 0.6931471805599453
iteration 9 batch 880 trainingloss 0.6916632528527511
iteration 9 batch 890 trainingloss 0.6931471805599453
iteration 9 batch 900 trainingloss 0.6931471805599453
iteration 9 batch 910 trainingloss 0.6931471805599453
iteration 9 batch 920 trainingloss 0.6931471805599453
iteration 9 batch 930 trainingloss 0.6931471805599453
iteration 9 batch 940 trainingloss 0.6931471805599453
iteration 9 batch 950 trainingloss 0.6931471805599453
iteration 9 batch 960 trainingloss 0.6931471805599453
iteration 9 batch 970 trainingloss 0.6931471805599453
iteration 9 batch 980 trainingloss 0.6916632528527511
iteration 9 batch 990 trainingloss 0.6931471805599453
iteration 9 batch 1000 trainingloss 0.6931471805599453
iteration 9 batch 1010 trainingloss 0.6931471805599453
iteration 9 batch 1020 trainingloss 0.6931471805599453
iteration 9 batch 1030 trainingloss 0.6931471805599453
iteration 9 batch 1040 trainingloss 0.6931471805599453
iteration 9 batch 1050 trainingloss 0.6931471805599453
iteration 9 batch 1060 trainingloss 0.6931471805599453
iteration 9 batch 1070 trainingloss 0.6931471805599453
iteration 9 batch 1080 trainingloss 0.6931471805599453
iteration 9 batch 1090 trainingloss 0.6931471805599453
iteration 9 batch 1100 trainingloss 0.6931471805599453
iteration 9 batch 1110 trainingloss 0.6931471805599453
iteration 9 batch 1120 trainingloss 0.6931471805599453
iteration 9 batch 1130 trainingloss 0.6931471805599453
iteration 9 batch 1140 trainingloss 0.6931471805599453
iteration 9 batch 1150 trainingloss 0.6931471805599453
iteration 9 batch 1160 trainingloss 0.6931471805599453
iteration 9 batch 1170 trainingloss 0.6931471805599453
iteration 9 batch 1180 trainingloss 0.6931471805599453
iteration 9 batch 1190 trainingloss 0.6931471805599453
iteration 9 batch 1200 trainingloss 0.6931471805599453
iteration 9 batch 1210 trainingloss 0.6931471805599453
iteration 9 batch 1220 trainingloss 0.6931471805599453
iteration 9 batch 1230 trainingloss 0.6931471805599453
iteration 9 batch 1240 trainingloss 0.6931471805599453
iteration 9 batch 1250 trainingloss 0.6931471805599453
iteration 9 batch 1260 trainingloss 0.6931471805599453
iteration 9 batch 1270 trainingloss 0.6931471805599453
iteration 9 batch 1280 trainingloss 0.6931471805599453
iteration 9 batch 1290 trainingloss 0.6931471805599453
iteration 9 batch 1300 trainingloss 0.6931471805599453
iteration 9 batch 1310 trainingloss 0.6931471805599453
iteration 9 batch 1320 trainingloss 0.6931471805599453
iteration 9 batch 1330 trainingloss 0.6931471805599453
iteration 9 batch 1340 trainingloss 0.6931471805599453
iteration 9 batch 1350 trainingloss 0.6931471805599453
iteration 9 batch 1360 trainingloss 0.6931471805599453
iteration 9 batch 1370 trainingloss 0.6931471805599453
iteration 9 batch 1380 trainingloss 0.6931471805599453
iteration 9 batch 1390 trainingloss 0.6931471805599453
iteration 9 batch 1400 trainingloss 0.6931471805599453
iteration 9 batch 1410 trainingloss 0.6931471805599453
iteration 9 batch 1420 trainingloss 0.6931471805599453
iteration 9 batch 1430 trainingloss 0.6931471805599453
iteration 9 batch 1440 trainingloss 0.6931471805599453
iteration 9 batch 1450 trainingloss 0.6931471805599453
iteration 9 batch 1460 trainingloss 0.6931471805599453
iteration 9 batch 1470 trainingloss 0.6931471805599453
iteration 9 batch 1480 trainingloss 0.6931471805599453
iteration 9 batch 1490 trainingloss 0.6931471805599453
iteration 9 batch 1500 trainingloss 0.6931471805599453
iteration 9 batch 1510 trainingloss 0.6931471805599453
iteration 9 batch 1520 trainingloss 0.6931471805599453
iteration 9 batch 1530 trainingloss 0.6931471805599453
iteration 9 batch 1540 trainingloss 0.6931471805599453
iteration 9 batch 1550 trainingloss 0.6931471805599453
iteration 9 batch 1560 trainingloss 0.6931471805599453
iteration 9 batch 1570 trainingloss 0.6931471805599453
iteration 9 batch 1580 trainingloss 0.6931471805599453
iteration 9 batch 1590 trainingloss 0.6931471805599453
iteration 9 batch 1600 trainingloss 0.6931471805599453
iteration 9 batch 1610 trainingloss 0.6931471805599453
iteration 9 batch 1620 trainingloss 0.6931471805599453
iteration 9 batch 1630 trainingloss 0.6931471805599453
iteration 9 batch 1640 trainingloss 0.6931471805599453
iteration 9 batch 1650 trainingloss 0.6931471805599453
iteration 9 batch 1660 trainingloss 0.6931471805599453
iteration 9 batch 1670 trainingloss 0.6931471805599453
iteration 9 batch 1680 trainingloss 0.6931471805599453
iteration 9 batch 1690 trainingloss 0.6931471805599453
iteration 9 batch 1700 trainingloss 0.6931471805599453
iteration 9 batch 1710 trainingloss 0.6931471805599453
iteration 9 batch 1720 trainingloss 0.6931471805599453
iteration 9 batch 1730 trainingloss 0.6931471805599453
iteration 9 batch 1740 trainingloss 0.6931471805599453
iteration 9 batch 1750 trainingloss 0.6931471805599453
iteration 9 batch 1760 trainingloss 0.6931471805599453
iteration 9 batch 1770 trainingloss 0.6931471805599453
iteration 9 batch 1780 trainingloss 0.6931471805599453
iteration 9 batch 1790 trainingloss 0.6931471805599453
iteration 9 batch 1800 trainingloss 0.6931471805599453
iteration 9 batch 1810 trainingloss 0.6931471805599453
iteration 9 batch 1820 trainingloss 0.6931471805599453
iteration 9 batch 1830 trainingloss 0.6931471805599453
iteration 9 batch 1840 trainingloss 0.6931471805599453
iteration 9 batch 1850 trainingloss 0.6916632528527511
iteration 9 batch 1860 trainingloss 0.6931471805599453
iteration 9 batch 1870 trainingloss 0.6931471805599453
iteration 9 batch 1880 trainingloss 0.6931471805599453
iteration 9 batch 1890 trainingloss 0.6931471805599453
iteration 9 batch 1900 trainingloss 0.6931471805599453
iteration 9 batch 1910 trainingloss 0.6931471805599453
iteration 9 batch 1920 trainingloss 0.6931471805599453
iteration 9 batch 1930 trainingloss 0.6931471805599453
iteration 9 batch 1940 trainingloss 0.6931471805599453
iteration 9 batch 1950 trainingloss 0.6931471805599453
iteration 9 batch 1960 trainingloss 0.6931471805599453
iteration 9 batch 1970 trainingloss 0.6931471805599453
iteration 9 batch 1980 trainingloss 0.6931471805599453
iteration 9 batch 1990 trainingloss 0.6931471805599453
iteration 9 batch 2000 trainingloss 0.6931471805599453
iteration 9 batch 2010 trainingloss 0.6931471805599453
iteration 9 batch 2020 trainingloss 0.6931471805599453
iteration 9 batch 2030 trainingloss 0.6931471805599453
iteration 9 batch 2040 trainingloss 0.6931471805599453
iteration 9 batch 2050 trainingloss 0.6931471805599453
iteration 9 batch 2060 trainingloss 0.6931471805599453
iteration 9 batch 2070 trainingloss 0.6931471805599453
iteration 9 batch 2080 trainingloss 0.6931471805599453
iteration 9 batch 2090 trainingloss 0.6931471805599453
iteration 9 batch 2100 trainingloss 0.6931471805599453
iteration 9 batch 2110 trainingloss 0.6931471805599453
iteration 9 batch 2120 trainingloss 0.6931471805599453
iteration 9 batch 2130 trainingloss 0.6931471805599453
iteration 9 batch 2140 trainingloss 0.6931471805599453
iteration 9 batch 2150 trainingloss 0.6931471805599453
iteration 9 batch 2160 trainingloss 0.6931471805599453
iteration 9 batch 2170 trainingloss 0.6931471805599453
iteration 9 batch 2180 trainingloss 0.6931471805599453
iteration 9 batch 2190 trainingloss 0.6931471805599453
iteration 9 batch 2200 trainingloss 0.6931471805599453
iteration 9 batch 2210 trainingloss 0.6931471805599453
iteration 9 batch 2220 trainingloss 0.6931471805599453
iteration 9 batch 2230 trainingloss 0.6931471805599453
iteration 9 batch 2240 trainingloss 0.6931471805599453
iteration 9 batch 2250 trainingloss 0.6931471805599453
iteration 9 batch 2260 trainingloss 0.6931471805599453
iteration 9 batch 2270 trainingloss 0.6931471805599453
iteration 9 batch 2280 trainingloss 0.6931471805599453
iteration 9 batch 2290 trainingloss 0.6931471805599453
iteration 9 batch 2300 trainingloss 0.6931471805599453
iteration 9 batch 2310 trainingloss 0.6931471805599453
iteration 9 batch 2320 trainingloss 0.6931471805599453
iteration 9 batch 2330 trainingloss 0.6931471805599453
iteration 9 batch 2340 trainingloss 0.6931471805599453
iteration 9 batch 2350 trainingloss 0.6931471805599453
iteration 9 batch 2360 trainingloss 0.6931471805599453
iteration 9 batch 2370 trainingloss 0.6931471805599453
iteration 9 batch 2380 trainingloss 0.6931471805599453
iteration 9 batch 2390 trainingloss 0.6931471805599453
iteration 9 batch 2400 trainingloss 0.6931471805599453
iteration 9 batch 2410 trainingloss 0.6931471805599453
iteration 9 batch 2420 trainingloss 0.6931471805599453
iteration 9 batch 2430 trainingloss 0.6931471805599453
iteration 9 batch 2440 trainingloss 0.6931471805599453
iteration 9 batch 2450 trainingloss 0.6931471805599453
iteration 9 batch 2460 trainingloss 0.6931471805599453
iteration 9 batch 2470 trainingloss 0.6931471805599453
iteration 9 batch 2480 trainingloss 0.6931471805599453
iteration 9 batch 2490 trainingloss 0.6931471805599453
iteration 9 batch 2500 trainingloss 0.6931471805599453
iteration 9 batch 2510 trainingloss 0.6931471805599453
iteration 9 batch 2520 trainingloss 0.6931471805599453
iteration 9 batch 2530 trainingloss 0.6931471805599453
iteration 9 batch 2540 trainingloss 0.6931471805599453
iteration 9 batch 2550 trainingloss 0.6931471805599453
iteration 9 batch 2560 trainingloss 0.6931471805599453
iteration 9 batch 2570 trainingloss 0.6931471805599453
iteration 9 batch 2580 trainingloss 0.6931471805599453
iteration 9 batch 2590 trainingloss 0.6931471805599453
iteration 9 batch 2600 trainingloss 0.6931471805599453
iteration 9 batch 2610 trainingloss 0.6931471805599453
iteration 9 batch 2620 trainingloss 0.6931471805599453
iteration 9 batch 2630 trainingloss 0.6931471805599453
iteration 9 batch 2640 trainingloss 0.6931471805599453
iteration 9 batch 2650 trainingloss 0.6931471805599453
iteration 9 batch 2660 trainingloss 0.6931471805599453
iteration 9 batch 2670 trainingloss 0.6931471805599453
iteration 9 batch 2680 trainingloss 0.6931471805599453
iteration 9 batch 2690 trainingloss 0.6931471805599453
iteration 9 batch 2700 trainingloss 0.6931471805599453
iteration 9 batch 2710 trainingloss 0.6931471805599453
iteration 9 batch 2720 trainingloss 0.6931471805599453
iteration 9 batch 2730 trainingloss 0.6916632528527511
iteration 9 batch 2740 trainingloss 0.6931471805599453
iteration 9 batch 2750 trainingloss 0.6931471805599453
iteration 9 batch 2760 trainingloss 0.6931471805599453
iteration 9 batch 2770 trainingloss 0.6931471805599453
iteration 9 batch 2780 trainingloss 0.6931471805599453
iteration 9 batch 2790 trainingloss 0.6916632528527511
iteration 9 batch 2800 trainingloss 0.6931471805599453
iteration 9 batch 2810 trainingloss 0.6931471805599453
iteration 9 batch 2820 trainingloss 0.6931471805599453
iteration 9 batch 2830 trainingloss 0.6931471805599453
iteration 9 batch 2840 trainingloss 0.6931471805599453
iteration 9 batch 2850 trainingloss 0.6931471805599453
iteration 9 batch 2860 trainingloss 0.6931471805599453
iteration 9 batch 2870 trainingloss 0.6931471805599453
iteration 9 batch 2880 trainingloss 0.6931471805599453
iteration 9 batch 2890 trainingloss 0.6931471805599453
iteration 9 batch 2900 trainingloss 0.6931471805599453
iteration 9 batch 2910 trainingloss 0.6931471805599453
iteration 9 batch 2920 trainingloss 0.6916632528527511
iteration 9 batch 2930 trainingloss 0.6931471805599453
iteration 9 batch 2940 trainingloss 0.6931471805599453
iteration 9 batch 2950 trainingloss 0.6931471805599453
iteration 9 batch 2960 trainingloss 0.6931471805599453
iteration 9 batch 2970 trainingloss 0.6931471805599453
iteration 9 batch 2980 trainingloss 0.6931471805599453
iteration 9 batch 2990 trainingloss 0.6931471805599453
iteration 9 batch 3000 trainingloss 0.6931471805599453
iteration 9 batch 3010 trainingloss 0.6931471805599453
iteration 9 batch 3020 trainingloss 0.6931471805599453
iteration 9 batch 3030 trainingloss 0.6931471805599453
iteration 9 batch 3040 trainingloss 0.6931471805599453
iteration 9 batch 3050 trainingloss 0.6931471805599453
iteration 9 batch 3060 trainingloss 0.6931471805599453
iteration 9 batch 3070 trainingloss 0.6931471805599453
iteration 9 batch 3080 trainingloss 0.6931471805599453
iteration 9 batch 3090 trainingloss 0.6931471805599453
iteration 9 batch 3100 trainingloss 0.6931471805599453
iteration 9 batch 3110 trainingloss 0.6931471805599453
iteration 9 batch 3120 trainingloss 0.6931471805599453
iteration 9 batch 3130 trainingloss 0.6931471805599453
iteration 9 batch 3140 trainingloss 0.6931471805599453
iteration 9 batch 3150 trainingloss 0.6931471805599453
iteration 9 batch 3160 trainingloss 0.6931471805599453
iteration 9 batch 3170 trainingloss 0.6931471805599453
iteration 9 batch 3180 trainingloss 0.6931471805599453
iteration 9 batch 3190 trainingloss 0.6931471805599453
iteration 9 batch 3200 trainingloss 0.6931471805599453
iteration 9 batch 3210 trainingloss 0.6931471805599453
iteration 9 batch 3220 trainingloss 0.6931471805599453
iteration 9 batch 3230 trainingloss 0.6931471805599453
iteration 9 batch 3240 trainingloss 0.6916632528527511
iteration 9 batch 3250 trainingloss 0.6931471805599453
iteration 9 batch 3260 trainingloss 0.6931471805599453
iteration 9 batch 3270 trainingloss 0.6931471805599453
iteration 9 batch 3280 trainingloss 0.6931471805599453
iteration 9 batch 3290 trainingloss 0.6931471805599453
iteration 9 batch 3300 trainingloss 0.6931471805599453
iteration 9 batch 3310 trainingloss 0.6931471805599453
iteration 9 batch 3320 trainingloss 0.6931471805599453
iteration 9 batch 3330 trainingloss 0.6931471805599453
iteration 9 batch 3340 trainingloss 0.6931471805599453
iteration 9 batch 3350 trainingloss 0.6931471805599453
iteration 9 batch 3360 trainingloss 0.6931471805599453
iteration 9 batch 3370 trainingloss 0.6931471805599453
iteration 9 batch 3380 trainingloss 0.6931471805599453
iteration 9 batch 3390 trainingloss 0.6931471805599453
iteration 9 batch 3400 trainingloss 0.6931471805599453
iteration 9 batch 3410 trainingloss 0.6931471805599453
iteration 9 batch 3420 trainingloss 0.6931471805599453
iteration 9 batch 3430 trainingloss 0.6931471805599453
iteration 9 batch 3440 trainingloss 0.6931471805599453
iteration 9 batch 3450 trainingloss 0.6931471805599453
iteration 9 batch 3460 trainingloss 0.6931471805599453
iteration 9 batch 3470 trainingloss 0.6931471805599453
iteration 9 batch 3480 trainingloss 0.6931471805599453
iteration 9 batch 3490 trainingloss 0.6931471805599453
iteration 9 batch 3500 trainingloss 0.6931471805599453
iteration 9 batch 3510 trainingloss 0.6931471805599453
iteration 9 batch 3520 trainingloss 0.6931471805599453
iteration 9 batch 3530 trainingloss 0.6931471805599453
iteration 9 batch 3540 trainingloss 0.6931471805599453
iteration 9 batch 3550 trainingloss 0.6931471805599453
iteration 9 batch 3560 trainingloss 0.6931471805599453
iteration 9 batch 3570 trainingloss 0.6931471805599453
iteration 9 batch 3580 trainingloss 0.6931471805599453
iteration 9 batch 3590 trainingloss 0.6916632528527511
iteration 9 batch 3600 trainingloss 0.6931471805599453
iteration 9 batch 3610 trainingloss 0.6931471805599453
iteration 9 batch 3620 trainingloss 0.6931471805599453
iteration 9 batch 3630 trainingloss 0.6916632528527511
iteration 9 batch 3640 trainingloss 0.6931471805599453
iteration 9 batch 3650 trainingloss 0.6931471805599453
iteration 9 batch 3660 trainingloss 0.6931471805599453
iteration 9 batch 3670 trainingloss 0.6931471805599453
iteration 9 batch 3680 trainingloss 0.6931471805599453
iteration 9 batch 3690 trainingloss 0.6931471805599453
iteration 9 batch 3700 trainingloss 0.6931471805599453
iteration 9 batch 3710 trainingloss 0.6916632528527511
iteration 9 batch 3720 trainingloss 0.6931471805599453
iteration 9 batch 3730 trainingloss 0.6931471805599453
iteration 9 batch 3740 trainingloss 0.6931471805599453
iteration 9 batch 3750 trainingloss 0.6931471805599453
iteration 9 batch 3760 trainingloss 0.6916632528527511
iteration 9 batch 3770 trainingloss 0.6931471805599453
iteration 9 batch 3780 trainingloss 0.6931471805599453
iteration 9 batch 3790 trainingloss 0.6916632528527511
iteration 9 batch 3800 trainingloss 0.6931471805599453
iteration 9 batch 3810 trainingloss 0.6931471805599453
iteration 9 batch 3820 trainingloss 0.6931471805599453
iteration 9 batch 3830 trainingloss 0.6931471805599453
iteration 9 batch 3840 trainingloss 0.6931471805599453
iteration 9 batch 3850 trainingloss 0.6931471805599453
iteration 9 batch 3860 trainingloss 0.6931471805599453
iteration 9 batch 3870 trainingloss 0.6931471805599453
iteration 9 batch 3880 trainingloss 0.6931471805599453
iteration 9 batch 3890 trainingloss 0.6931471805599453
iteration 9 batch 3900 trainingloss 0.6931471805599453
iteration 9 batch 3910 trainingloss 0.6931471805599453
iteration 9 batch 3920 trainingloss 0.6931471805599453
iteration 9 batch 3930 trainingloss 0.6931471805599453
iteration 9 batch 3940 trainingloss 0.6931471805599453
iteration 9 batch 3950 trainingloss 0.6931471805599453
iteration 9 batch 3960 trainingloss 0.6931471805599453
iteration 9 batch 3970 trainingloss 0.6931471805599453
iteration 9 batch 3980 trainingloss 0.6931471805599453
iteration 9 batch 3990 trainingloss 0.6931471805599453
iteration 9 batch 4000 trainingloss 0.6931471805599453
iteration 9 batch 4010 trainingloss 0.6931471805599453
iteration 9 batch 4020 trainingloss 0.6931471805599453
iteration 9 batch 4030 trainingloss 0.6931471805599453
iteration 9 batch 4040 trainingloss 0.6931471805599453
iteration 9 batch 4050 trainingloss 0.6931471805599453
iteration 9 batch 4060 trainingloss 0.6931471805599453
iteration 9 batch 4070 trainingloss 0.6931471805599453
iteration 9 batch 4080 trainingloss 0.6931471805599453
iteration 9 batch 4090 trainingloss 0.6931471805599453
iteration 9 batch 4100 trainingloss 0.6931471805599453
iteration 9 batch 4110 trainingloss 0.6931471805599453
iteration 9 batch 4120 trainingloss 0.6931471805599453
iteration 9 batch 4130 trainingloss 0.6931471805599453
iteration 9 batch 4140 trainingloss 0.6931471805599453
iteration 9 batch 4150 trainingloss 0.6931471805599453
iteration 9 batch 4160 trainingloss 0.6931471805599453
iteration 9 batch 4170 trainingloss 0.6931471805599453
iteration 9 batch 4180 trainingloss 0.6931471805599453
iteration 9 batch 4190 trainingloss 0.6931471805599453
iteration 9 batch 4200 trainingloss 0.6931471805599453
iteration 9 batch 4210 trainingloss 0.6931471805599453
iteration 9 batch 4220 trainingloss 0.6931471805599453
iteration 9 batch 4230 trainingloss 0.6931471805599453
iteration 9 batch 4240 trainingloss 0.6931471805599453
iteration 9 batch 4250 trainingloss 0.6931471805599453
iteration 9 batch 4260 trainingloss 0.6931471805599453
iteration 9 batch 4270 trainingloss 0.6931471805599453
iteration 9 batch 4280 trainingloss 0.6931471805599453
iteration 9 batch 4290 trainingloss 0.6916632528527511
iteration 9 batch 4300 trainingloss 0.6931471805599453
iteration 9 batch 4310 trainingloss 0.6931471805599453
iteration 9 batch 4320 trainingloss 0.6931471805599453
iteration 9 batch 4330 trainingloss 0.6931471805599453
iteration 9 batch 4340 trainingloss 0.6931471805599453
iteration 9 batch 4350 trainingloss 0.6931471805599453
iteration 9 batch 4360 trainingloss 0.6931471805599453
iteration 9 batch 4370 trainingloss 0.6931471805599453
iteration 9 batch 4380 trainingloss 0.6931471805599453
iteration 9 batch 4390 trainingloss 0.6931471805599453
iteration 9 batch 4400 trainingloss 0.6931471805599453
iteration 9 batch 4410 trainingloss 0.6931471805599453
iteration 9 batch 4420 trainingloss 0.6931471805599453
iteration 9 batch 4430 trainingloss 0.6931471805599453
iteration 9 batch 4440 trainingloss 0.6931471805599453
iteration 9 batch 4450 trainingloss 0.6931471805599453
iteration 9 batch 4460 trainingloss 0.6931471805599453
iteration 9 batch 4470 trainingloss 0.6931471805599453
iteration 9 batch 4480 trainingloss 0.6931471805599453
iteration 9 batch 4490 trainingloss 0.6931471805599453
iteration 9 batch 4500 trainingloss 0.6931471805599453
iteration 9 batch 4510 trainingloss 0.6931471805599453
iteration 9 batch 4520 trainingloss 0.6931471805599453
iteration 9 batch 4530 trainingloss 0.6931471805599453
iteration 9 batch 4540 trainingloss 0.6931471805599453
iteration 9 batch 4550 trainingloss 0.6916632528527511
iteration 9 batch 4560 trainingloss 0.6931471805599453
iteration 9 batch 4570 trainingloss 0.6931471805599453
iteration 9 batch 4580 trainingloss 0.6931471805599453
iteration 9 batch 4590 trainingloss 0.6931471805599453
iteration 9 batch 4600 trainingloss 0.6931471805599453
iteration 9 batch 4610 trainingloss 0.6916632528527511
iteration 9 batch 4620 trainingloss 0.6931471805599453
iteration 9 batch 4630 trainingloss 0.6931471805599453
iteration 9 batch 4640 trainingloss 0.6931471805599453
iteration 9 batch 4650 trainingloss 0.6931471805599453
iteration 9 batch 4660 trainingloss 0.6931471805599453
iteration 9 batch 4670 trainingloss 0.6931471805599453
iteration 9 batch 4680 trainingloss 0.6931471805599453
iteration 9 batch 4690 trainingloss 0.6931471805599453
iteration 9 batch 4700 trainingloss 0.6931471805599453
iteration 9 batch 4710 trainingloss 0.6931471805599453
iteration 9 batch 4720 trainingloss 0.6931471805599453
iteration 9 batch 4730 trainingloss 0.6931471805599453
iteration 9 batch 4740 trainingloss 0.6931471805599453
iteration 9 batch 4750 trainingloss 0.6931471805599453
iteration 9 batch 4760 trainingloss 0.6931471805599453
iteration 9 batch 4770 trainingloss 0.6931471805599453
iteration 9 batch 4780 trainingloss 0.6931471805599453
iteration 9 batch 4790 trainingloss 0.6916632528527511
iteration 9 batch 4800 trainingloss 0.6931471805599453
iteration 9 batch 4810 trainingloss 0.6931471805599453
iteration 9 batch 4820 trainingloss 0.6931471805599453
iteration 9 batch 4830 trainingloss 0.6931471805599453
iteration 9 batch 4840 trainingloss 0.6931471805599453
iteration 9 batch 4850 trainingloss 0.6931471805599453
iteration 9 batch 4860 trainingloss 0.6931471805599453
iteration 9 batch 4870 trainingloss 0.6931471805599453
iteration 9 batch 4880 trainingloss 0.6931471805599453
iteration 9 batch 4890 trainingloss 0.6931471805599453
iteration 9 batch 4900 trainingloss 0.6931471805599453
iteration 9 batch 4910 trainingloss 0.6931471805599453
iteration 9 batch 4920 trainingloss 0.6931471805599453
iteration 9 batch 4930 trainingloss 0.6931471805599453
iteration 9 batch 4940 trainingloss 0.6931471805599453
iteration 9 batch 4950 trainingloss 0.6931471805599453
iteration 9 batch 4960 trainingloss 0.6931471805599453
iteration 9 batch 4970 trainingloss 0.6931471805599453
iteration 9 batch 4980 trainingloss 0.6931471805599453
iteration 9 batch 4990 trainingloss 0.6931471805599453
iteration 9 batch 5000 trainingloss 0.6931471805599453
iteration 9 batch 5010 trainingloss 0.6931471805599453
iteration 9 batch 5020 trainingloss 0.6931471805599453
iteration 9 batch 5030 trainingloss 0.6931471805599453
iteration 9 batch 5040 trainingloss 0.6931471805599453
iteration 9 batch 5050 trainingloss 0.6916632528527511
iteration 9 batch 5060 trainingloss 0.6931471805599453
iteration 9 batch 5070 trainingloss 0.6931471805599453
iteration 9 batch 5080 trainingloss 0.6931471805599453
iteration 9 batch 5090 trainingloss 0.6916632528527511
iteration 9 batch 5100 trainingloss 0.6931471805599453
iteration 9 batch 5110 trainingloss 0.6931471805599453
iteration 9 batch 5120 trainingloss 0.6931471805599453
iteration 9 batch 5130 trainingloss 0.6931471805599453
iteration 9 batch 5140 trainingloss 0.6931471805599453
iteration 9 batch 5150 trainingloss 0.6931471805599453
iteration 9 batch 5160 trainingloss 0.6931471805599453
iteration 9 batch 5170 trainingloss 0.6931471805599453
iteration 9 batch 5180 trainingloss 0.6931471805599453
iteration 9 batch 5190 trainingloss 0.6931471805599453
iteration 9 batch 5200 trainingloss 0.6931471805599453
iteration 9 batch 5210 trainingloss 0.6916632528527511
iteration 9 batch 5220 trainingloss 0.6931471805599453
iteration 9 batch 5230 trainingloss 0.6931471805599453
iteration 9 batch 5240 trainingloss 0.6931471805599453
iteration 9 batch 5250 trainingloss 0.6931471805599453
iteration 9 batch 5260 trainingloss 0.6916632528527511
iteration 9 batch 5270 trainingloss 0.6931471805599453
iteration 9 batch 5280 trainingloss 0.6931471805599453
iteration 9 batch 5290 trainingloss 0.6931471805599453
iteration 9 batch 5300 trainingloss 0.6931471805599453
iteration 9 batch 5310 trainingloss 0.6931471805599453
iteration 9 batch 5320 trainingloss 0.6931471805599453
iteration 9 batch 5330 trainingloss 0.6931471805599453
iteration 9 batch 5340 trainingloss 0.6931471805599453
iteration 9 batch 5350 trainingloss 0.6931471805599453
iteration 9 batch 5360 trainingloss 0.6931471805599453
iteration 9 batch 5370 trainingloss 0.6931471805599453
iteration 9 batch 5380 trainingloss 0.6931471805599453
iteration 9 batch 5390 trainingloss 0.6931471805599453
iteration 9 batch 5400 trainingloss 0.6931471805599453
iteration 9 batch 5410 trainingloss 0.6931471805599453
iteration 9 batch 5420 trainingloss 0.6931471805599453
iteration 9 batch 5430 trainingloss 0.6931471805599453
iteration 9 batch 5440 trainingloss 0.6931471805599453
iteration 9 batch 5450 trainingloss 0.6931471805599453
iteration 9 batch 5460 trainingloss 0.6931471805599453
iteration 9 batch 5470 trainingloss 0.6931471805599453
iteration 9 batch 5480 trainingloss 0.6931471805599453
iteration 9 batch 5490 trainingloss 0.6916632528527511
iteration 9 batch 5500 trainingloss 0.6931471805599453
iteration 9 batch 5510 trainingloss 0.6931471805599453
iteration 9 batch 5520 trainingloss 0.6931471805599453
iteration 9 batch 5530 trainingloss 0.6931471805599453
iteration 9 batch 5540 trainingloss 0.6931471805599453
iteration 9 batch 5550 trainingloss 0.6931471805599453
iteration 9 batch 5560 trainingloss 0.6931471805599453
iteration 9 batch 5570 trainingloss 0.6931471805599453
iteration 9 batch 5580 trainingloss 0.6916632528527511
iteration 9 batch 5590 trainingloss 0.6931471805599453
iteration 9 batch 5600 trainingloss 0.6931471805599453
iteration 9 batch 5610 trainingloss 0.6931471805599453
iteration 9 batch 5620 trainingloss 0.6931471805599453
iteration 9 batch 5630 trainingloss 0.6931471805599453
iteration 9 batch 5640 trainingloss 0.6931471805599453
iteration 9 batch 5650 trainingloss 0.6931471805599453
iteration 9 batch 5660 trainingloss 0.6931471805599453
iteration 9 batch 5670 trainingloss 0.6931471805599453
iteration 9 batch 5680 trainingloss 0.6931471805599453
iteration 9 batch 5690 trainingloss 0.6931471805599453
iteration 9 batch 5700 trainingloss 0.6931471805599453
iteration 9 batch 5710 trainingloss 0.6931471805599453
iteration 9 batch 5720 trainingloss 0.6931471805599453
iteration 9 batch 5730 trainingloss 0.6931471805599453
iteration 9 batch 5740 trainingloss 0.6931471805599453
iteration 9 batch 5750 trainingloss 0.6931471805599453
iteration 9 batch 5760 trainingloss 0.6931471805599453
iteration 9 batch 5770 trainingloss 0.6931471805599453
iteration 9 batch 5780 trainingloss 0.6916632528527511
iteration 9 batch 5790 trainingloss 0.6931471805599453
iteration 9 batch 5800 trainingloss 0.6931471805599453
iteration 9 batch 5810 trainingloss 0.6916632528527511
iteration 9 batch 5820 trainingloss 0.6931471805599453
iteration 9 batch 5830 trainingloss 0.6931471805599453
iteration 9 batch 5840 trainingloss 0.6931471805599453
iteration 9 batch 5850 trainingloss 0.6931471805599453
iteration 9 batch 5860 trainingloss 0.6931471805599453
iteration 9 batch 5870 trainingloss 0.6931471805599453
iteration 9 batch 5880 trainingloss 0.6931471805599453
iteration 9 batch 5890 trainingloss 0.6931471805599453
iteration 9 batch 5900 trainingloss 0.6931471805599453
iteration 9 batch 5910 trainingloss 0.6931471805599453
iteration 9 batch 5920 trainingloss 0.6931471805599453
iteration 9 batch 5930 trainingloss 0.6931471805599453
iteration 9 batch 5940 trainingloss 0.6931471805599453
iteration 9 batch 5950 trainingloss 0.6931471805599453
iteration 9 batch 5960 trainingloss 0.6931471805599453
iteration 9 batch 5970 trainingloss 0.6931471805599453
iteration 9 batch 5980 trainingloss 0.6931471805599453
iteration 9 batch 5990 trainingloss 0.6931471805599453
iteration 9 batch 6000 trainingloss 0.6931471805599453
iteration 9 batch 6010 trainingloss 0.6931471805599453
iteration 9 batch 6020 trainingloss 0.6931471805599453
iteration 9 batch 6030 trainingloss 0.6931471805599453
iteration 9 batch 6040 trainingloss 0.6931471805599453
iteration 9 batch 6050 trainingloss 0.6931471805599453
iteration 9 batch 6060 trainingloss 0.6931471805599453
iteration 9 batch 6070 trainingloss 0.6931471805599453
iteration 9 batch 6080 trainingloss 0.6931471805599453
iteration 9 batch 6090 trainingloss 0.6931471805599453
iteration 9 batch 6100 trainingloss 0.6931471805599453
iteration 9 batch 6110 trainingloss 0.6931471805599453
iteration 9 batch 6120 trainingloss 0.6931471805599453
iteration 9 batch 6130 trainingloss 0.6931471805599453
iteration 9 batch 6140 trainingloss 0.6931471805599453
iteration 9 batch 6150 trainingloss 0.6931471805599453
iteration 9 batch 6160 trainingloss 0.6931471805599453
iteration 9 batch 6170 trainingloss 0.6931471805599453
iteration 9 batch 6180 trainingloss 0.6931471805599453
iteration 9 batch 6190 trainingloss 0.6931471805599453
iteration 9 batch 6200 trainingloss 0.6931471805599453
iteration 9 batch 6210 trainingloss 0.6931471805599453
iteration 9 batch 6220 trainingloss 0.6931471805599453
iteration 9 batch 6230 trainingloss 0.6916632528527511
iteration 9 batch 6240 trainingloss 0.6916632528527511
iteration 9 batch 6250 trainingloss 0.6931471805599453
iteration 9 batch 6260 trainingloss 0.6931471805599453
iteration 9 batch 6270 trainingloss 0.6931471805599453
iteration 9 batch 6280 trainingloss 0.6931471805599453
iteration 9 batch 6290 trainingloss 0.6901793251455568
iteration 9 batch 6300 trainingloss 0.6931471805599453
iteration 9 batch 6310 trainingloss 0.6931471805599453
iteration 9 batch 6320 trainingloss 0.6931471805599453
iteration 9 batch 6330 trainingloss 0.6931471805599453
iteration 9 batch 6340 trainingloss 0.6931471805599453
iteration 9 batch 6350 trainingloss 0.6931471805599453
iteration 9 batch 6360 trainingloss 0.6931471805599453
iteration 9 batch 6370 trainingloss 0.6931471805599453
iteration 9 batch 6380 trainingloss 0.6931471805599453
iteration 9 batch 6390 trainingloss 0.6931471805599453
iteration 9 batch 6400 trainingloss 0.6931471805599453
iteration 9 batch 6410 trainingloss 0.6931471805599453
iteration 9 batch 6420 trainingloss 0.6931471805599453
iteration 9 batch 6430 trainingloss 0.6931471805599453
iteration 9 batch 6440 trainingloss 0.6931471805599453
iteration 9 batch 6450 trainingloss 0.6931471805599453
iteration 9 batch 6460 trainingloss 0.6931471805599453
iteration 9 batch 6470 trainingloss 0.6931471805599453
iteration 9 batch 6480 trainingloss 0.6931471805599453
iteration 9 batch 6490 trainingloss 0.6931471805599453
iteration 9 batch 6500 trainingloss 0.6916632528527511
iteration 9 batch 6510 trainingloss 0.6931471805599453
iteration 9 batch 6520 trainingloss 0.6931471805599453
iteration 9 batch 6530 trainingloss 0.6931471805599453
iteration 9 batch 6540 trainingloss 0.6931471805599453
iteration 9 batch 6550 trainingloss 0.6931471805599453
iteration 9 batch 6560 trainingloss 0.6931471805599453
iteration 9 batch 6570 trainingloss 0.6931471805599453
iteration 9 batch 6580 trainingloss 0.6931471805599453
iteration 9 batch 6590 trainingloss 0.6931471805599453
iteration 9 batch 6600 trainingloss 0.6931471805599453
iteration 9 batch 6610 trainingloss 0.6931471805599453
iteration 9 batch 6620 trainingloss 0.6931471805599453
iteration 9 batch 6630 trainingloss 0.6931471805599453
iteration 9 batch 6640 trainingloss 0.6931471805599453
iteration 9 batch 6650 trainingloss 0.6931471805599453
iteration 9 batch 6660 trainingloss 0.6931471805599453
iteration 9 batch 6670 trainingloss 0.6931471805599453
iteration 9 batch 6680 trainingloss 0.6931471805599453
iteration 9 batch 6690 trainingloss 0.6931471805599453
iteration 9 batch 6700 trainingloss 0.6931471805599453
iteration 9 batch 6710 trainingloss 0.6931471805599453
iteration 9 batch 6720 trainingloss 0.6931471805599453
iteration 9 batch 6730 trainingloss 0.6931471805599453
iteration 9 batch 6740 trainingloss 0.6931471805599453
iteration 9 batch 6750 trainingloss 0.6931471805599453
iteration 9 batch 6760 trainingloss 0.6931471805599453
iteration 9 batch 6770 trainingloss 0.6931471805599453
iteration 9 batch 6780 trainingloss 0.6931471805599453
iteration 9 batch 6790 trainingloss 0.6931471805599453
iteration 9 batch 6800 trainingloss 0.6931471805599453
iteration 9 batch 6810 trainingloss 0.6931471805599453
iteration 9 batch 6820 trainingloss 0.6931471805599453
iteration 9 batch 6830 trainingloss 0.6931471805599453
iteration 9 batch 6840 trainingloss 0.6931471805599453
iteration 9 batch 6850 trainingloss 0.6931471805599453
iteration 9 batch 6860 trainingloss 0.6931471805599453
iteration 9 batch 6870 trainingloss 0.6931471805599453
iteration 9 batch 6880 trainingloss 0.6931471805599453
iteration 9 batch 6890 trainingloss 0.6931471805599453
iteration 9 batch 6900 trainingloss 0.6931471805599453
iteration 9 batch 6910 trainingloss 0.6931471805599453
iteration 9 batch 6920 trainingloss 0.6931471805599453
iteration 9 batch 6930 trainingloss 0.6931471805599453
iteration 9 batch 6940 trainingloss 0.6931471805599453
iteration 9 batch 6950 trainingloss 0.6931471805599453
iteration 9 batch 6960 trainingloss 0.6931471805599453
iteration 9 batch 6970 trainingloss 0.6931471805599453
iteration 9 batch 6980 trainingloss 0.6931471805599453
iteration 9 batch 6990 trainingloss 0.6931471805599453
iteration 9 batch 7000 trainingloss 0.6931471805599453
iteration 9 batch 7010 trainingloss 0.6931471805599453
iteration 9 batch 7020 trainingloss 0.6931471805599453
iteration 9 batch 7030 trainingloss 0.6931471805599453
iteration 9 batch 7040 trainingloss 0.6931471805599453
iteration 9 batch 7050 trainingloss 0.6931471805599453
iteration 9 batch 7060 trainingloss 0.6931471805599453
iteration 9 batch 7070 trainingloss 0.6931471805599453
iteration 9 batch 7080 trainingloss 0.6931471805599453
iteration 9 batch 7090 trainingloss 0.6931471805599453
iteration 9 batch 7100 trainingloss 0.6931471805599453
iteration 9 batch 7110 trainingloss 0.6931471805599453
iteration 9 batch 7120 trainingloss 0.6931471805599453
iteration 9 batch 7130 trainingloss 0.6931471805599453
iteration 9 batch 7140 trainingloss 0.6931471805599453
iteration 9 batch 7150 trainingloss 0.6931471805599453
iteration 9 batch 7160 trainingloss 0.6931471805599453
iteration 9 batch 7170 trainingloss 0.6931471805599453
iteration 9 batch 7180 trainingloss 0.6931471805599453
iteration 9 batch 7190 trainingloss 0.6931471805599453
iteration 9 batch 7200 trainingloss 0.6931471805599453
iteration 9 batch 7210 trainingloss 0.6931471805599453
iteration 9 batch 7220 trainingloss 0.6931471805599453
iteration 9 batch 7230 trainingloss 0.6931471805599453
iteration 9 batch 7240 trainingloss 0.6931471805599453
iteration 9 batch 7250 trainingloss 0.6931471805599453
iteration 9 batch 7260 trainingloss 0.6931471805599453
iteration 9 batch 7270 trainingloss 0.6931471805599453
iteration 9 batch 7280 trainingloss 0.6931471805599453
iteration 9 batch 7290 trainingloss 0.6931471805599453
iteration 9 batch 7300 trainingloss 0.6931471805599453
iteration 9 batch 7310 trainingloss 0.6931471805599453
iteration 9 batch 7320 trainingloss 0.6931471805599453
iteration 9 batch 7330 trainingloss 0.6931471805599453
iteration 9 batch 7340 trainingloss 0.6931471805599453
iteration 9 batch 7350 trainingloss 0.6931471805599453
iteration 9 batch 7360 trainingloss 0.6931471805599453
iteration 9 batch 7370 trainingloss 0.6916632528527511
iteration 9 batch 7380 trainingloss 0.6931471805599453
iteration 9 batch 7390 trainingloss 0.6931471805599453
iteration 9 batch 7400 trainingloss 0.6931471805599453
iteration 9 batch 7410 trainingloss 0.6931471805599453
iteration 9 batch 7420 trainingloss 0.6931471805599453
iteration 9 batch 7430 trainingloss 0.6931471805599453
iteration 9 batch 7440 trainingloss 0.6931471805599453
iteration 9 batch 7450 trainingloss 0.6931471805599453
iteration 9 batch 7460 trainingloss 0.6916632528527511
iteration 9 batch 7470 trainingloss 0.6931471805599453
iteration 9 batch 7480 trainingloss 0.6931471805599453
iteration 9 batch 7490 trainingloss 0.6931471805599453
iteration 9 batch 7500 trainingloss 0.6931471805599453
iteration 9 batch 7510 trainingloss 0.6931471805599453
iteration 9 batch 7520 trainingloss 0.6931471805599453
iteration 9 batch 7530 trainingloss 0.6931471805599453
iteration 9 batch 7540 trainingloss 0.6931471805599453
iteration 9 batch 7550 trainingloss 0.6931471805599453
iteration 9 batch 7560 trainingloss 0.6931471805599453
iteration 9 batch 7570 trainingloss 0.6931471805599453
iteration 9 batch 7580 trainingloss 0.6931471805599453
iteration 9 batch 7590 trainingloss 0.6931471805599453
iteration 9 batch 7600 trainingloss 0.6931471805599453
iteration 9 batch 7610 trainingloss 0.6931471805599453
iteration 9 batch 7620 trainingloss 0.6931471805599453
iteration 9 batch 7630 trainingloss 0.6931471805599453
iteration 9 batch 7640 trainingloss 0.6931471805599453
iteration 9 batch 7650 trainingloss 0.6931471805599453
iteration 9 batch 7660 trainingloss 0.6931471805599453
iteration 9 batch 7670 trainingloss 0.6931471805599453
iteration 9 batch 7680 trainingloss 0.6931471805599453
iteration 9 batch 7690 trainingloss 0.6916632528527511
iteration 9 batch 7700 trainingloss 0.6931471805599453
iteration 9 batch 7710 trainingloss 0.6931471805599453
iteration 9 batch 7720 trainingloss 0.6916632528527511
iteration 9 batch 7730 trainingloss 0.6931471805599453
iteration 9 batch 7740 trainingloss 0.6931471805599453
iteration 9 batch 7750 trainingloss 0.6931471805599453
iteration 9 batch 7760 trainingloss 0.6916632528527511
iteration 9 batch 7770 trainingloss 0.6931471805599453
iteration 9 batch 7780 trainingloss 0.6931471805599453
iteration 9 batch 7790 trainingloss 0.6931471805599453
iteration 9 batch 7800 trainingloss 0.6931471805599453
iteration 9 batch 7810 trainingloss 0.6931471805599453
iteration 9 batch 7820 trainingloss 0.6931471805599453
iteration 9 batch 7830 trainingloss 0.6931471805599453
iteration 9 batch 7840 trainingloss 0.6931471805599453
iteration 9 batch 7850 trainingloss 0.6931471805599453
iteration 9 batch 7860 trainingloss 0.6931471805599453
iteration 9 batch 7870 trainingloss 0.6931471805599453
iteration 9 batch 7880 trainingloss 0.6931471805599453
iteration 9 batch 7890 trainingloss 0.6931471805599453
iteration 9 batch 7900 trainingloss 0.6931471805599453
iteration 9 batch 7910 trainingloss 0.6931471805599453
iteration 9 batch 7920 trainingloss 0.6931471805599453
iteration 9 batch 7930 trainingloss 0.6931471805599453
iteration 9 batch 7940 trainingloss 0.6931471805599453
iteration 9 batch 7950 trainingloss 0.6931471805599453
iteration 9 batch 7960 trainingloss 0.6931471805599453
iteration 9 batch 7970 trainingloss 0.6931471805599453
iteration 9 batch 7980 trainingloss 0.6931471805599453
iteration 9 batch 7990 trainingloss 0.6931471805599453
iteration 9 batch 8000 trainingloss 0.6931471805599453
iteration 9 batch 8010 trainingloss 0.6931471805599453
iteration 9 batch 8020 trainingloss 0.6931471805599453
iteration 9 batch 8030 trainingloss 0.6931471805599453
iteration 9 batch 8040 trainingloss 0.6931471805599453
iteration 9 batch 8050 trainingloss 0.6931471805599453
iteration 9 batch 8060 trainingloss 0.6931471805599453
iteration 9 batch 8070 trainingloss 0.6931471805599453
iteration 9 batch 8080 trainingloss 0.6916632528527511
iteration 9 batch 8090 trainingloss 0.6931471805599453
iteration 9 batch 8100 trainingloss 0.6931471805599453
iteration 9 batch 8110 trainingloss 0.6931471805599453
iteration 9 batch 8120 trainingloss 0.6931471805599453
iteration 9 batch 8130 trainingloss 0.6931471805599453
iteration 9 batch 8140 trainingloss 0.6931471805599453
iteration 9 batch 8150 trainingloss 0.6931471805599453
iteration 9 batch 8160 trainingloss 0.6931471805599453
iteration 9 batch 8170 trainingloss 0.6931471805599453
iteration 9 batch 8180 trainingloss 0.6931471805599453
iteration 9 batch 8190 trainingloss 0.6931471805599453
iteration 9 batch 8200 trainingloss 0.6931471805599453
iteration 9 batch 8210 trainingloss 0.6931471805599453
iteration 9 batch 8220 trainingloss 0.6931471805599453
iteration 9 batch 8230 trainingloss 0.6931471805599453
iteration 9 batch 8240 trainingloss 0.6931471805599453
iteration 9 batch 8250 trainingloss 0.6931471805599453
iteration 9 batch 8260 trainingloss 0.6931471805599453
iteration 9 batch 8270 trainingloss 0.6931471805599453
iteration 9 batch 8280 trainingloss 0.6931471805599453
iteration 9 batch 8290 trainingloss 0.6916632528527511
iteration 9 batch 8300 trainingloss 0.6931471805599453
iteration 9 batch 8310 trainingloss 0.6931471805599453
iteration 9 batch 8320 trainingloss 0.6931471805599453
iteration 9 batch 8330 trainingloss 0.6931471805599453
iteration 9 batch 8340 trainingloss 0.6931471805599453
iteration 9 batch 8350 trainingloss 0.6931471805599453
iteration 9 batch 8360 trainingloss 0.6931471805599453
iteration 9 batch 8370 trainingloss 0.6931471805599453
iteration 9 batch 8380 trainingloss 0.6931471805599453
iteration 9 batch 8390 trainingloss 0.6931471805599453
iteration 9 batch 8400 trainingloss 0.6931471805599453
iteration 9 batch 8410 trainingloss 0.6931471805599453
iteration 9 batch 8420 trainingloss 0.6931471805599453
iteration 9 batch 8430 trainingloss 0.6931471805599453
iteration 9 batch 8440 trainingloss 0.6931471805599453
iteration 9 batch 8450 trainingloss 0.6931471805599453
iteration 9 batch 8460 trainingloss 0.6931471805599453
iteration 9 batch 8470 trainingloss 0.6931471805599453
iteration 9 batch 8480 trainingloss 0.6931471805599453
iteration 9 batch 8490 trainingloss 0.6931471805599453
iteration 9 batch 8500 trainingloss 0.6931471805599453
iteration 9 batch 8510 trainingloss 0.6931471805599453
iteration 9 batch 8520 trainingloss 0.6916632528527511
iteration 9 batch 8530 trainingloss 0.6931471805599453
iteration 9 batch 8540 trainingloss 0.6931471805599453
iteration 9 batch 8550 trainingloss 0.6931471805599453
iteration 9 batch 8560 trainingloss 0.6931471805599453
iteration 9 batch 8570 trainingloss 0.6931471805599453
iteration 9 batch 8580 trainingloss 0.6931471805599453
iteration 9 batch 8590 trainingloss 0.6931471805599453
iteration 9 batch 8600 trainingloss 0.6931471805599453
iteration 9 batch 8610 trainingloss 0.6931471805599453
iteration 9 batch 8620 trainingloss 0.6931471805599453
iteration 9 batch 8630 trainingloss 0.6931471805599453
iteration 9 batch 8640 trainingloss 0.6931471805599453
iteration 9 batch 8650 trainingloss 0.6931471805599453
iteration 9 batch 8660 trainingloss 0.6931471805599453
iteration 9 batch 8670 trainingloss 0.6931471805599453
iteration 9 batch 8680 trainingloss 0.6931471805599453
iteration 9 batch 8690 trainingloss 0.6931471805599453
iteration 9 batch 8700 trainingloss 0.6931471805599453
iteration 9 batch 8710 trainingloss 0.6931471805599453
iteration 9 batch 8720 trainingloss 0.6931471805599453
iteration 9 batch 8730 trainingloss 0.6931471805599453
iteration 9 batch 8740 trainingloss 0.6931471805599453
iteration 9 batch 8750 trainingloss 0.6931471805599453
iteration 9 batch 8760 trainingloss 0.6931471805599453
iteration 9 batch 8770 trainingloss 0.6931471805599453
iteration 9 batch 8780 trainingloss 0.6931471805599453
iteration 9 batch 8790 trainingloss 0.6931471805599453
iteration 9 batch 8800 trainingloss 0.6931471805599453
iteration 9 batch 8810 trainingloss 0.6931471805599453
iteration 9 batch 8820 trainingloss 0.6931471805599453
iteration 9 batch 8830 trainingloss 0.6931471805599453
iteration 9 batch 8840 trainingloss 0.6931471805599453
iteration 9 batch 8850 trainingloss 0.6931471805599453
iteration 9 batch 8860 trainingloss 0.6931471805599453
iteration 9 batch 8870 trainingloss 0.6931471805599453
iteration 9 batch 8880 trainingloss 0.6931471805599453
iteration 9 batch 8890 trainingloss 0.6931471805599453
iteration 9 batch 8900 trainingloss 0.6931471805599453
iteration 9 batch 8910 trainingloss 0.6931471805599453
iteration 9 batch 8920 trainingloss 0.6931471805599453
iteration 9 batch 8930 trainingloss 0.6931471805599453
iteration 9 batch 8940 trainingloss 0.6931471805599453
iteration 9 batch 8950 trainingloss 0.6931471805599453
iteration 9 batch 8960 trainingloss 0.6931471805599453
iteration 9 batch 8970 trainingloss 0.6931471805599453
iteration 9 batch 8980 trainingloss 0.6931471805599453
iteration 9 batch 8990 trainingloss 0.6931471805599453
iteration 9 batch 9000 trainingloss 0.6931471805599453
iteration 9 batch 9010 trainingloss 0.6931471805599453
iteration 9 batch 9020 trainingloss 0.6931471805599453
iteration 9 batch 9030 trainingloss 0.6931471805599453
iteration 9 batch 9040 trainingloss 0.6931471805599453
iteration 9 batch 9050 trainingloss 0.6931471805599453
iteration 9 batch 9060 trainingloss 0.6931471805599453
iteration 9 batch 9070 trainingloss 0.6931471805599453
iteration 9 batch 9080 trainingloss 0.6931471805599453
iteration 9 batch 9090 trainingloss 0.6931471805599453
iteration 9 batch 9100 trainingloss 0.6931471805599453
iteration 9 batch 9110 trainingloss 0.6931471805599453
iteration 9 batch 9120 trainingloss 0.6931471805599453
iteration 9 batch 9130 trainingloss 0.6931471805599453
iteration 9 batch 9140 trainingloss 0.6931471805599453
iteration 9 batch 9150 trainingloss 0.6931471805599453
iteration 9 batch 9160 trainingloss 0.6931471805599453
iteration 9 batch 9170 trainingloss 0.6931471805599453
iteration 9 batch 9180 trainingloss 0.6931471805599453
iteration 9 batch 9190 trainingloss 0.6916632528527511
iteration 9 batch 9200 trainingloss 0.6931471805599453
iteration 9 batch 9210 trainingloss 0.6931471805599453
iteration 9 batch 9220 trainingloss 0.6931471805599453
iteration 9 batch 9230 trainingloss 0.6916632528527511
iteration 9 batch 9240 trainingloss 0.6931471805599453
iteration 9 batch 9250 trainingloss 0.6931471805599453
iteration 9 batch 9260 trainingloss 0.6931471805599453
iteration 9 batch 9270 trainingloss 0.6931471805599453
iteration 9 batch 9280 trainingloss 0.6931471805599453
iteration 9 batch 9290 trainingloss 0.6931471805599453
iteration 9 batch 9300 trainingloss 0.6931471805599453
iteration 9 batch 9310 trainingloss 0.6931471805599453
iteration 9 batch 9320 trainingloss 0.6931471805599453
iteration 9 batch 9330 trainingloss 0.6931471805599453
iteration 9 batch 9340 trainingloss 0.6931471805599453
iteration 9 batch 9350 trainingloss 0.6931471805599453
iteration 9 batch 9360 trainingloss 0.6931471805599453
iteration 9 batch 9370 trainingloss 0.6916632528527511
iteration 9 batch 9380 trainingloss 0.6931471805599453
iteration 9 batch 9390 trainingloss 0.6931471805599453
iteration 9 batch 9400 trainingloss 0.6931471805599453
iteration 9 batch 9410 trainingloss 0.6931471805599453
iteration 9 batch 9420 trainingloss 0.6931471805599453
iteration 9 batch 9430 trainingloss 0.6931471805599453
iteration 9 batch 9440 trainingloss 0.6916632528527511
iteration 9 batch 9450 trainingloss 0.6931471805599453
iteration 9 batch 9460 trainingloss 0.6931471805599453
iteration 9 batch 9470 trainingloss 0.6931471805599453
iteration 9 batch 9480 trainingloss 0.6931471805599453
iteration 9 batch 9490 trainingloss 0.6931471805599453
iteration 9 batch 9500 trainingloss 0.6931471805599453
iteration 9 batch 9510 trainingloss 0.6931471805599453
iteration 9 batch 9520 trainingloss 0.6931471805599453
iteration 9 batch 9530 trainingloss 0.6931471805599453
iteration 9 batch 9540 trainingloss 0.6931471805599453
iteration 9 batch 9550 trainingloss 0.6916632528527511
iteration 9 batch 9560 trainingloss 0.6931471805599453
iteration 9 batch 9570 trainingloss 0.6916632528527511
iteration 9 batch 9580 trainingloss 0.6931471805599453
iteration 9 batch 9590 trainingloss 0.6931471805599453
iteration 9 batch 9600 trainingloss 0.6931471805599453
iteration 9 batch 9610 trainingloss 0.6931471805599453
iteration 9 batch 9620 trainingloss 0.6931471805599453
iteration 9 batch 9630 trainingloss 0.6931471805599453
iteration 9 batch 9640 trainingloss 0.6931471805599453
iteration 9 batch 9650 trainingloss 0.6931471805599453
iteration 9 batch 9660 trainingloss 0.6931471805599453
iteration 9 batch 9670 trainingloss 0.6931471805599453
iteration 9 batch 9680 trainingloss 0.6931471805599453
iteration 9 batch 9690 trainingloss 0.6916632528527511
iteration 9 batch 9700 trainingloss 0.6931471805599453
iteration 9 batch 9710 trainingloss 0.6931471805599453
iteration 9 batch 9720 trainingloss 0.6931471805599453
iteration 9 batch 9730 trainingloss 0.6931471805599453
iteration 9 batch 9740 trainingloss 0.6931471805599453
iteration 9 batch 9750 trainingloss 0.6931471805599453
iteration 9 batch 9760 trainingloss 0.6931471805599453
iteration 9 batch 9770 trainingloss 0.6931471805599453
iteration 9 batch 9780 trainingloss 0.6931471805599453
iteration 9 batch 9790 trainingloss 0.6931471805599453
iteration 9 batch 9800 trainingloss 0.6931471805599453
iteration 9 batch 9810 trainingloss 0.6931471805599453
iteration 9 batch 9820 trainingloss 0.6931471805599453
iteration 9 batch 9830 trainingloss 0.6931471805599453
iteration 9 batch 9840 trainingloss 0.6931471805599453
iteration 9 batch 9850 trainingloss 0.6931471805599453
iteration 9 batch 9860 trainingloss 0.6931471805599453
iteration 9 batch 9870 trainingloss 0.6931471805599453
iteration 9 batch 9880 trainingloss 0.6916632528527511
iteration 9 batch 9890 trainingloss 0.6931471805599453
iteration 9 batch 9900 trainingloss 0.6931471805599453
iteration 9 batch 9910 trainingloss 0.6931471805599453
iteration 9 batch 9920 trainingloss 0.6931471805599453
iteration 9 batch 9930 trainingloss 0.6931471805599453
iteration 9 batch 9940 trainingloss 0.6931471805599453
iteration 9 batch 9950 trainingloss 0.6931471805599453
iteration 9 batch 9960 trainingloss 0.6931471805599453
iteration 9 batch 9970 trainingloss 0.6916632528527511
iteration 9 batch 9980 trainingloss 0.6931471805599453
iteration 9 batch 9990 trainingloss 0.6931471805599453
iteration 9 batch 10000 trainingloss 0.6931471805599453
iteration 9 batch 10010 trainingloss 0.6931471805599453
iteration 9 batch 10020 trainingloss 0.6931471805599453
iteration 9 batch 10030 trainingloss 0.6931471805599453
iteration 9 batch 10040 trainingloss 0.6931471805599453
iteration 9 batch 10050 trainingloss 0.6931471805599453
iteration 9 batch 10060 trainingloss 0.6931471805599453
iteration 9 batch 10070 trainingloss 0.6931471805599453
iteration 9 batch 10080 trainingloss 0.6931471805599453
iteration 9 batch 10090 trainingloss 0.6931471805599453
iteration 9 batch 10100 trainingloss 0.6931471805599453
iteration 9 batch 10110 trainingloss 0.6931471805599453
iteration 9 batch 10120 trainingloss 0.6931471805599453
iteration 9 batch 10130 trainingloss 0.6931471805599453
iteration 9 batch 10140 trainingloss 0.6931471805599453
iteration 9 batch 10150 trainingloss 0.6931471805599453
iteration 9 batch 10160 trainingloss 0.6931471805599453
iteration 9 batch 10170 trainingloss 0.6931471805599453
iteration 9 batch 10180 trainingloss 0.6931471805599453
iteration 9 batch 10190 trainingloss 0.6931471805599453
iteration 9 batch 10200 trainingloss 0.6931471805599453
iteration 9 batch 10210 trainingloss 0.6931471805599453
iteration 9 batch 10220 trainingloss 0.6916632528527511
iteration 9 batch 10230 trainingloss 0.6916632528527511
iteration 9 batch 10240 trainingloss 0.6931471805599453
iteration 9 batch 10250 trainingloss 0.6931471805599453
iteration 9 batch 10260 trainingloss 0.6931471805599453
iteration 9 batch 10270 trainingloss 0.6931471805599453
iteration 9 batch 10280 trainingloss 0.6931471805599453
iteration 9 batch 10290 trainingloss 0.6931471805599453
iteration 9 batch 10300 trainingloss 0.6931471805599453
iteration 9 batch 10310 trainingloss 0.6931471805599453
iteration 9 batch 10320 trainingloss 0.6931471805599453
iteration 9 batch 10330 trainingloss 0.6931471805599453
iteration 9 batch 10340 trainingloss 0.6931471805599453
iteration 9 batch 10350 trainingloss 0.6916632528527511
iteration 9 batch 10360 trainingloss 0.6931471805599453
iteration 9 batch 10370 trainingloss 0.6931471805599453
iteration 9 batch 10380 trainingloss 0.6931471805599453
iteration 9 batch 10390 trainingloss 0.6931471805599453
iteration 9 batch 10400 trainingloss 0.6931471805599453
iteration 9 batch 10410 trainingloss 0.6931471805599453
iteration 9 batch 10420 trainingloss 0.6931471805599453
iteration 9 batch 10430 trainingloss 0.6931471805599453
iteration 9 batch 10440 trainingloss 0.6931471805599453
iteration 9 batch 10450 trainingloss 0.6931471805599453
iteration 9 batch 10460 trainingloss 0.6931471805599453
iteration 9 batch 10470 trainingloss 0.6931471805599453
iteration 9 batch 10480 trainingloss 0.6931471805599453
iteration 9 batch 10490 trainingloss 0.6931471805599453
iteration 9 batch 10500 trainingloss 0.6931471805599453
iteration 9 batch 10510 trainingloss 0.6931471805599453
iteration 9 batch 10520 trainingloss 0.6931471805599453
iteration 9 batch 10530 trainingloss 0.6931471805599453
iteration 9 batch 10540 trainingloss 0.6931471805599453
iteration 9 batch 10550 trainingloss 0.6931471805599453
iteration 9 batch 10560 trainingloss 0.6931471805599453
iteration 9 batch 10570 trainingloss 0.6931471805599453
iteration 9 batch 10580 trainingloss 0.6931471805599453
iteration 9 batch 10590 trainingloss 0.6931471805599453
iteration 9 batch 10600 trainingloss 0.6931471805599453
iteration 9 batch 10610 trainingloss 0.6931471805599453
iteration 9 batch 10620 trainingloss 0.6931471805599453
iteration 9 batch 10630 trainingloss 0.6931471805599453
iteration 9 batch 10640 trainingloss 0.6931471805599453
iteration 9 batch 10650 trainingloss 0.6931471805599453
iteration 9 batch 10660 trainingloss 0.6916632528527511
iteration 9 batch 10670 trainingloss 0.6931471805599453
iteration 9 batch 10680 trainingloss 0.6931471805599453
iteration 9 batch 10690 trainingloss 0.6931471805599453
iteration 9 batch 10700 trainingloss 0.6931471805599453
iteration 9 batch 10710 trainingloss 0.6931471805599453
iteration 9 batch 10720 trainingloss 0.6916632528527511
iteration 9 batch 10730 trainingloss 0.6931471805599453
iteration 9 batch 10740 trainingloss 0.6931471805599453
iteration 9 batch 10750 trainingloss 0.6931471805599453
iteration 9 batch 10760 trainingloss 0.6931471805599453
iteration 9 batch 10770 trainingloss 0.6931471805599453
iteration 9 batch 10780 trainingloss 0.6931471805599453
iteration 9 batch 10790 trainingloss 0.6931471805599453
iteration 9 batch 10800 trainingloss 0.6931471805599453
iteration 9 batch 10810 trainingloss 0.6931471805599453
iteration 9 batch 10820 trainingloss 0.6931471805599453
iteration 9 batch 10830 trainingloss 0.6931471805599453
iteration 9 batch 10840 trainingloss 0.6931471805599453
iteration 9 batch 10850 trainingloss 0.6931471805599453
iteration 9 batch 10860 trainingloss 0.6916632528527511
iteration 9 batch 10870 trainingloss 0.6931471805599453
iteration 9 batch 10880 trainingloss 0.6931471805599453
iteration 9 batch 10890 trainingloss 0.6931471805599453
iteration 9 batch 10900 trainingloss 0.6931471805599453
iteration 9 batch 10910 trainingloss 0.6931471805599453
iteration 9 batch 10920 trainingloss 0.6931471805599453
iteration 9 batch 10930 trainingloss 0.6931471805599453
iteration 9 batch 10940 trainingloss 0.6931471805599453
iteration 9 batch 10950 trainingloss 0.6931471805599453
iteration 9 batch 10960 trainingloss 0.6931471805599453
iteration 9 batch 10970 trainingloss 0.6931471805599453
iteration 9 batch 10980 trainingloss 0.6916632528527511
iteration 9 batch 10990 trainingloss 0.6931471805599453
iteration 9 batch 11000 trainingloss 0.6931471805599453
iteration 9 batch 11010 trainingloss 0.6931471805599453
iteration 9 batch 11020 trainingloss 0.6931471805599453
iteration 9 batch 11030 trainingloss 0.6931471805599453
iteration 9 batch 11040 trainingloss 0.6931471805599453
iteration 9 batch 11050 trainingloss 0.6931471805599453
iteration 9 batch 11060 trainingloss 0.6931471805599453
iteration 9 batch 11070 trainingloss 0.6931471805599453
iteration 9 batch 11080 trainingloss 0.6916632528527511
iteration 9 batch 11090 trainingloss 0.6931471805599453
iteration 9 batch 11100 trainingloss 0.6931471805599453
iteration 9 batch 11110 trainingloss 0.6931471805599453
iteration 9 batch 11120 trainingloss 0.6931471805599453
iteration 9 batch 11130 trainingloss 0.6931471805599453
iteration 9 batch 11140 trainingloss 0.6931471805599453
iteration 9 batch 11150 trainingloss 0.6931471805599453
iteration 9 batch 11160 trainingloss 0.6931471805599453
iteration 9 batch 11170 trainingloss 0.6931471805599453
iteration 9 batch 11180 trainingloss 0.6916632528527511
iteration 9 batch 11190 trainingloss 0.6931471805599453
iteration 9 batch 11200 trainingloss 0.6931471805599453
iteration 9 batch 11210 trainingloss 0.6931471805599453
iteration 9 batch 11220 trainingloss 0.6931471805599453
iteration 9 batch 11230 trainingloss 0.6931471805599453
iteration 9 batch 11240 trainingloss 0.6931471805599453
iteration 9 batch 11250 trainingloss 0.6931471805599453
iteration 9 batch 11260 trainingloss 0.6931471805599453
iteration 9 batch 11270 trainingloss 0.6931471805599453
iteration 9 batch 11280 trainingloss 0.6931471805599453
iteration 9 batch 11290 trainingloss 0.6931471805599453
iteration 9 batch 11300 trainingloss 0.6931471805599453
iteration 9 batch 11310 trainingloss 0.6931471805599453
iteration 9 batch 11320 trainingloss 0.6931471805599453
iteration 9 batch 11330 trainingloss 0.6931471805599453
iteration 9 batch 11340 trainingloss 0.6931471805599453
iteration 9 batch 11350 trainingloss 0.6931471805599453
iteration 9 batch 11360 trainingloss 0.6931471805599453
iteration 9 batch 11370 trainingloss 0.6931471805599453
iteration 9 batch 11380 trainingloss 0.6931471805599453
iteration 9 batch 11390 trainingloss 0.6916632528527511
iteration 9 batch 11400 trainingloss 0.6931471805599453
iteration 9 batch 11410 trainingloss 0.6931471805599453
iteration 9 batch 11420 trainingloss 0.6931471805599453
iteration 9 batch 11430 trainingloss 0.6931471805599453
iteration 9 batch 11440 trainingloss 0.6931471805599453
iteration 9 batch 11450 trainingloss 0.6931471805599453
iteration 9 batch 11460 trainingloss 0.6931471805599453
iteration 9 batch 11470 trainingloss 0.6931471805599453
iteration 9 batch 11480 trainingloss 0.6931471805599453
iteration 9 batch 11490 trainingloss 0.6931471805599453
iteration 9 batch 11500 trainingloss 0.6931471805599453
iteration 9 batch 11510 trainingloss 0.6931471805599453
iteration 9 batch 11520 trainingloss 0.6931471805599453
iteration 9 batch 11530 trainingloss 0.6931471805599453
iteration 9 batch 11540 trainingloss 0.6931471805599453
iteration 9 batch 11550 trainingloss 0.6931471805599453
iteration 9 batch 11560 trainingloss 0.6931471805599453
iteration 9 batch 11570 trainingloss 0.6931471805599453
iteration 9 batch 11580 trainingloss 0.6931471805599453
iteration 9 batch 11590 trainingloss 0.6931471805599453
iteration 9 batch 11600 trainingloss 0.6931471805599453
iteration 9 batch 11610 trainingloss 0.6931471805599453
iteration 9 batch 11620 trainingloss 0.6931471805599453
iteration 9 batch 11630 trainingloss 0.6931471805599453
iteration 9 batch 11640 trainingloss 0.6931471805599453
iteration 9 batch 11650 trainingloss 0.6931471805599453
iteration 9 batch 11660 trainingloss 0.6931471805599453
iteration 9 batch 11670 trainingloss 0.6931471805599453
iteration 9 batch 11680 trainingloss 0.6931471805599453
iteration 9 batch 11690 trainingloss 0.6931471805599453
iteration 9 batch 11700 trainingloss 0.6931471805599453
iteration 9 batch 11710 trainingloss 0.6931471805599453
iteration 9 batch 11720 trainingloss 0.6931471805599453
iteration 9 batch 11730 trainingloss 0.6916632528527511
iteration 9 batch 11740 trainingloss 0.6931471805599453
iteration 9 batch 11750 trainingloss 0.6916632528527511
iteration 9 batch 11760 trainingloss 0.6931471805599453
iteration 9 batch 11770 trainingloss 0.6931471805599453
iteration 9 batch 11780 trainingloss 0.6916632528527511
iteration 9 batch 11790 trainingloss 0.6931471805599453
iteration 9 batch 11800 trainingloss 0.6931471805599453
iteration 9 batch 11810 trainingloss 0.6931471805599453
iteration 9 batch 11820 trainingloss 0.6931471805599453
iteration 9 batch 11830 trainingloss 0.6931471805599453
iteration 9 batch 11840 trainingloss 0.6931471805599453
iteration 9 batch 11850 trainingloss 0.6931471805599453
iteration 9 batch 11860 trainingloss 0.6931471805599453
iteration 9 batch 11870 trainingloss 0.6931471805599453
iteration 9 batch 11880 trainingloss 0.6931471805599453
iteration 9 batch 11890 trainingloss 0.6931471805599453
iteration 9 batch 11900 trainingloss 0.6931471805599453
iteration 9 batch 11910 trainingloss 0.6931471805599453
iteration 9 batch 11920 trainingloss 0.6931471805599453
iteration 9 batch 11930 trainingloss 0.6931471805599453
iteration 9 batch 11940 trainingloss 0.6931471805599453
iteration 9 batch 11950 trainingloss 0.6931471805599453
iteration 9 batch 11960 trainingloss 0.6931471805599453
iteration 9 batch 11970 trainingloss 0.6916632528527511
iteration 9 batch 11980 trainingloss 0.6931471805599453
iteration 9 batch 11990 trainingloss 0.6931471805599453
iteration 9 batch 12000 trainingloss 0.6931471805599453
iteration 9 batch 12010 trainingloss 0.6916632528527511
iteration 9 batch 12020 trainingloss 0.6931471805599453
iteration 9 batch 12030 trainingloss 0.6931471805599453
iteration 9 batch 12040 trainingloss 0.6931471805599453
iteration 9 batch 12050 trainingloss 0.6931471805599453
iteration 9 batch 12060 trainingloss 0.6931471805599453
iteration 9 batch 12070 trainingloss 0.6931471805599453
iteration 9 batch 12080 trainingloss 0.6931471805599453
iteration 9 batch 12090 trainingloss 0.6931471805599453
iteration 9 batch 12100 trainingloss 0.6931471805599453
iteration 9 batch 12110 trainingloss 0.6931471805599453
iteration 9 batch 12120 trainingloss 0.6931471805599453
iteration 9 batch 12130 trainingloss 0.6931471805599453
iteration 9 batch 12140 trainingloss 0.6931471805599453
iteration 9 batch 12150 trainingloss 0.6931471805599453
iteration 9 batch 12160 trainingloss 0.6931471805599453
iteration 9 batch 12170 trainingloss 0.6931471805599453
iteration 9 batch 12180 trainingloss 0.6931471805599453
iteration 9 batch 12190 trainingloss 0.6931471805599453
iteration 9 batch 12200 trainingloss 0.6931471805599453
iteration 9 batch 12210 trainingloss 0.6931471805599453
iteration 9 batch 12220 trainingloss 0.6931471805599453
iteration 9 batch 12230 trainingloss 0.6931471805599453
iteration 9 batch 12240 trainingloss 0.6931471805599453
iteration 9 batch 12250 trainingloss 0.6931471805599453
iteration 9 batch 12260 trainingloss 0.6931471805599453
iteration 9 batch 12270 trainingloss 0.6931471805599453
iteration 9 batch 12280 trainingloss 0.6931471805599453
iteration 9 batch 12290 trainingloss 0.6931471805599453
iteration 9 batch 12300 trainingloss 0.6931471805599453
iteration 9 batch 12310 trainingloss 0.6931471805599453
iteration 9 batch 12320 trainingloss 0.6931471805599453
iteration 9 batch 12330 trainingloss 0.6931471805599453
iteration 9 batch 12340 trainingloss 0.6931471805599453
iteration 9 batch 12350 trainingloss 0.6931471805599453
iteration 9 batch 12360 trainingloss 0.6931471805599453
iteration 9 batch 12370 trainingloss 0.6931471805599453
iteration 9 batch 12380 trainingloss 0.6931471805599453
iteration 9 batch 12390 trainingloss 0.6931471805599453
iteration 9 batch 12400 trainingloss 0.6931471805599453
iteration 9 batch 12410 trainingloss 0.6931471805599453
iteration 9 batch 12420 trainingloss 0.6931471805599453
iteration 9 batch 12430 trainingloss 0.6931471805599453
iteration 9 batch 12440 trainingloss 0.6931471805599453
iteration 9 batch 12450 trainingloss 0.6931471805599453
iteration 9 batch 12460 trainingloss 0.6931471805599453
iteration 9 batch 12470 trainingloss 0.6931471805599453
iteration 9 batch 12480 trainingloss 0.6931471805599453
iteration 9 batch 12490 trainingloss 0.6931471805599453
iteration 9 batch 12500 trainingloss 0.6931471805599453
iteration 9 batch 12510 trainingloss 0.6931471805599453
iteration 9 batch 12520 trainingloss 0.6931471805599453
iteration 9 batch 12530 trainingloss 0.6931471805599453
iteration 9 batch 12540 trainingloss 0.6931471805599453
iteration 9 batch 12550 trainingloss 0.6931471805599453
iteration 9 batch 12560 trainingloss 0.6931471805599453
iteration 9 batch 12570 trainingloss 0.6931471805599453
iteration 9 batch 12580 trainingloss 0.6931471805599453
iteration 9 batch 12590 trainingloss 0.6931471805599453
iteration 9 batch 12600 trainingloss 0.6931471805599453
iteration 9 batch 12610 trainingloss 0.6931471805599453
iteration 9 batch 12620 trainingloss 0.6931471805599453
iteration 9 batch 12630 trainingloss 0.6931471805599453
iteration 9 batch 12640 trainingloss 0.6931471805599453
iteration 9 batch 12650 trainingloss 0.6931471805599453
iteration 9 batch 12660 trainingloss 0.6931471805599453
iteration 9 batch 12670 trainingloss 0.6931471805599453
iteration 9 batch 12680 trainingloss 0.6916632528527511
iteration 9 batch 12690 trainingloss 0.6931471805599453
iteration 9 batch 12700 trainingloss 0.6931471805599453
iteration 9 batch 12710 trainingloss 0.6931471805599453
iteration 9 batch 12720 trainingloss 0.6931471805599453
iteration 9 batch 12730 trainingloss 0.6931471805599453
iteration 9 batch 12740 trainingloss 0.6931471805599453
iteration 9 batch 12750 trainingloss 0.6931471805599453
iteration 9 batch 12760 trainingloss 0.6931471805599453
iteration 9 batch 12770 trainingloss 0.6931471805599453
iteration 9 batch 12780 trainingloss 0.6931471805599453
iteration 9 batch 12790 trainingloss 0.6931471805599453
iteration 9 batch 12800 trainingloss 0.6931471805599453
iteration 9 batch 12810 trainingloss 0.6931471805599453
iteration 9 batch 12820 trainingloss 0.6931471805599453
iteration 9 batch 12830 trainingloss 0.6931471805599453
iteration 9 batch 12840 trainingloss 0.6931471805599453
iteration 9 batch 12850 trainingloss 0.6931471805599453
iteration 9 batch 12860 trainingloss 0.6931471805599453
iteration 9 batch 12870 trainingloss 0.6931471805599453
iteration 9 batch 12880 trainingloss 0.6931471805599453
iteration 9 batch 12890 trainingloss 0.6931471805599453
iteration 9 batch 12900 trainingloss 0.6931471805599453
iteration 9 batch 12910 trainingloss 0.6931471805599453
iteration 9 batch 12920 trainingloss 0.6931471805599453
iteration 9 batch 12930 trainingloss 0.6931471805599453
iteration 9 batch 12940 trainingloss 0.6931471805599453
iteration 9 batch 12950 trainingloss 0.6916632528527511
iteration 9 batch 12960 trainingloss 0.6931471805599453
iteration 9 batch 12970 trainingloss 0.6931471805599453
iteration 9 batch 12980 trainingloss 0.6931471805599453
iteration 9 batch 12990 trainingloss 0.6931471805599453
iteration 9 batch 13000 trainingloss 0.6931471805599453
iteration 9 batch 13010 trainingloss 0.6931471805599453
iteration 9 batch 13020 trainingloss 0.6931471805599453
iteration 9 batch 13030 trainingloss 0.6931471805599453
iteration 9 batch 13040 trainingloss 0.6931471805599453
iteration 9 batch 13050 trainingloss 0.6931471805599453
iteration 9 batch 13060 trainingloss 0.6931471805599453
iteration 9 batch 13070 trainingloss 0.6931471805599453
iteration 9 batch 13080 trainingloss 0.6931471805599453
iteration 9 batch 13090 trainingloss 0.6931471805599453
iteration 9 batch 13100 trainingloss 0.6931471805599453
iteration 9 batch 13110 trainingloss 0.6931471805599453
iteration 9 batch 13120 trainingloss 0.6931471805599453
iteration 9 batch 13130 trainingloss 0.6931471805599453
iteration 9 batch 13140 trainingloss 0.6931471805599453
iteration 9 batch 13150 trainingloss 0.6931471805599453
iteration 9 batch 13160 trainingloss 0.6931471805599453
iteration 9 batch 13170 trainingloss 0.6931471805599453
iteration 9 batch 13180 trainingloss 0.6931471805599453
iteration 9 batch 13190 trainingloss 0.6931471805599453
iteration 9 batch 13200 trainingloss 0.6931471805599453
iteration 9 batch 13210 trainingloss 0.6901793251455568
iteration 9 batch 13220 trainingloss 0.6931471805599453
iteration 9 batch 13230 trainingloss 0.6931471805599453
iteration 9 batch 13240 trainingloss 0.6931471805599453
iteration 9 batch 13250 trainingloss 0.6931471805599453
iteration 9 batch 13260 trainingloss 0.6931471805599453
iteration 9 batch 13270 trainingloss 0.6931471805599453
iteration 9 batch 13280 trainingloss 0.6931471805599453
iteration 9 batch 13290 trainingloss 0.6931471805599453
iteration 9 batch 13300 trainingloss 0.6931471805599453
iteration 9 batch 13310 trainingloss 0.6931471805599453
iteration 9 batch 13320 trainingloss 0.6916632528527511
iteration 9 batch 13330 trainingloss 0.6931471805599453
iteration 9 batch 13340 trainingloss 0.6931471805599453
iteration 9 batch 13350 trainingloss 0.6931471805599453
iteration 9 batch 13360 trainingloss 0.6931471805599453
iteration 9 batch 13370 trainingloss 0.6931471805599453
iteration 9 batch 13380 trainingloss 0.6931471805599453
iteration 9 batch 13390 trainingloss 0.6931471805599453
iteration 9 batch 13400 trainingloss 0.6931471805599453
iteration 9 batch 13410 trainingloss 0.6931471805599453
iteration 9 batch 13420 trainingloss 0.6931471805599453
iteration 9 batch 13430 trainingloss 0.6931471805599453
iteration 9 batch 13440 trainingloss 0.6916632528527511
iteration 9 batch 13450 trainingloss 0.6931471805599453
iteration 9 batch 13460 trainingloss 0.6931471805599453
iteration 9 batch 13470 trainingloss 0.6931471805599453
iteration 9 batch 13480 trainingloss 0.6931471805599453
iteration 9 batch 13490 trainingloss 0.6931471805599453
iteration 9 batch 13500 trainingloss 0.6931471805599453
iteration 9 batch 13510 trainingloss 0.6931471805599453
iteration 9 batch 13520 trainingloss 0.6931471805599453
iteration 9 batch 13530 trainingloss 0.6931471805599453
iteration 9 batch 13540 trainingloss 0.6931471805599453
iteration 9 batch 13550 trainingloss 0.6931471805599453
iteration 9 batch 13560 trainingloss 0.6931471805599453
iteration 9 batch 13570 trainingloss 0.6931471805599453
iteration 9 batch 13580 trainingloss 0.6931471805599453
iteration 9 batch 13590 trainingloss 0.6931471805599453
iteration 9 batch 13600 trainingloss 0.6931471805599453
iteration 9 batch 13610 trainingloss 0.6931471805599453
iteration 9 batch 13620 trainingloss 0.6916632528527511
iteration 9 batch 13630 trainingloss 0.6931471805599453
iteration 9 batch 13640 trainingloss 0.6931471805599453
iteration 9 batch 13650 trainingloss 0.6931471805599453
iteration 9 batch 13660 trainingloss 0.6931471805599453
iteration 9 batch 13670 trainingloss 0.6931471805599453
iteration 9 batch 13680 trainingloss 0.6931471805599453
iteration 9 batch 13690 trainingloss 0.6931471805599453
iteration 9 batch 13700 trainingloss 0.6931471805599453
iteration 9 batch 13710 trainingloss 0.6931471805599453
iteration 9 batch 13720 trainingloss 0.6931471805599453
iteration 9 batch 13730 trainingloss 0.6931471805599453
iteration 9 batch 13740 trainingloss 0.6931471805599453
iteration 9 batch 13750 trainingloss 0.6931471805599453
iteration 9 batch 13760 trainingloss 0.6931471805599453
iteration 9 batch 13770 trainingloss 0.6931471805599453
iteration 9 batch 13780 trainingloss 0.6931471805599453
iteration 9 batch 13790 trainingloss 0.6931471805599453
iteration 9 batch 13800 trainingloss 0.6931471805599453
iteration 9 batch 13810 trainingloss 0.6931471805599453
iteration 9 batch 13820 trainingloss 0.6931471805599453
iteration 9 batch 13830 trainingloss 0.6916632528527511
iteration 9 batch 13840 trainingloss 0.6931471805599453
iteration 9 batch 13850 trainingloss 0.6916632528527511
iteration 9 batch 13860 trainingloss 0.6931471805599453
iteration 9 batch 13870 trainingloss 0.6916632528527511
iteration 9 batch 13880 trainingloss 0.6931471805599453
iteration 9 batch 13890 trainingloss 0.6931471805599453
iteration 9 batch 13900 trainingloss 0.6931471805599453
iteration 9 batch 13910 trainingloss 0.6931471805599453
iteration 9 batch 13920 trainingloss 0.6931471805599453
iteration 9 batch 13930 trainingloss 0.6916632528527511
iteration 9 batch 13940 trainingloss 0.6931471805599453
iteration 9 batch 13950 trainingloss 0.6931471805599453
iteration 9 batch 13960 trainingloss 0.6931471805599453
iteration 9 batch 13970 trainingloss 0.6931471805599453
iteration 9 batch 13980 trainingloss 0.6931471805599453
iteration 9 batch 13990 trainingloss 0.6931471805599453
iteration 9 batch 14000 trainingloss 0.6931471805599453
iteration 9 batch 14010 trainingloss 0.6931471805599453
iteration 9 batch 14020 trainingloss 0.6931471805599453
iteration 9 batch 14030 trainingloss 0.6931471805599453
iteration 9 batch 14040 trainingloss 0.6931471805599453
iteration 9 batch 14050 trainingloss 0.6916632528527511
iteration 9 batch 14060 trainingloss 0.6916632528527511
iteration 9 batch 14070 trainingloss 0.6931471805599453
iteration 9 batch 14080 trainingloss 0.6931471805599453
iteration 9 batch 14090 trainingloss 0.6931471805599453
iteration 9 batch 14100 trainingloss 0.6931471805599453
iteration 9 batch 14110 trainingloss 0.6931471805599453
iteration 9 batch 14120 trainingloss 0.6931471805599453
iteration 9 batch 14130 trainingloss 0.6931471805599453
iteration 9 batch 14140 trainingloss 0.6931471805599453
iteration 9 batch 14150 trainingloss 0.6931471805599453
iteration 9 batch 14160 trainingloss 0.6931471805599453
iteration 9 batch 14170 trainingloss 0.6931471805599453
iteration 9 batch 14180 trainingloss 0.6931471805599453
iteration 9 batch 14190 trainingloss 0.6931471805599453
iteration 9 batch 14200 trainingloss 0.6931471805599453
iteration 9 batch 14210 trainingloss 0.6931471805599453
iteration 9 batch 14220 trainingloss 0.6931471805599453
iteration 9 batch 14230 trainingloss 0.6931471805599453
iteration 9 batch 14240 trainingloss 0.6931471805599453
iteration 9 batch 14250 trainingloss 0.6931471805599453
iteration 9 batch 14260 trainingloss 0.6931471805599453
iteration 9 batch 14270 trainingloss 0.6931471805599453
iteration 9 batch 14280 trainingloss 0.6931471805599453
iteration 9 batch 14290 trainingloss 0.6931471805599453
iteration 9 batch 14300 trainingloss 0.6931471805599453
iteration 9 batch 14310 trainingloss 0.6931471805599453
iteration 9 batch 14320 trainingloss 0.6931471805599453
iteration 9 batch 14330 trainingloss 0.6931471805599453
iteration 9 batch 14340 trainingloss 0.6931471805599453
iteration 9 batch 14350 trainingloss 0.6931471805599453
iteration 9 batch 14360 trainingloss 0.6931471805599453
iteration 9 batch 14370 trainingloss 0.6931471805599453
iteration 9 batch 14380 trainingloss 0.6931471805599453
iteration 9 batch 14390 trainingloss 0.6931471805599453
iteration 9 batch 14400 trainingloss 0.6931471805599453
iteration 9 batch 14410 trainingloss 0.6931471805599453
iteration 9 batch 14420 trainingloss 0.6931471805599453
iteration 9 batch 14430 trainingloss 0.6931471805599453
iteration 9 batch 14440 trainingloss 0.6931471805599453
iteration 9 batch 14450 trainingloss 0.6931471805599453
iteration 9 batch 14460 trainingloss 0.6931471805599453
iteration 9 batch 14470 trainingloss 0.6931471805599453
iteration 9 batch 14480 trainingloss 0.6931471805599453
iteration 9 batch 14490 trainingloss 0.6931471805599453
iteration 9 batch 14500 trainingloss 0.6931471805599453
iteration 9 batch 14510 trainingloss 0.6931471805599453
iteration 9 batch 14520 trainingloss 0.6931471805599453
iteration 9 batch 14530 trainingloss 0.6931471805599453
iteration 9 batch 14540 trainingloss 0.6931471805599453
iteration 9 batch 14550 trainingloss 0.6931471805599453
iteration 9 batch 14560 trainingloss 0.6931471805599453
iteration 9 batch 14570 trainingloss 0.6931471805599453
iteration 9 batch 14580 trainingloss 0.6931471805599453
iteration 9 batch 14590 trainingloss 0.6931471805599453
iteration 9 batch 14600 trainingloss 0.6931471805599453
iteration 9 batch 14610 trainingloss 0.6931471805599453
iteration 9 batch 14620 trainingloss 0.6931471805599453
iteration 9 batch 14630 trainingloss 0.6931471805599453
iteration 9 batch 14640 trainingloss 0.6931471805599453
iteration 9 batch 14650 trainingloss 0.6931471805599453
iteration 9 batch 14660 trainingloss 0.6931471805599453
iteration 9 batch 14670 trainingloss 0.6931471805599453
iteration 9 batch 14680 trainingloss 0.6931471805599453
iteration 9 batch 14690 trainingloss 0.6931471805599453
iteration 9 batch 14700 trainingloss 0.6931471805599453
iteration 9 batch 14710 trainingloss 0.6931471805599453
iteration 9 batch 14720 trainingloss 0.6931471805599453
iteration 9 batch 14730 trainingloss 0.6931471805599453
iteration 9 batch 14740 trainingloss 0.6931471805599453
iteration 9 batch 14750 trainingloss 0.6931471805599453
iteration 9 batch 14760 trainingloss 0.6931471805599453
iteration 9 batch 14770 trainingloss 0.6931471805599453
iteration 9 batch 14780 trainingloss 0.6931471805599453
iteration 9 batch 14790 trainingloss 0.6931471805599453
iteration 9 batch 14800 trainingloss 0.6931471805599453
iteration 9 batch 14810 trainingloss 0.6931471805599453
iteration 9 batch 14820 trainingloss 0.6931471805599453
iteration 9 batch 14830 trainingloss 0.6931471805599453
iteration 9 batch 14840 trainingloss 0.6931471805599453
iteration 9 batch 14850 trainingloss 0.6931471805599453
iteration 9 batch 14860 trainingloss 0.6931471805599453
iteration 9 batch 14870 trainingloss 0.6931471805599453
iteration 9 batch 14880 trainingloss 0.6931471805599453
iteration 9 batch 14890 trainingloss 0.6931471805599453
iteration 9 batch 14900 trainingloss 0.6931471805599453
iteration 9 batch 14910 trainingloss 0.6931471805599453
iteration 9 batch 14920 trainingloss 0.6931471805599453
iteration 9 batch 14930 trainingloss 0.6916632528527511
iteration 9 batch 14940 trainingloss 0.6931471805599453
iteration 9 batch 14950 trainingloss 0.6931471805599453
iteration 9 batch 14960 trainingloss 0.6931471805599453
iteration 9 batch 14970 trainingloss 0.6931471805599453
iteration 9 batch 14980 trainingloss 0.6916632528527511
iteration 9 batch 14990 trainingloss 0.6931471805599453
iteration 9 batch 15000 trainingloss 0.6931471805599453
iteration 9 batch 15010 trainingloss 0.6931471805599453
iteration 9 batch 15020 trainingloss 0.6931471805599453
iteration 9 batch 15030 trainingloss 0.6931471805599453
iteration 9 batch 15040 trainingloss 0.6931471805599453
iteration 9 batch 15050 trainingloss 0.6931471805599453
iteration 9 batch 15060 trainingloss 0.6931471805599453
iteration 9 batch 15070 trainingloss 0.6931471805599453
iteration 9 batch 15080 trainingloss 0.6931471805599453
iteration 9 batch 15090 trainingloss 0.6931471805599453
iteration 9 batch 15100 trainingloss 0.6916632528527511
iteration 9 batch 15110 trainingloss 0.6931471805599453
iteration 9 batch 15120 trainingloss 0.6931471805599453
iteration 9 batch 15130 trainingloss 0.6931471805599453
iteration 9 batch 15140 trainingloss 0.6931471805599453
iteration 9 batch 15150 trainingloss 0.6916632528527511
iteration 9 batch 15160 trainingloss 0.6931471805599453
iteration 9 batch 15170 trainingloss 0.6931471805599453
iteration 9 batch 15180 trainingloss 0.6931471805599453
iteration 9 batch 15190 trainingloss 0.6931471805599453
iteration 9 batch 15200 trainingloss 0.6931471805599453
iteration 9 batch 15210 trainingloss 0.6931471805599453
iteration 9 batch 15220 trainingloss 0.6931471805599453
iteration 9 batch 15230 trainingloss 0.6931471805599453
iteration 9 batch 15240 trainingloss 0.6931471805599453
iteration 9 batch 15250 trainingloss 0.6931471805599453
iteration 9 batch 15260 trainingloss 0.6931471805599453
iteration 9 batch 15270 trainingloss 0.6931471805599453
iteration 9 batch 15280 trainingloss 0.6931471805599453
iteration 9 batch 15290 trainingloss 0.6931471805599453
iteration 9 batch 15300 trainingloss 0.6931471805599453
iteration 9 batch 15310 trainingloss 0.6931471805599453
iteration 9 batch 15320 trainingloss 0.6931471805599453
iteration 9 batch 15330 trainingloss 0.6931471805599453
iteration 9 batch 15340 trainingloss 0.6931471805599453
iteration 9 batch 15350 trainingloss 0.6931471805599453
iteration 9 batch 15360 trainingloss 0.6931471805599453
iteration 9 batch 15370 trainingloss 0.6916632528527511
iteration 9 batch 15380 trainingloss 0.6931471805599453
iteration 9 batch 15390 trainingloss 0.6931471805599453
iteration 9 batch 15400 trainingloss 0.6931471805599453
iteration 9 batch 15410 trainingloss 0.6931471805599453
iteration 9 batch 15420 trainingloss 0.6931471805599453
iteration 9 batch 15430 trainingloss 0.6931471805599453
iteration 9 batch 15440 trainingloss 0.6931471805599453
iteration 9 batch 15450 trainingloss 0.6931471805599453
iteration 9 batch 15460 trainingloss 0.6931471805599453
iteration 9 batch 15470 trainingloss 0.6931471805599453
iteration 9 batch 15480 trainingloss 0.6931471805599453
iteration 9 batch 15490 trainingloss 0.6931471805599453
iteration 9 batch 15500 trainingloss 0.6931471805599453
iteration 9 batch 15510 trainingloss 0.6931471805599453
iteration 9 batch 15520 trainingloss 0.6931471805599453
iteration 9 batch 15530 trainingloss 0.6931471805599453
iteration 9 batch 15540 trainingloss 0.6931471805599453
iteration 9 batch 15550 trainingloss 0.6931471805599453
iteration 9 batch 15560 trainingloss 0.6931471805599453
iteration 9 batch 15570 trainingloss 0.6931471805599453
iteration 9 batch 15580 trainingloss 0.6931471805599453
iteration 9 batch 15590 trainingloss 0.6931471805599453
iteration 9 batch 15600 trainingloss 0.6931471805599453
iteration 9 batch 15610 trainingloss 0.6931471805599453
iteration 9 batch 15620 trainingloss 0.6931471805599453
iteration 9 batch 15630 trainingloss 0.6931471805599453
iteration 9 batch 15640 trainingloss 0.6931471805599453
iteration 9 batch 15650 trainingloss 0.6931471805599453
iteration 9 batch 15660 trainingloss 0.6931471805599453
iteration 9 batch 15670 trainingloss 0.6916632528527511
iteration 9 batch 15680 trainingloss 0.6931471805599453
iteration 9 batch 15690 trainingloss 0.6931471805599453
iteration 9 batch 15700 trainingloss 0.6931471805599453
iteration 9 batch 15710 trainingloss 0.6931471805599453
iteration 9 batch 15720 trainingloss 0.6931471805599453
iteration 9 batch 15730 trainingloss 0.6931471805599453
iteration 9 batch 15740 trainingloss 0.6931471805599453
iteration 9 batch 15750 trainingloss 0.6931471805599453
iteration 9 batch 15760 trainingloss 0.6931471805599453
iteration 9 batch 15770 trainingloss 0.6931471805599453
iteration 9 batch 15780 trainingloss 0.6931471805599453
iteration 9 batch 15790 trainingloss 0.6931471805599453
iteration 9 batch 15800 trainingloss 0.6931471805599453
iteration 9 batch 15810 trainingloss 0.6931471805599453
iteration 9 batch 15820 trainingloss 0.6931471805599453
iteration 9 batch 15830 trainingloss 0.6931471805599453
iteration 9 batch 15840 trainingloss 0.6931471805599453
iteration 9 batch 15850 trainingloss 0.6931471805599453
iteration 9 batch 15860 trainingloss 0.6931471805599453
iteration 9 batch 15870 trainingloss 0.6931471805599453
iteration 9 batch 15880 trainingloss 0.6931471805599453
iteration 9 batch 15890 trainingloss 0.6931471805599453
iteration 9 batch 15900 trainingloss 0.6931471805599453
iteration 9 batch 15910 trainingloss 0.6931471805599453
iteration 9 batch 15920 trainingloss 0.6931471805599453
iteration 9 batch 15930 trainingloss 0.6931471805599453
iteration 9 batch 15940 trainingloss 0.6931471805599453
iteration 9 batch 15950 trainingloss 0.6931471805599453
iteration 9 batch 15960 trainingloss 0.6931471805599453
iteration 9 batch 15970 trainingloss 0.6931471805599453
iteration 9 batch 15980 trainingloss 0.6931471805599453
iteration 9 batch 15990 trainingloss 0.6931471805599453
iteration 9 batch 16000 trainingloss 0.6931471805599453
iteration 9 batch 16010 trainingloss 0.6931471805599453
iteration 9 batch 16020 trainingloss 0.6931471805599453
iteration 9 batch 16030 trainingloss 0.6931471805599453
iteration 9 batch 16040 trainingloss 0.6931471805599453
iteration 9 batch 16050 trainingloss 0.6931471805599453
iteration 9 batch 16060 trainingloss 0.6931471805599453
iteration 9 batch 16070 trainingloss 0.6931471805599453
iteration 9 batch 16080 trainingloss 0.6931471805599453
iteration 9 batch 16090 trainingloss 0.6931471805599453
iteration 9 batch 16100 trainingloss 0.6931471805599453
iteration 9 batch 16110 trainingloss 0.6916632528527511
iteration 9 batch 16120 trainingloss 0.6916632528527511
iteration 9 batch 16130 trainingloss 0.6931471805599453
iteration 9 batch 16140 trainingloss 0.6931471805599453
iteration 9 batch 16150 trainingloss 0.6931471805599453
iteration 9 batch 16160 trainingloss 0.6931471805599453
iteration 9 batch 16170 trainingloss 0.6931471805599453
iteration 9 batch 16180 trainingloss 0.6931471805599453
iteration 9 batch 16190 trainingloss 0.6931471805599453
iteration 9 batch 16200 trainingloss 0.6931471805599453
iteration 9 batch 16210 trainingloss 0.6916632528527511
iteration 9 batch 16220 trainingloss 0.6931471805599453
iteration 9 batch 16230 trainingloss 0.6931471805599453
iteration 9 batch 16240 trainingloss 0.6931471805599453
iteration 9 batch 16250 trainingloss 0.6931471805599453
iteration 9 batch 16260 trainingloss 0.6931471805599453
iteration 9 batch 16270 trainingloss 0.6931471805599453
iteration 9 batch 16280 trainingloss 0.6931471805599453
iteration 9 batch 16290 trainingloss 0.6931471805599453
iteration 9 batch 16300 trainingloss 0.6931471805599453
iteration 9 batch 16310 trainingloss 0.6931471805599453
iteration 9 batch 16320 trainingloss 0.6931471805599453
iteration 9 batch 16330 trainingloss 0.6931471805599453
iteration 9 batch 16340 trainingloss 0.6931471805599453
iteration 9 batch 16350 trainingloss 0.6931471805599453
iteration 9 batch 16360 trainingloss 0.6931471805599453
iteration 9 batch 16370 trainingloss 0.6931471805599453
iteration 9 batch 16380 trainingloss 0.6931471805599453
iteration 9 batch 16390 trainingloss 0.6916632528527511
iteration 9 batch 16400 trainingloss 0.6931471805599453
iteration 9 batch 16410 trainingloss 0.6931471805599453
iteration 9 batch 16420 trainingloss 0.6931471805599453
iteration 9 batch 16430 trainingloss 0.6931471805599453
iteration 9 batch 16440 trainingloss 0.6931471805599453
iteration 9 batch 16450 trainingloss 0.6931471805599453
iteration 9 batch 16460 trainingloss 0.6931471805599453
iteration 9 batch 16470 trainingloss 0.6931471805599453
iteration 9 batch 16480 trainingloss 0.6931471805599453
iteration 9 batch 16490 trainingloss 0.6931471805599453
iteration 9 batch 16500 trainingloss 0.6931471805599453
iteration 9 batch 16510 trainingloss 0.6931471805599453
iteration 9 batch 16520 trainingloss 0.6931471805599453
iteration 9 batch 16530 trainingloss 0.6931471805599453
iteration 9 batch 16540 trainingloss 0.6931471805599453
iteration 9 batch 16550 trainingloss 0.6931471805599453
iteration 9 batch 16560 trainingloss 0.6931471805599453
iteration 9 batch 16570 trainingloss 0.6931471805599453
iteration 9 batch 16580 trainingloss 0.6931471805599453
iteration 9 batch 16590 trainingloss 0.6931471805599453
iteration 9 batch 16600 trainingloss 0.6931471805599453
iteration 9 batch 16610 trainingloss 0.6931471805599453
iteration 9 batch 16620 trainingloss 0.6931471805599453
iteration 9 batch 16630 trainingloss 0.6931471805599453
iteration 9 batch 16640 trainingloss 0.6916632528527511
iteration 9 batch 16650 trainingloss 0.6931471805599453
iteration 9 batch 16660 trainingloss 0.6931471805599453
iteration 9 batch 16670 trainingloss 0.6931471805599453
iteration 9 batch 16680 trainingloss 0.6931471805599453
iteration 9 batch 16690 trainingloss 0.6931471805599453
iteration 9 batch 16700 trainingloss 0.6931471805599453
iteration 9 batch 16710 trainingloss 0.6931471805599453
iteration 9 batch 16720 trainingloss 0.6931471805599453
iteration 9 batch 16730 trainingloss 0.6931471805599453
iteration 9 batch 16740 trainingloss 0.6931471805599453
iteration 9 batch 16750 trainingloss 0.6931471805599453
iteration 9 batch 16760 trainingloss 0.6931471805599453
iteration 9 batch 16770 trainingloss 0.6931471805599453
iteration 9 batch 16780 trainingloss 0.6931471805599453
iteration 9 batch 16790 trainingloss 0.6931471805599453
iteration 9 batch 16800 trainingloss 0.6931471805599453
iteration 9 batch 16810 trainingloss 0.6931471805599453
iteration 9 batch 16820 trainingloss 0.6931471805599453
iteration 9 batch 16830 trainingloss 0.6931471805599453
iteration 9 batch 16840 trainingloss 0.6931471805599453
iteration 9 batch 16850 trainingloss 0.6931471805599453
iteration 9 batch 16860 trainingloss 0.6931471805599453
iteration 9 batch 16870 trainingloss 0.6931471805599453
iteration 9 batch 16880 trainingloss 0.6931471805599453
iteration 9 batch 16890 trainingloss 0.6931471805599453
iteration 9 batch 16900 trainingloss 0.6916632528527511
iteration 9 batch 16910 trainingloss 0.6931471805599453
iteration 9 batch 16920 trainingloss 0.6931471805599453
iteration 9 batch 16930 trainingloss 0.6931471805599453
iteration 9 batch 16940 trainingloss 0.6931471805599453
iteration 9 batch 16950 trainingloss 0.6931471805599453
iteration 9 batch 16960 trainingloss 0.6931471805599453
iteration 9 batch 16970 trainingloss 0.6931471805599453
iteration 9 batch 16980 trainingloss 0.6931471805599453
iteration 9 batch 16990 trainingloss 0.6916632528527511
iteration 9 batch 17000 trainingloss 0.6931471805599453
iteration 9 batch 17010 trainingloss 0.6931471805599453
iteration 9 batch 17020 trainingloss 0.6931471805599453
iteration 9 batch 17030 trainingloss 0.6931471805599453
iteration 9 batch 17040 trainingloss 0.6931471805599453
iteration 9 batch 17050 trainingloss 0.6916632528527511
iteration 9 batch 17060 trainingloss 0.6931471805599453
iteration 9 batch 17070 trainingloss 0.6931471805599453
iteration 9 batch 17080 trainingloss 0.6931471805599453
iteration 9 batch 17090 trainingloss 0.6931471805599453
iteration 9 batch 17100 trainingloss 0.6931471805599453
iteration 9 batch 17110 trainingloss 0.6931471805599453
iteration 9 batch 17120 trainingloss 0.6931471805599453
iteration 9 batch 17130 trainingloss 0.6931471805599453
iteration 9 batch 17140 trainingloss 0.6931471805599453
iteration 9 batch 17150 trainingloss 0.6931471805599453
iteration 9 batch 17160 trainingloss 0.6931471805599453
iteration 9 batch 17170 trainingloss 0.6931471805599453
iteration 9 batch 17180 trainingloss 0.6931471805599453
iteration 9 batch 17190 trainingloss 0.6931471805599453
iteration 9 batch 17200 trainingloss 0.6931471805599453
iteration 9 batch 17210 trainingloss 0.6931471805599453
iteration 9 batch 17220 trainingloss 0.6931471805599453
iteration 9 batch 17230 trainingloss 0.6931471805599453
iteration 9 batch 17240 trainingloss 0.6931471805599453
iteration 9 batch 17250 trainingloss 0.6916632528527511
iteration 9 batch 17260 trainingloss 0.6931471805599453
iteration 9 batch 17270 trainingloss 0.6931471805599453
iteration 9 batch 17280 trainingloss 0.6931471805599453
iteration 9 batch 17290 trainingloss 0.6931471805599453
iteration 9 batch 17300 trainingloss 0.6931471805599453
iteration 9 batch 17310 trainingloss 0.6931471805599453
iteration 9 batch 17320 trainingloss 0.6931471805599453
iteration 9 batch 17330 trainingloss 0.6916632528527511
iteration 9 batch 17340 trainingloss 0.6931471805599453
iteration 9 batch 17350 trainingloss 0.6931471805599453
iteration 9 batch 17360 trainingloss 0.6931471805599453
iteration 9 batch 17370 trainingloss 0.6931471805599453
iteration 9 batch 17380 trainingloss 0.6931471805599453
iteration 9 batch 17390 trainingloss 0.6931471805599453
iteration 9 batch 17400 trainingloss 0.6931471805599453
iteration 9 batch 17410 trainingloss 0.6931471805599453
iteration 9 batch 17420 trainingloss 0.6931471805599453
iteration 9 batch 17430 trainingloss 0.6931471805599453
iteration 9 batch 17440 trainingloss 0.6931471805599453
iteration 9 batch 17450 trainingloss 0.6916632528527511
iteration 9 batch 17460 trainingloss 0.6931471805599453
iteration 9 batch 17470 trainingloss 0.6931471805599453
iteration 9 batch 17480 trainingloss 0.6931471805599453
iteration 9 batch 17490 trainingloss 0.6931471805599453
iteration 9 batch 17500 trainingloss 0.6916632528527511
iteration 9 batch 17510 trainingloss 0.6931471805599453
iteration 9 batch 17520 trainingloss 0.6931471805599453
iteration 9 batch 17530 trainingloss 0.6931471805599453
iteration 9 batch 17540 trainingloss 0.6931471805599453
iteration 9 batch 17550 trainingloss 0.6931471805599453
iteration 9 batch 17560 trainingloss 0.6931471805599453
iteration 9 batch 17570 trainingloss 0.6931471805599453
iteration 9 batch 17580 trainingloss 0.6931471805599453
iteration 9 batch 17590 trainingloss 0.6931471805599453
iteration 9 batch 17600 trainingloss 0.6931471805599453
iteration 9 batch 17610 trainingloss 0.6931471805599453
iteration 9 batch 17620 trainingloss 0.6931471805599453
iteration 9 batch 17630 trainingloss 0.6931471805599453
iteration 9 batch 17640 trainingloss 0.6931471805599453
iteration 9 batch 17650 trainingloss 0.6931471805599453
iteration 9 batch 17660 trainingloss 0.6931471805599453
iteration 9 batch 17670 trainingloss 0.6931471805599453
iteration 9 batch 17680 trainingloss 0.6931471805599453
iteration 9 batch 17690 trainingloss 0.6931471805599453
iteration 9 batch 17700 trainingloss 0.6931471805599453
iteration 9 batch 17710 trainingloss 0.6931471805599453
iteration 9 batch 17720 trainingloss 0.6931471805599453
iteration 9 batch 17730 trainingloss 0.6916632528527511
iteration 9 batch 17740 trainingloss 0.6931471805599453
iteration 9 batch 17750 trainingloss 0.6931471805599453
iteration 9 batch 17760 trainingloss 0.6931471805599453
iteration 9 batch 17770 trainingloss 0.6931471805599453
iteration 9 batch 17780 trainingloss 0.6931471805599453
iteration 9 batch 17790 trainingloss 0.6931471805599453
iteration 9 batch 17800 trainingloss 0.6931471805599453
iteration 9 batch 17810 trainingloss 0.6931471805599453
iteration 9 batch 17820 trainingloss 0.6931471805599453
iteration 9 batch 17830 trainingloss 0.6931471805599453
iteration 9 batch 17840 trainingloss 0.6931471805599453
iteration 9 batch 17850 trainingloss 0.6931471805599453
iteration 9 batch 17860 trainingloss 0.6931471805599453
iteration 9 batch 17870 trainingloss 0.6931471805599453
iteration 9 batch 17880 trainingloss 0.6931471805599453
iteration 9 batch 17890 trainingloss 0.6931471805599453
iteration 9 batch 17900 trainingloss 0.6931471805599453
iteration 9 batch 17910 trainingloss 0.6931471805599453
iteration 9 batch 17920 trainingloss 0.6931471805599453
iteration 9 batch 17930 trainingloss 0.6931471805599453
iteration 9 batch 17940 trainingloss 0.6931471805599453
iteration 9 batch 17950 trainingloss 0.6931471805599453
iteration 9 batch 17960 trainingloss 0.6931471805599453
iteration 9 batch 17970 trainingloss 0.6931471805599453
iteration 9 batch 17980 trainingloss 0.6931471805599453
iteration 9 batch 17990 trainingloss 0.6931471805599453
iteration 9 batch 18000 trainingloss 0.6931471805599453
iteration 9 batch 18010 trainingloss 0.6931471805599453
iteration 9 batch 18020 trainingloss 0.6931471805599453
iteration 9 batch 18030 trainingloss 0.6931471805599453
iteration 9 batch 18040 trainingloss 0.6931471805599453
iteration 9 batch 18050 trainingloss 0.6931471805599453
iteration 9 batch 18060 trainingloss 0.6931471805599453
iteration 9 batch 18070 trainingloss 0.6931471805599453
iteration 9 batch 18080 trainingloss 0.6931471805599453
iteration 9 batch 18090 trainingloss 0.6931471805599453
iteration 9 batch 18100 trainingloss 0.6931471805599453
iteration 9 batch 18110 trainingloss 0.6931471805599453
iteration 9 batch 18120 trainingloss 0.6931471805599453
iteration 9 batch 18130 trainingloss 0.6931471805599453
iteration 9 batch 18140 trainingloss 0.6931471805599453
iteration 9 batch 18150 trainingloss 0.6931471805599453
iteration 9 batch 18160 trainingloss 0.6931471805599453
iteration 9 batch 18170 trainingloss 0.6931471805599453
iteration 9 batch 18180 trainingloss 0.6916632528527511
iteration 9 batch 18190 trainingloss 0.6931471805599453
iteration 9 batch 18200 trainingloss 0.6931471805599453
iteration 9 batch 18210 trainingloss 0.6931471805599453
iteration 9 batch 18220 trainingloss 0.6931471805599453
iteration 9 batch 18230 trainingloss 0.6931471805599453
iteration 9 batch 18240 trainingloss 0.6931471805599453
iteration 9 batch 18250 trainingloss 0.6931471805599453
iteration 9 batch 18260 trainingloss 0.6931471805599453
iteration 9 batch 18270 trainingloss 0.6931471805599453
iteration 9 batch 18280 trainingloss 0.6931471805599453
iteration 9 batch 18290 trainingloss 0.6931471805599453
iteration 9 batch 18300 trainingloss 0.6931471805599453
iteration 9 batch 18310 trainingloss 0.6931471805599453
iteration 9 batch 18320 trainingloss 0.6931471805599453
iteration 9 batch 18330 trainingloss 0.6931471805599453
iteration 9 batch 18340 trainingloss 0.6931471805599453
iteration 9 batch 18350 trainingloss 0.6931471805599453
iteration 9 batch 18360 trainingloss 0.6931471805599453
iteration 9 batch 18370 trainingloss 0.6931471805599453
iteration 9 batch 18380 trainingloss 0.6931471805599453
iteration 9 batch 18390 trainingloss 0.6916632528527511
iteration 9 batch 18400 trainingloss 0.6931471805599453
iteration 9 batch 18410 trainingloss 0.6916632528527511
iteration 9 batch 18420 trainingloss 0.6931471805599453
iteration 9 batch 18430 trainingloss 0.6931471805599453
iteration 9 batch 18440 trainingloss 0.6931471805599453
iteration 9 batch 18450 trainingloss 0.6931471805599453
iteration 9 batch 18460 trainingloss 0.6931471805599453
iteration 9 batch 18470 trainingloss 0.6931471805599453
iteration 9 batch 18480 trainingloss 0.6931471805599453
iteration 9 batch 18490 trainingloss 0.6931471805599453
iteration 9 batch 18500 trainingloss 0.6931471805599453
iteration 9 batch 18510 trainingloss 0.6931471805599453
iteration 9 batch 18520 trainingloss 0.6931471805599453
iteration 9 batch 18530 trainingloss 0.6931471805599453
iteration 9 batch 18540 trainingloss 0.6931471805599453
iteration 9 batch 18550 trainingloss 0.6931471805599453
iteration 9 batch 18560 trainingloss 0.6931471805599453
iteration 9 batch 18570 trainingloss 0.6931471805599453
iteration 9 batch 18580 trainingloss 0.6931471805599453
iteration 9 batch 18590 trainingloss 0.6931471805599453
iteration 9 batch 18600 trainingloss 0.6931471805599453
iteration 9 batch 18610 trainingloss 0.6931471805599453