\section{Conclusion}

We study learning over distributed features (vertically partitioned data) where none of the parties shall share the local data. %The motivation is in contrast to the most existing literature on collaborative and distributed machine learning where the data samples (but not the features) are assumed to distribute and work in a data-parallel fashion. 
We propose the parallel ADMM sharing algorithm to solve this challenging problem where only intermediate values are shared, without even sharing model parameters. We have shown the convergence for convex and non-convex loss functions. To further protect the data privacy, we apply the differential privacy technique in the training procedure to derive a privacy guarantee within $T$ epochs. 
We implement a prototype system and evaluate the proposed algorithm on two representative datasets in risk minimization. The result shows that the ADMM sharing algorithm converges efficiently, especially on dataset with large number of features. Furthermore, the differentially private ADMM algorithm yields better prediction accuracy than model trained from only local features while ensuring a certain level of differential privacy guarantee. 